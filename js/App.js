/******/ (function(modules) { // webpackBootstrap
/******/ 	// install a JSONP callback for chunk loading
/******/ 	function webpackJsonpCallback(data) {
/******/ 		var chunkIds = data[0];
/******/ 		var moreModules = data[1];
/******/ 		var executeModules = data[2];
/******/
/******/ 		// add "moreModules" to the modules object,
/******/ 		// then flag all "chunkIds" as loaded and fire callback
/******/ 		var moduleId, chunkId, i = 0, resolves = [];
/******/ 		for(;i < chunkIds.length; i++) {
/******/ 			chunkId = chunkIds[i];
/******/ 			if(installedChunks[chunkId]) {
/******/ 				resolves.push(installedChunks[chunkId][0]);
/******/ 			}
/******/ 			installedChunks[chunkId] = 0;
/******/ 		}
/******/ 		for(moduleId in moreModules) {
/******/ 			if(Object.prototype.hasOwnProperty.call(moreModules, moduleId)) {
/******/ 				modules[moduleId] = moreModules[moduleId];
/******/ 			}
/******/ 		}
/******/ 		if(parentJsonpFunction) parentJsonpFunction(data);
/******/
/******/ 		while(resolves.length) {
/******/ 			resolves.shift()();
/******/ 		}
/******/
/******/ 		// add entry modules from loaded chunk to deferred list
/******/ 		deferredModules.push.apply(deferredModules, executeModules || []);
/******/
/******/ 		// run deferred modules when all chunks ready
/******/ 		return checkDeferredModules();
/******/ 	};
/******/ 	function checkDeferredModules() {
/******/ 		var result;
/******/ 		for(var i = 0; i < deferredModules.length; i++) {
/******/ 			var deferredModule = deferredModules[i];
/******/ 			var fulfilled = true;
/******/ 			for(var j = 1; j < deferredModule.length; j++) {
/******/ 				var depId = deferredModule[j];
/******/ 				if(installedChunks[depId] !== 0) fulfilled = false;
/******/ 			}
/******/ 			if(fulfilled) {
/******/ 				deferredModules.splice(i--, 1);
/******/ 				result = __webpack_require__(__webpack_require__.s = deferredModule[0]);
/******/ 			}
/******/ 		}
/******/ 		return result;
/******/ 	}
/******/
/******/ 	// The module cache
/******/ 	var installedModules = {};
/******/
/******/ 	// object to store loaded CSS chunks
/******/ 	var installedCssChunks = {
/******/ 		"App": 0
/******/ 	}
/******/
/******/ 	// object to store loaded and loading chunks
/******/ 	// undefined = chunk not loaded, null = chunk preloaded/prefetched
/******/ 	// Promise = chunk loading, 0 = chunk loaded
/******/ 	var installedChunks = {
/******/ 		"App": 0
/******/ 	};
/******/
/******/ 	var deferredModules = [];
/******/
/******/ 	// script path function
/******/ 	function jsonpScriptSrc(chunkId) {
/******/ 		return __webpack_require__.p + "js/" + ({"Actions":"Actions","Rules":"Rules","ActionsOverview":"ActionsOverview","ListActions":"ListActions","ListRules":"ListRules"}[chunkId]||chunkId) + ".js"
/******/ 	}
/******/
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/
/******/ 		// Check if module is in cache
/******/ 		if(installedModules[moduleId]) {
/******/ 			return installedModules[moduleId].exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = installedModules[moduleId] = {
/******/ 			i: moduleId,
/******/ 			l: false,
/******/ 			exports: {}
/******/ 		};
/******/
/******/ 		// Execute the module function
/******/ 		modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);
/******/
/******/ 		// Flag the module as loaded
/******/ 		module.l = true;
/******/
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/
/******/ 	// This file contains only the entry chunk.
/******/ 	// The chunk loading function for additional chunks
/******/ 	__webpack_require__.e = function requireEnsure(chunkId) {
/******/ 		var promises = [];
/******/
/******/
/******/ 		// mini-css-extract-plugin CSS loading
/******/ 		var cssChunks = {"1":1,"ListActions":1,"ListRules":1};
/******/ 		if(installedCssChunks[chunkId]) promises.push(installedCssChunks[chunkId]);
/******/ 		else if(installedCssChunks[chunkId] !== 0 && cssChunks[chunkId]) {
/******/ 			promises.push(installedCssChunks[chunkId] = new Promise(function(resolve, reject) {
/******/ 				var href = "css/" + ({"Actions":"Actions","Rules":"Rules","ActionsOverview":"ActionsOverview","ListActions":"ListActions","ListRules":"ListRules"}[chunkId]||chunkId) + ".css";
/******/ 				var fullhref = __webpack_require__.p + href;
/******/ 				var existingLinkTags = document.getElementsByTagName("link");
/******/ 				for(var i = 0; i < existingLinkTags.length; i++) {
/******/ 					var tag = existingLinkTags[i];
/******/ 					var dataHref = tag.getAttribute("data-href") || tag.getAttribute("href");
/******/ 					if(tag.rel === "stylesheet" && (dataHref === href || dataHref === fullhref)) return resolve();
/******/ 				}
/******/ 				var existingStyleTags = document.getElementsByTagName("style");
/******/ 				for(var i = 0; i < existingStyleTags.length; i++) {
/******/ 					var tag = existingStyleTags[i];
/******/ 					var dataHref = tag.getAttribute("data-href");
/******/ 					if(dataHref === href || dataHref === fullhref) return resolve();
/******/ 				}
/******/ 				var linkTag = document.createElement("link");
/******/ 				linkTag.rel = "stylesheet";
/******/ 				linkTag.type = "text/css";
/******/ 				linkTag.onload = resolve;
/******/ 				linkTag.onerror = function(event) {
/******/ 					var request = event && event.target && event.target.src || fullhref;
/******/ 					var err = new Error("Loading CSS chunk " + chunkId + " failed.\n(" + request + ")");
/******/ 					err.request = request;
/******/ 					reject(err);
/******/ 				};
/******/ 				linkTag.href = fullhref;
/******/ 				var head = document.getElementsByTagName("head")[0];
/******/ 				head.appendChild(linkTag);
/******/ 			}).then(function() {
/******/ 				installedCssChunks[chunkId] = 0;
/******/ 			}));
/******/ 		}
/******/
/******/ 		// JSONP chunk loading for javascript
/******/
/******/ 		var installedChunkData = installedChunks[chunkId];
/******/ 		if(installedChunkData !== 0) { // 0 means "already installed".
/******/
/******/ 			// a Promise means "currently loading".
/******/ 			if(installedChunkData) {
/******/ 				promises.push(installedChunkData[2]);
/******/ 			} else {
/******/ 				// setup Promise in chunk cache
/******/ 				var promise = new Promise(function(resolve, reject) {
/******/ 					installedChunkData = installedChunks[chunkId] = [resolve, reject];
/******/ 				});
/******/ 				promises.push(installedChunkData[2] = promise);
/******/
/******/ 				// start chunk loading
/******/ 				var head = document.getElementsByTagName('head')[0];
/******/ 				var script = document.createElement('script');
/******/ 				var onScriptComplete;
/******/
/******/ 				script.charset = 'utf-8';
/******/ 				script.timeout = 120;
/******/ 				if (__webpack_require__.nc) {
/******/ 					script.setAttribute("nonce", __webpack_require__.nc);
/******/ 				}
/******/ 				script.src = jsonpScriptSrc(chunkId);
/******/
/******/ 				onScriptComplete = function (event) {
/******/ 					// avoid mem leaks in IE.
/******/ 					script.onerror = script.onload = null;
/******/ 					clearTimeout(timeout);
/******/ 					var chunk = installedChunks[chunkId];
/******/ 					if(chunk !== 0) {
/******/ 						if(chunk) {
/******/ 							var errorType = event && (event.type === 'load' ? 'missing' : event.type);
/******/ 							var realSrc = event && event.target && event.target.src;
/******/ 							var error = new Error('Loading chunk ' + chunkId + ' failed.\n(' + errorType + ': ' + realSrc + ')');
/******/ 							error.type = errorType;
/******/ 							error.request = realSrc;
/******/ 							chunk[1](error);
/******/ 						}
/******/ 						installedChunks[chunkId] = undefined;
/******/ 					}
/******/ 				};
/******/ 				var timeout = setTimeout(function(){
/******/ 					onScriptComplete({ type: 'timeout', target: script });
/******/ 				}, 120000);
/******/ 				script.onerror = script.onload = onScriptComplete;
/******/ 				head.appendChild(script);
/******/ 			}
/******/ 		}
/******/ 		return Promise.all(promises);
/******/ 	};
/******/
/******/ 	// expose the modules object (__webpack_modules__)
/******/ 	__webpack_require__.m = modules;
/******/
/******/ 	// expose the module cache
/******/ 	__webpack_require__.c = installedModules;
/******/
/******/ 	// define getter function for harmony exports
/******/ 	__webpack_require__.d = function(exports, name, getter) {
/******/ 		if(!__webpack_require__.o(exports, name)) {
/******/ 			Object.defineProperty(exports, name, { enumerable: true, get: getter });
/******/ 		}
/******/ 	};
/******/
/******/ 	// define __esModule on exports
/******/ 	__webpack_require__.r = function(exports) {
/******/ 		if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 			Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 		}
/******/ 		Object.defineProperty(exports, '__esModule', { value: true });
/******/ 	};
/******/
/******/ 	// create a fake namespace object
/******/ 	// mode & 1: value is a module id, require it
/******/ 	// mode & 2: merge all properties of value into the ns
/******/ 	// mode & 4: return value when already ns object
/******/ 	// mode & 8|1: behave like require
/******/ 	__webpack_require__.t = function(value, mode) {
/******/ 		if(mode & 1) value = __webpack_require__(value);
/******/ 		if(mode & 8) return value;
/******/ 		if((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;
/******/ 		var ns = Object.create(null);
/******/ 		__webpack_require__.r(ns);
/******/ 		Object.defineProperty(ns, 'default', { enumerable: true, value: value });
/******/ 		if(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));
/******/ 		return ns;
/******/ 	};
/******/
/******/ 	// getDefaultExport function for compatibility with non-harmony modules
/******/ 	__webpack_require__.n = function(module) {
/******/ 		var getter = module && module.__esModule ?
/******/ 			function getDefault() { return module['default']; } :
/******/ 			function getModuleExports() { return module; };
/******/ 		__webpack_require__.d(getter, 'a', getter);
/******/ 		return getter;
/******/ 	};
/******/
/******/ 	// Object.prototype.hasOwnProperty.call
/******/ 	__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };
/******/
/******/ 	// __webpack_public_path__
/******/ 	__webpack_require__.p = "/insights/platform/advisor/";
/******/
/******/ 	// on error function for async loading
/******/ 	__webpack_require__.oe = function(err) { console.error(err); throw err; };
/******/
/******/ 	var jsonpArray = window["webpackJsonp"] = window["webpackJsonp"] || [];
/******/ 	var oldJsonpFunction = jsonpArray.push.bind(jsonpArray);
/******/ 	jsonpArray.push = webpackJsonpCallback;
/******/ 	jsonpArray = jsonpArray.slice();
/******/ 	for(var i = 0; i < jsonpArray.length; i++) webpackJsonpCallback(jsonpArray[i]);
/******/ 	var parentJsonpFunction = oldJsonpFunction;
/******/
/******/
/******/ 	// add entry module to deferred list
/******/ 	deferredModules.push(["./src/entry-dev.js","vendor"]);
/******/ 	// run deferred modules when ready
/******/ 	return checkDeferredModules();
/******/ })
/************************************************************************/
/******/ ({

/***/ "./mockData/actions-types-ids_impacted-systems.json":
/*!**********************************************************!*\
  !*** ./mockData/actions-types-ids_impacted-systems.json ***!
  \**********************************************************/
/*! exports provided: resources, total, default */
/***/ (function(module) {

module.exports = {"resources":[{"toString":"0506883e-488a-436f-9d83-3b0e75464cb0","isCheckingIn":true,"system_id":"0506883e-488a-436f-9d83-3b0e75464cb0","display_name":null,"remote_branch":"6d8447bc-5c24-4d54-99a2-026488b483de","remote_leaf":"84a0b426-d235-43a7-a34f-173e799a1dec","account_number":"1460290","hostname":"","last_check_in":"2018-08-16T20:34:10.000Z","created_at":"2018-08-16T20:07:14.000Z","updated_at":"2018-08-16T20:34:10.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":6},{"toString":"cf3.example.com","isCheckingIn":false,"system_id":"d8b1f868-8a38-43f3-9339-5082bd4a94a5","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"cf3.example.com","last_check_in":"2018-08-05T04:10:36.000Z","created_at":"2018-07-25T15:47:56.000Z","updated_at":"2018-08-10T10:52:34.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":9},{"toString":"demo1.mihalabo.com","isCheckingIn":false,"system_id":"7a0215ca-bf74-4f6b-adff-4af70ab2075f","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"demo1.mihalabo.com","last_check_in":"2018-08-06T03:30:07.000Z","created_at":"2018-08-06T03:29:46.000Z","updated_at":"2018-08-10T10:52:34.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":19},{"toString":"demo2.mihalabo.com","isCheckingIn":false,"system_id":"1d505fbc-65b4-4ee1-986d-627fcafba7d9","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"demo2.mihalabo.com","last_check_in":"2018-08-03T06:55:06.000Z","created_at":"2018-07-25T12:12:39.000Z","updated_at":"2018-08-10T10:52:34.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":7},{"toString":"glusterstg01.labs.corp","isCheckingIn":false,"system_id":"0beb6cdb-c90b-4210-be47-6d59cf2fad95","display_name":null,"remote_branch":"5fd9d09b-2fbb-4380-aabd-433e7af11eab","remote_leaf":"720b8af6-86f9-4ada-b53a-57e9d6389b71","account_number":"1460290","hostname":"glusterstg01.labs.corp","last_check_in":"2018-08-15T06:58:02.000Z","created_at":"2018-08-01T18:54:57.000Z","updated_at":"2018-08-15T06:58:02.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":7},{"toString":"idm.labs.corp","isCheckingIn":false,"system_id":"72fffdcb-9412-43a3-9f12-f21bf3dbd514","display_name":null,"remote_branch":"5fd9d09b-2fbb-4380-aabd-433e7af11eab","remote_leaf":"1841b205-5063-4c24-96f2-dcd268c99789","account_number":"1460290","hostname":"idm.labs.corp","last_check_in":"2018-08-15T04:11:27.000Z","created_at":"2018-08-01T18:25:12.000Z","updated_at":"2018-08-15T04:11:27.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":4},{"toString":"localhost.localdomain","isCheckingIn":false,"system_id":"2b8262b1-da91-4ac3-b5dd-618e9a3a6a96","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"localhost.localdomain","last_check_in":"2018-08-05T03:25:33.000Z","created_at":"2018-08-05T03:25:24.000Z","updated_at":"2018-08-10T10:52:34.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":19},{"toString":"rhel74.localdomain","isCheckingIn":false,"system_id":"1c3f29dc-90b3-4ece-85e9-7cde540751bc","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"rhel74.localdomain","last_check_in":"2018-07-28T05:14:28.000Z","created_at":"2018-07-16T13:04:19.000Z","updated_at":"2018-07-30T17:51:47.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":3},{"toString":"rhel7svr1","isCheckingIn":false,"system_id":"2e38982d-69fc-4871-9288-c31a984b60ba","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"rhel7svr1","last_check_in":"2018-07-17T18:59:58.000Z","created_at":"2018-07-17T18:59:33.000Z","updated_at":"2018-07-30T17:51:47.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":5},{"toString":"rhel7svr4","isCheckingIn":false,"system_id":"fc4d910e-fe24-4d8c-889b-6cfdfc26992d","display_name":null,"remote_branch":null,"remote_leaf":null,"account_number":"1460290","hostname":"rhel7svr4","last_check_in":"2018-07-17T18:52:40.000Z","created_at":"2018-07-17T18:52:19.000Z","updated_at":"2018-07-30T17:51:47.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":3},{"toString":"teste001.labs.corp","isCheckingIn":false,"system_id":"4f37891e-0a7c-4e6e-8aea-d57f03e33708","display_name":null,"remote_branch":"5fd9d09b-2fbb-4380-aabd-433e7af11eab","remote_leaf":"ecc2e7af-f382-4617-b955-449a27987f95","account_number":"1460290","hostname":"teste001.labs.corp","last_check_in":"2018-08-03T18:20:15.000Z","created_at":"2018-08-02T20:55:47.000Z","updated_at":"2018-08-10T10:52:34.000Z","unregistered_at":null,"system_type_id":105,"role":"host","product_code":"rhel","report_count":5}],"total":11};

/***/ }),

/***/ "./mockData/medium-risk.json":
/*!***********************************!*\
  !*** ./mockData/medium-risk.json ***!
  \***********************************/
/*! exports provided: content_html, summary_html, rules, ruleBinding, hitCount, affectedSystemCount, alwaysShow, id, title, summary, content, priority, listed, tag, category, severity, hidden, slug, default */
/***/ (function(module) {

module.exports = {"content_html":"<p>Actions identified with a medium level of risk</p>\n","summary_html":"<p>Actions identified with a medium level of risk</p>\n","rules":[{"rule_id":"xinetd_per_source_limit|XINETD_PER_SOURCE_LIMIT","description":"Services controlled by xinetd fail to respond when  per_source_limit of xinetd is reached","category":"Stability","severity":"WARN","hitCount":1,"summary":"A large number of \"xinetd per_source_limit\" messages are being reported because services are reaching limits defined in the xinetd configuration file.","summary_html":"<p>A large number of &quot;xinetd per_source_limit&quot; messages are being reported because services are reaching limits defined in the xinetd configuration file.</p>\n","plugin":"xinetd_per_source_limit","error_key":"XINETD_PER_SOURCE_LIMIT","plugin_name":"Services controlled by xinetd fail to respond when  per_source_limit of xinetd is reached","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"xen_pv_guest_boot_failure|XEN_PV_GUEST_BOOT_FAILURE","description":"Xen PV instance fails to boot with specific versions of kernel","category":"Stability","severity":"WARN","hitCount":0,"summary":"Xen PV instance fails to boot with kernel 2.6.32-696.18.7.el6 after kernel is upgraded to this version.\n","summary_html":"<p>Xen PV instance fails to boot with kernel 2.6.32-696.18.7.el6 after kernel is upgraded to this version.</p>\n","plugin":"xen_pv_guest_boot_failure","error_key":"XEN_PV_GUEST_BOOT_FAILURE","plugin_name":"Xen PV instance fails to boot with specific versions of kernel","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"xen_kdump_supported|XEN_KDUMP_SUPPORTED","description":"Kdump does not work due to XEN/AWS's limitation.","category":"Availability","severity":"WARN","hitCount":0,"summary":"Xen-based environments do not currently support `kdump`.\n","summary_html":"<p>Xen-based environments do not currently support <code>kdump</code>.</p>\n","plugin":"xen_kdump_supported","error_key":"XEN_KDUMP_SUPPORTED","plugin_name":"Kdump does not work due to XEN/AWS's limitation.","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"vpd_rw_failed|VPD_RW_FAILED_KERNEL","description":"System lockups when abnormal VPD data returned from HBA due to bug in kernel","category":"Stability","severity":"WARN","hitCount":0,"summary":"System lockups or VPD R/W failed messages can occur if the kernel reads past end VPD data tags or attempts to read any past invalid VPD tags that it finds.\n","summary_html":"<p>System lockups or VPD R/W failed messages can occur if the kernel reads past end VPD data tags or attempts to read any past invalid VPD tags that it finds.</p>\n","plugin":"vpd_rw_failed","error_key":"VPD_RW_FAILED_KERNEL","plugin_name":"System lockups or VPD R/W failed messages can occur when reading VPD data abnormally","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"vpd_rw_failed|VPD_RW_FAILED_FIRMWARE","description":"System lockups when abnormal VPD data returned from HBA firmware","category":"Stability","severity":"WARN","hitCount":0,"summary":"VPD error message occurs when running a QLogic HBA card with outdated firmware.","summary_html":"<p>VPD error message occurs when running a QLogic HBA card with outdated firmware.</p>\n","plugin":"vpd_rw_failed","error_key":"VPD_RW_FAILED_FIRMWARE","plugin_name":"System lockups or VPD R/W failed messages can occur when reading VPD data abnormally","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"vm_swappiness_oom|VM_SWAPPINESS_OOM_ISSUE_WARN","description":"Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions where database is running","category":"Performance","severity":"WARN","hitCount":0,"summary":"Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.\n","summary_html":"<p>Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.</p>\n","plugin":"vm_swappiness_oom","error_key":"VM_SWAPPINESS_OOM_ISSUE_WARN","plugin_name":"Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"vm_swappiness_oom|VM_SWAPPINESS_OOM_ISSUE_INFO","description":"Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions","category":"Performance","severity":"WARN","hitCount":0,"summary":"Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.\n","summary_html":"<p>Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.</p>\n","plugin":"vm_swappiness_oom","error_key":"VM_SWAPPINESS_OOM_ISSUE_INFO","plugin_name":"Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"vmw_driver_issue|VMW_DRIVER_ISSUE","description":"VM guest with vmw_pvscsi driver can crash when using the specific kernel versions on VMware ESXi 5 platform","category":"Stability","severity":"WARN","hitCount":0,"summary":"VM guests with the `vmw_pvscsi` paravirtualized driver can crash when using the kernel version prior to `kernel-2.6.32-504.el6.x86_64` on VMware ESXi 5 platform.\n","summary_html":"<p>VM guests with the <code>vmw_pvscsi</code> paravirtualized driver can crash when using the kernel version prior to <code>kernel-2.6.32-504.el6.x86_64</code> on VMware ESXi 5 platform.</p>\n","plugin":"vmw_driver_issue","error_key":"VMW_DRIVER_ISSUE","plugin_name":"VM guest with vmw_pvscsi driver can crash when using the specific kernel versions on VMware ESXi 5 platform","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"vmware_slabinfo_nmi_crash|VMWARE_SLABINFO_NMI_CRASH","description":"NMI enabled on VMWare guest may cause system crash","category":"Stability","severity":"WARN","hitCount":0,"summary":"Guest virtual machines running under VMware can be crashed by the NMI watchdog when a process accesses `/proc/slabinfo` if the host system has been under heavy memory pressure.","summary_html":"<p>Guest virtual machines running under VMware can be crashed by the NMI watchdog when a process accesses <code>/proc/slabinfo</code> if the host system has been under heavy memory pressure.</p>\n","plugin":"vmware_slabinfo_nmi_crash","error_key":"VMWARE_SLABINFO_NMI_CRASH","plugin_name":"NMI enabled on VMWare guest may cause system crash","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":2,"acked":false},{"rule_id":"vmware_guest_clock_unstable|VMWARE_GUEST_CLOCK_UNSTABLE","description":"Failure to synchronize system clock with remote NTP servers on VMWare guest due to a bug in kernel","category":"Stability","severity":"WARN","hitCount":0,"summary":"VMWare guests fail to synchronize system clock with remote NTP servers due to a bug in kernel.\n","summary_html":"<p>VMWare guests fail to synchronize system clock with remote NTP servers due to a bug in kernel.</p>\n","plugin":"vmware_guest_clock_unstable","error_key":"VMWARE_GUEST_CLOCK_UNSTABLE","plugin_name":"Failure to synchronize system clock with remote NTP servers on VMWare guest due to a bug in kernel","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"unsupported_journal_mode|UNSUPPORTED_JOURNAL_MODE_WARN","description":"Risk of filesystem corruption when unsupported journal mode will be used in the next restart","category":"Stability","severity":"WARN","hitCount":0,"summary":"Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of \"ordered\".\n","summary_html":"<p>Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of &quot;ordered&quot;.</p>\n","plugin":"unsupported_journal_mode","error_key":"UNSUPPORTED_JOURNAL_MODE_WARN","plugin_name":"Filesystem corruption when using unsupported journal modes","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"unsupported_journal_mode|UNSUPPORTED_JOURNAL_MODE_ERROR_3","description":"Risk of filesystem corruption when unsupported journal mode is being used in the host","category":"Stability","severity":"WARN","hitCount":0,"summary":"Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of \"ordered\".\n","summary_html":"<p>Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of &quot;ordered&quot;.</p>\n","plugin":"unsupported_journal_mode","error_key":"UNSUPPORTED_JOURNAL_MODE_ERROR_3","plugin_name":"Filesystem corruption when using unsupported journal modes","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"udev_bug|UDEV_BUG","description":"Udev rules fail to set correct ownership on devices during reboot when using specific versions of udev","category":"Availability","severity":"WARN","hitCount":0,"summary":"The `udev-147-2.73.el6_8.1` package has a bug that fails to set the correct ownership on devices on reboot.","summary_html":"<p>The <code>udev-147-2.73.el6_8.1</code> package has a bug that fails to set the correct ownership on devices on reboot.</p>\n","plugin":"udev_bug","error_key":"UDEV_BUG","plugin_name":"Udev rules fail to set correct ownership on devices during reboot when using specific versions of udev","ansible":1,"rec_impact":1,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"turla_malware|TURLA_MALWARE_DETECTED","description":"Compromised system by Turla malware","category":"Security","severity":"WARN","hitCount":0,"summary":"This rule tests for the presence of [Turla malware](https://access.redhat.com/articles/1290883) suite characteristics on the host.","summary_html":"<p>This rule tests for the presence of <a href=\"https://access.redhat.com/articles/1290883\">Turla malware</a> suite characteristics on the host.</p>\n","plugin":"turla_malware","error_key":"TURLA_MALWARE_DETECTED","plugin_name":"Compromised system by Turla malware","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"tune_haproxy_openstack|TUNE_HAPROXY_OPENSTACK","description":"OpenStack APIs respond slowly when exceeding maximum number of MariaDB connections","category":"Performance","severity":"WARN","hitCount":0,"summary":"In OpenStack highly available deployments, the APIs behind HAProxy might respond very slowly or fail if the controller node Maria DB max_connections are not tuned correctly.","summary_html":"<p>In OpenStack highly available deployments, the APIs behind HAProxy might respond very slowly or fail if the controller node Maria DB max_connections are not tuned correctly.</p>\n","plugin":"tune_haproxy_openstack","error_key":"TUNE_HAPROXY_OPENSTACK","plugin_name":"OpenStack APIs respond slowly when exceeding maximum number of MariaDB connections","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"tsc_rhel612_hung|TSC_BUG_NO_PROPER_KERNEL_PARAMETER","description":"System Experiencing Short due to bug in the C3/C6 state transition","category":"Stability","severity":"WARN","hitCount":0,"summary":"System Experiencing Short due to bug in the C3/C6 state transition.\n","summary_html":"<p>System Experiencing Short due to bug in the C3/C6 state transition.</p>\n","plugin":"tsc_rhel612_hung","error_key":"TSC_BUG_NO_PROPER_KERNEL_PARAMETER","plugin_name":"System Experiencing Short due to bug in the C3/C6 state transition","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"thp_defrag_performance|THP_DEFRAG_PERFORMANCE","description":"CPU spike and performance problems occur when enabling Transparent Huge Pages","category":"Performance","severity":"WARN","hitCount":0,"summary":"CPU spike and performance problems occur when enabling Transparent Huge Pages.\n","summary_html":"<p>CPU spike and performance problems occur when enabling Transparent Huge Pages.</p>\n","plugin":"thp_defrag_performance","error_key":"THP_DEFRAG_PERFORMANCE","plugin_name":"CPU spike and performance problems occur when enabling Transparent Huge Pages","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"temp_core_message|CORE_TEMPERATURE_ISSUE","description":"Core temperature warning logged when CPU temperatures were above acceptable thresholds","category":"Performance","severity":"WARN","hitCount":0,"summary":"Core temperature warning messages might indicate potential failure of cooling system, clogged air intakes, faulty on-board sensors, or unreasonably low thresholds.","summary_html":"<p>Core temperature warning messages might indicate potential failure of cooling system, clogged air intakes, faulty on-board sensors, or unreasonably low thresholds.</p>\n","plugin":"temp_core_message","error_key":"CORE_TEMPERATURE_ISSUE","plugin_name":"Core temperature warning logged when CPU temperatures were above acceptable thresholds","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"team_arp_flood_issue|TEAM_ARP_FLOOD_ISSUE","description":"Gratuitous ARPs on team interface can flood network and cause network connectivity loss when using specific kernel versions","category":"Performance","severity":"WARN","hitCount":0,"summary":"Gratuitous Address Resolution Protocol (ARPs) packets on `team` interface can flood network and cause network connectivity loss when using specific kernel versions on Red Hat Enterprise Linux 7.\n","summary_html":"<p>Gratuitous Address Resolution Protocol (ARPs) packets on <code>team</code> interface can flood network and cause network connectivity loss when using specific kernel versions on Red Hat Enterprise Linux 7.</p>\n","plugin":"team_arp_flood_issue","error_key":"TEAM_ARP_FLOOD_ISSUE","plugin_name":"Gratuitous ARPs on team interface can flood network and cause network connectivity loss when using specific kernel versions","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"system_hang_with_multipath|SYSTEM_HANG_ON_REBOOT_WARN","description":"System hang on reboot caused by faulty paths on multipath devices when using multipath with incompatible configuration options in device-mapper-multipath","category":"Availability","severity":"WARN","hitCount":0,"summary":"When shutting down or rebooting a system with multipath devices configured, the system may hang during the shutdown process.\n","summary_html":"<p>When shutting down or rebooting a system with multipath devices configured, the system may hang during the shutdown process.</p>\n","plugin":"system_hang_with_multipath","error_key":"SYSTEM_HANG_ON_REBOOT_WARN","plugin_name":"System hang on reboot when using multipath with incompatible configuration options in device-mapper-multipath","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"system_hang_with_multipath|SYSTEM_HANG_ON_REBOOT_INFO","description":"System hang on reboot when using multipath with incompatible configuration options in device-mapper-multipath","category":"Availability","severity":"WARN","hitCount":0,"summary":"When shutting down or rebooting a system with multipath devices configured, the system may hang during the shutdown process.\n","summary_html":"<p>When shutting down or rebooting a system with multipath devices configured, the system may hang during the shutdown process.</p>\n","plugin":"system_hang_with_multipath","error_key":"SYSTEM_HANG_ON_REBOOT_INFO","plugin_name":"System hang on reboot when using multipath with incompatible configuration options in device-mapper-multipath","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"systemd_unmount_filesystem_on_multipath_device|SYSTEMD_UNMOUNT_FILESYSTEM","description":"Systemd unmount file system on multipath device when paths lost","category":"Stability","severity":"WARN","hitCount":0,"summary":"The device mapper sets the `DM_UDEV_DISABLE_OTHER_RULES_FLAG` signal to indicate that other rules should not react to the event. However, systemd previously considered the related device not to be ready for use, which is only true for the ADD event. As a consequence, file systems on the devices were unmounted even if only one path became unavailable, but other paths were still accessible.","summary_html":"<p>The device mapper sets the <code>DM_UDEV_DISABLE_OTHER_RULES_FLAG</code> signal to indicate that other rules should not react to the event. However, systemd previously considered the related device not to be ready for use, which is only true for the ADD event. As a consequence, file systems on the devices were unmounted even if only one path became unavailable, but other paths were still accessible.</p>\n","plugin":"systemd_unmount_filesystem_on_multipath_device","error_key":"SYSTEMD_UNMOUNT_FILESYSTEM","plugin_name":"Systemd unmount file system on multipath device when paths lost","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":1,"acked":false},{"rule_id":"systemd_open_watchdog_nmi|SYSTEMD_HP_GEN8_GEN9_NMI","description":"HP Gen8 and Gen9 system crash due to NMI when watchdog timer times out","category":"Stability","severity":"WARN","hitCount":0,"summary":"HP Gen8 and Gen9 system crash due to NMI when watchdog timer times out.\n","summary_html":"<p>HP Gen8 and Gen9 system crash due to NMI when watchdog timer times out.</p>\n","plugin":"systemd_open_watchdog_nmi","error_key":"SYSTEMD_HP_GEN8_GEN9_NMI","plugin_name":"HP Gen8 and Gen9 system crash due to NMI when watchdog timer times out","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"systemd_non_persistent_ifname|NON_PERSISTENT_IFNAME","description":"Network device naming on the host is inconsistent due to using net.ifnames=0 improperly","category":"Availability","severity":"WARN","hitCount":0,"summary":"Predictable Network Interface Names has been disabled on this host so network device naming on the host can be inconsistent.\n","summary_html":"<p>Predictable Network Interface Names has been disabled on this host so network device naming on the host can be inconsistent.</p>\n","plugin":"systemd_non_persistent_ifname","error_key":"NON_PERSISTENT_IFNAME","plugin_name":"Network device naming on the host is inconsistent due to using net.ifnames=0 improperly","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"swift_memcached_localhost_address|SWIFT_MEMCACHED_LOCALHOST_ADDRESS","description":"Performance degradation of swift object-expirer when memcached address is set to localhost","category":"Performance","severity":"WARN","hitCount":0,"summary":"Performance degradation of swift object-expirer when memcached address is set to localhost in `object-expirer.conf`\n","summary_html":"<p>Performance degradation of swift object-expirer when memcached address is set to localhost in <code>object-expirer.conf</code></p>\n","plugin":"swift_memcached_localhost_address","error_key":"SWIFT_MEMCACHED_LOCALHOST_ADDRESS","plugin_name":"Performance degradation of swift object-expirer when memcached address is set to localhost","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"swift_gnocchi_infinite_loop|SWIFT_GNOCCHI_INFINITE_LOOP","description":"OpenStack environment is down due to an infinite loop between Gnocchi and Swift","category":"Availability","severity":"WARN","hitCount":0,"summary":"When Ceilometer events are stored by Swift, new Ceilometer events are generated resulting in an infinite loop.\n","summary_html":"<p>When Ceilometer events are stored by Swift, new Ceilometer events are generated resulting in an infinite loop.</p>\n","plugin":"swift_gnocchi_infinite_loop","error_key":"SWIFT_GNOCCHI_INFINITE_LOOP","plugin_name":"Whole OpenStack environment is down as infinite loop between gnocchi and swift","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"storage_nfs_over_loop|NFS_LOOPBACK_MOUNT_V2","description":"System hangs when mounting an NFS share over loopback","category":"Stability","severity":"WARN","hitCount":0,"summary":"This host is serving an NFS file system and mounting that file system at the same time, which is known as an NFS loopback and is unsupported.\n","summary_html":"<p>This host is serving an NFS file system and mounting that file system at the same time, which is known as an NFS loopback and is unsupported.</p>\n","plugin":"storage_nfs_over_loop","error_key":"NFS_LOOPBACK_MOUNT_V2","plugin_name":"System hangs when mounting an NFS share over loopback","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"storage_netfs_without_netdev|NETWORK_FILESYSTEM_NOT_MOUNT_ON_STARTUP","description":"Network file system fails to be mounted during bootup when the _netdev parameter is not set in the fstab file","category":"Availability","severity":"WARN","hitCount":0,"summary":"If the `_netdev` parameter is not set in `/etc/fstab` and the `netfs` service (RHEL6) or `remote-fs.target`service (RHEL7) is not enabled, the network file system (like NFS, SMB, and those on iSCSI) will not be mounted as expected during boot.\n","summary_html":"<p>If the <code>_netdev</code> parameter is not set in <code>/etc/fstab</code> and the <code>netfs</code> service (RHEL6) or <code>remote-fs.target</code>service (RHEL7) is not enabled, the network file system (like NFS, SMB, and those on iSCSI) will not be mounted as expected during boot.</p>\n","plugin":"storage_netfs_without_netdev","error_key":"NETWORK_FILESYSTEM_NOT_MOUNT_ON_STARTUP","plugin_name":"Network file system fails to be mounted during bootup when the _netdev parameter is not set in the fstab file","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"stack_corrupted_using_xfs|STACK_CORRUPTED_USING_XFS","description":"Kernel panic when threads overrun or corrupt kernel stack with xfs in use.","category":"Stability","severity":"WARN","hitCount":0,"summary":"Kernel panic happens when the thread overran the stack or the stack was corrupted while running xfs due to a known bug in kernel.\n","summary_html":"<p>Kernel panic happens when the thread overran the stack or the stack was corrupted while running xfs due to a known bug in kernel.</p>\n","plugin":"stack_corrupted_using_xfs","error_key":"STACK_CORRUPTED_USING_XFS","plugin_name":"Kernel pacic when the thread overran the stack or the stack corrupted with xfs used.","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"setup_netdev_packetdrop_params|NETDEV_PACKET_DROPS_OBSERVED","description":"Network performance degradation when networking parameters are not properly tuned","category":"Performance","severity":"WARN","hitCount":0,"summary":"Dropped packets result in degraded network performance when networking parameters are not tuned for high throughput. \n","summary_html":"<p>Dropped packets result in degraded network performance when networking parameters are not tuned for high throughput. </p>\n","plugin":"setup_netdev_packetdrop_params","error_key":"NETDEV_PACKET_DROPS_OBSERVED","plugin_name":"Network performance degradation when networking parameters are not properly tuned","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"security_hacking_team_malware|HACKING_TEAM_MALWARE","description":"Compromised machine by Hacking Team malware","category":"Security","severity":"WARN","hitCount":0,"summary":"The Hacking Team is a malware producer whose software has been known to affect Linux machines. The intent is to perform remote monitoring functions such as keystroke logging, collecting emails, and listening to a device's microphone.","summary_html":"<p>The Hacking Team is a malware producer whose software has been known to affect Linux machines. The intent is to perform remote monitoring functions such as keystroke logging, collecting emails, and listening to a device&#39;s microphone.</p>\n","plugin":"security_hacking_team_malware","error_key":"HACKING_TEAM_MALWARE","plugin_name":"Compromised machine by Hacking Team malware","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"scsi_device_reset|SCSI_DEVICE_RESET","description":"System falls into hang state when SCSI commands fail to access the storage device","category":"Availability","severity":"WARN","hitCount":0,"summary":"System/Guest falls into a hang state when SCSI commands fail to access the storage device. It is indicated by the error log \"scsi device reset\" in the `/var/log/messages` file.\n","summary_html":"<p>System/Guest falls into a hang state when SCSI commands fail to access the storage device. It is indicated by the error log &quot;scsi device reset&quot; in the <code>/var/log/messages</code> file.</p>\n","plugin":"scsi_device_reset","error_key":"SCSI_DEVICE_RESET","plugin_name":"System falls into hang state when SCSI commands fail to access the storage device","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":1,"acked":false},{"rule_id":"sat_postgresql_deadlock_detected|SAT_UPDATE_DB_DEADLOCK_DETECTED","description":"Failure to synchronize content to Satellite due to deadlock in PostgreSQL when database needs cleaning","category":"Availability","severity":"WARN","hitCount":0,"summary":"Client communication to Satellite fails when PostgreSQL database contains too much dirty data and needs to be vacuumed.\n","summary_html":"<p>Client communication to Satellite fails when PostgreSQL database contains too much dirty data and needs to be vacuumed.</p>\n","plugin":"sat_postgresql_deadlock_detected","error_key":"SAT_UPDATE_DB_DEADLOCK_DETECTED","plugin_name":"Failure to synchronize content to Satellite due to deadlock in PostgreSQL when database needs cleaning","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"sat_low_disk_space_issue|SAT_LOW_DISK_SPACE_ISSUE","description":"Failure of critical Satellite services when the available disk space of Satellite partitions is too low","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the available disk space of Satellite partitions is full, some key services of Satellite will fail.\n","summary_html":"<p>When the available disk space of Satellite partitions is full, some key services of Satellite will fail.</p>\n","plugin":"sat_low_disk_space_issue","error_key":"SAT_LOW_DISK_SPACE_ISSUE","plugin_name":"Decreased stability and/or performance and key services of Satellite will fail when the available disk space of Satellite partitions is too low","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"sat_low_disk_space_issue|SAT_FILESYSTEM_CAPACITY","description":"Decreased stability and/or performance due to filesystem over 95% capacity","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the capacity of filesystems is over 95%, decreased stability and/or performance of the system can occur.\n","summary_html":"<p>When the capacity of filesystems is over 95%, decreased stability and/or performance of the system can occur.</p>\n","plugin":"sat_low_disk_space_issue","error_key":"SAT_FILESYSTEM_CAPACITY","plugin_name":"Decreased stability and/or performance and key services of Satellite will fail when the available disk space of Satellite partitions is too low","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"sat_eol_check|SAT_REACH_EOL_WARN","description":"Red Hat will discontinue technical support services as well as software maintenance services","category":"Availability","severity":"WARN","hitCount":0,"summary":"When Satellite reaches End Of Life - EOL (End of Maintenance Support 2), Red Hat will discontinue technical support services as well as software maintenance services.\n","summary_html":"<p>When Satellite reaches End Of Life - EOL (End of Maintenance Support 2), Red Hat will discontinue technical support services as well as software maintenance services.</p>\n","plugin":"sat_eol_check","error_key":"SAT_REACH_EOL_WARN","plugin_name":"Reminder notification of the End Of Life (EOL) for Satellite","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"sat_eol_check|SAT_REACH_EOL_INFO","description":"Red Hat will discontinue technical support services as well as software maintenance services after EOL","category":"Availability","severity":"WARN","hitCount":0,"summary":"When Satellite reaches End Of Life - EOL (End of Maintenance Support 2), Red Hat will discontinue technical support services as well as software maintenance services.\n","summary_html":"<p>When Satellite reaches End Of Life - EOL (End of Maintenance Support 2), Red Hat will discontinue technical support services as well as software maintenance services.</p>\n","plugin":"sat_eol_check","error_key":"SAT_REACH_EOL_INFO","plugin_name":"Reminder notification of the End Of Life (EOL) for Satellite","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"sat_eol_check|SAT_REACH_EOL_ERROR","description":"Red Hat has discontinued technical support services as well as software maintenance services","category":"Availability","severity":"WARN","hitCount":0,"summary":"When Satellite reaches End Of Life - EOL (End of Maintenance Support 2), Red Hat will discontinue technical support services as well as software maintenance services.\n","summary_html":"<p>When Satellite reaches End Of Life - EOL (End of Maintenance Support 2), Red Hat will discontinue technical support services as well as software maintenance services.</p>\n","plugin":"sat_eol_check","error_key":"SAT_REACH_EOL_ERROR","plugin_name":"Reminder notification of the End Of Life (EOL) for Satellite","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"sat6_virt_who_hyper_v_uuid_break|HYPER_V_VM_MAPPING_FAIL_WITH_UUID_BREAK","description":"Virt-who fails to report the host-guest mapping information of a Hyper-V hypervisor when VMs do not have uuid","category":"Availability","severity":"WARN","hitCount":0,"summary":"Virt-who fails to report the host-guest mapping information of a Hyper-V hypervisor to Satellite 6 when Hyper-V VMs do not have uuid which results in Hyper-V VMs failing to subscribe to Satellite 6.\n","summary_html":"<p>Virt-who fails to report the host-guest mapping information of a Hyper-V hypervisor to Satellite 6 when Hyper-V VMs do not have uuid which results in Hyper-V VMs failing to subscribe to Satellite 6.</p>\n","plugin":"sat6_virt_who_hyper_v_uuid_break","error_key":"HYPER_V_VM_MAPPING_FAIL_WITH_UUID_BREAK","plugin_name":"Virt-who fails to report the host-guest mapping information of a Hyper-V hypervisor when VMs do not have uuid","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_virt_who_ESX_hyper_issue|SAT_VIRTWHO_ESX_HYPER_WARN","description":"Failure to retrieve information from ESX hypervisors due to running unpatched virt-who on Satellite 6.","category":"Availability","severity":"WARN","hitCount":0,"summary":"The virt-who command has failed to report the host-guest mapping information of an ESX hypervisor to Satellite 6.\n","summary_html":"<p>The virt-who command has failed to report the host-guest mapping information of an ESX hypervisor to Satellite 6.</p>\n","plugin":"sat6_virt_who_ESX_hyper_issue","error_key":"SAT_VIRTWHO_ESX_HYPER_WARN","plugin_name":"The virt-who is not able to report the host-guest information of ESX hypervisor to the Satellite 6 server","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_virt_who_ESX_hyper_issue|SAT_VIRTWHO_ESX_HYPER_INFO","description":"Possible failure to retrieve information from ESX hypervisors when running unpatched virt-who on Satellite 6.","category":"Availability","severity":"WARN","hitCount":0,"summary":"The virt-who command will not be able to report the host-guest mapping information of an ESX hypervisor to Satellite 6 if the ESX hypervisor's domain name is not configured correctly.\n","summary_html":"<p>The virt-who command will not be able to report the host-guest mapping information of an ESX hypervisor to Satellite 6 if the ESX hypervisor&#39;s domain name is not configured correctly.</p>\n","plugin":"sat6_virt_who_ESX_hyper_issue","error_key":"SAT_VIRTWHO_ESX_HYPER_INFO","plugin_name":"The virt-who is not able to report the host-guest information of ESX hypervisor to the Satellite 6 server","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_tuned_adm|TUNED_NOT_BEST_PRACTICE","description":"Decreased performance or key services failure occurs when Satellite 6 is not running with recommended tuned service configuration","category":"Availability","severity":"WARN","hitCount":0,"summary":"Running Satellite 6 without recommended tuned service configuration causes decreased performance or key services failure.\n","summary_html":"<p>Running Satellite 6 without recommended tuned service configuration causes decreased performance or key services failure.</p>\n","plugin":"sat6_tuned_adm","error_key":"TUNED_NOT_BEST_PRACTICE","plugin_name":"Decreased performance or key services failure occurs when Satellite 6 is not running with recommended tuned service configuration","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"sat6_tmp_without_sticky_bit|SAT6_TMP_STICKY_BIT_LOST","description":"Satellite 6 Web UI is inaccessible when the permission of /tmp is not correct","category":"Availability","severity":"WARN","hitCount":0,"summary":"The permission of /tmp is not correct on Satellite 6 resulting in Satellite 6 WebUI becoming inaccessible.\n","summary_html":"<p>The permission of /tmp is not correct on Satellite 6 resulting in Satellite 6 WebUI becoming inaccessible.</p>\n","plugin":"sat6_tmp_without_sticky_bit","error_key":"SAT6_TMP_STICKY_BIT_LOST","plugin_name":"Satellite 6 Web UI is inaccessible when the permission of /tmp is not correct","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"sat6_task_duplicatekey_err|DUPLICATE_KEY_ERROR_OCCURRED","description":"UploadPackageProfile jobs failed when pulp scripts crashed in Satellite 6","category":"Availability","severity":"WARN","hitCount":0,"summary":"UploadPackageProfile jobs failed when pulp scripts crashed in Satellite 6.\n","summary_html":"<p>UploadPackageProfile jobs failed when pulp scripts crashed in Satellite 6.</p>\n","plugin":"sat6_task_duplicatekey_err","error_key":"DUPLICATE_KEY_ERROR_OCCURRED","plugin_name":"UploadPackageProfile jobs failed when pulp scripts crashed in Satellite 6","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"sat6_qpidd_qdrouterd_tune|SAT6_TUNE_QPIDD_QDROUTERD","description":"Decreased performance or key services failure occurs when Satellite 6 is running without recommended configuration of qpidd and qrouterd services","category":"Availability","severity":"WARN","hitCount":0,"summary":"Running Satellite 6 without recommended configuration of qpidd and qdrouterd services will cause decreased performance or key services failure.\n","summary_html":"<p>Running Satellite 6 without recommended configuration of qpidd and qdrouterd services will cause decreased performance or key services failure.</p>\n","plugin":"sat6_qpidd_qdrouterd_tune","error_key":"SAT6_TUNE_QPIDD_QDROUTERD","plugin_name":"Decreased performance or key services failure when Satellite is running without recommended configuration of qpidd and qdrouted services","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"sat6_qpidd_node_not_found|SAT6_QPIDD_NODE_NOT_FOUND","description":"Unable to update or manage clients due to missing pulp agent queue in Satellite 6","category":"Availability","severity":"WARN","hitCount":0,"summary":"If the pulp agent queue (pulp.agent.UUID) for a client does not exist, the client cannot receive updates or notifications. This will prevent the Satellite server from managing and updating the client.\n\n","summary_html":"<p>If the pulp agent queue (pulp.agent.UUID) for a client does not exist, the client cannot receive updates or notifications. This will prevent the Satellite server from managing and updating the client.</p>\n","plugin":"sat6_qpidd_node_not_found","error_key":"SAT6_QPIDD_NODE_NOT_FOUND","plugin_name":"Unable to update or manage clients due to missing pulp agent queue in Satellite 6","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_qdrouterd_0_4_19_segfault|QDROUTERD_0_4_19_SEGFAULT","description":"Satellite 6 client operations fail when a segfault occurs in qdrouterd due to a bug in qpid-dispatch-router","category":"Availability","severity":"WARN","hitCount":0,"summary":"A segfault occurs in qdrouterd service causing clients of Satellite 6 to fail to apply an errata or a package via katello-agent.\n","summary_html":"<p>A segfault occurs in qdrouterd service causing clients of Satellite 6 to fail to apply an errata or a package via katello-agent.</p>\n","plugin":"sat6_qdrouterd_0_4_19_segfault","error_key":"QDROUTERD_0_4_19_SEGFAULT","plugin_name":"Satellite 6 client operations fail when a segfault occurs in qdrouterd due to a bug in qpid-dispatch-router","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_pulp_tasks_waiting_but_pulp_idle|SAT6_PULP_TASKS_WAITING_BUT_PULP_IDLE","description":"Pulp will become inactive in Satellite 6 with a huge amount of celery workers when PULP_MAX_TASKS_PER_CHILD is set","category":"Stability","severity":"WARN","hitCount":0,"summary":"When the PULP_MAX_TASKS_PER_CHILD option is set in /etc/default/pulp_workers, pulp attempts to start multiple workers and shares with each worker its MongoDB connection. However, the MongoDB connection is not designed to be shared in this way and a silent deadlock can occur, causing the pulp tasks to stop responding.\n","summary_html":"<p>When the PULP_MAX_TASKS_PER_CHILD option is set in /etc/default/pulp_workers, pulp attempts to start multiple workers and shares with each worker its MongoDB connection. However, the MongoDB connection is not designed to be shared in this way and a silent deadlock can occur, causing the pulp tasks to stop responding.</p>\n","plugin":"sat6_pulp_tasks_waiting_but_pulp_idle","error_key":"SAT6_PULP_TASKS_WAITING_BUT_PULP_IDLE","plugin_name":"Pulp becomes inactive in Satellite 6 when PULP_MAX_TASKS_PER_CHILD is set","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"sat6_pulp_tasks_waiting_but_pulp_idle|SAT6_PULP_CONFIGURED_FOR_MANY_TASKS","description":"Pulp will become inactive in Satellite 6 when PULP_MAX_TASKS_PER_CHILD is set","category":"Stability","severity":"WARN","hitCount":0,"summary":"When the PULP_MAX_TASKS_PER_CHILD option is set in /etc/default/pulp_workers, pulp attempts to start multiple workers and shares with each worker its MongoDB connection. However, the MongoDB connection is not designed to be shared in this way and a silent deadlock can occur, causing the pulp tasks to stop responding.\n","summary_html":"<p>When the PULP_MAX_TASKS_PER_CHILD option is set in /etc/default/pulp_workers, pulp attempts to start multiple workers and shares with each worker its MongoDB connection. However, the MongoDB connection is not designed to be shared in this way and a silent deadlock can occur, causing the pulp tasks to stop responding.</p>\n","plugin":"sat6_pulp_tasks_waiting_but_pulp_idle","error_key":"SAT6_PULP_CONFIGURED_FOR_MANY_TASKS","plugin_name":"Pulp becomes inactive in Satellite 6 when PULP_MAX_TASKS_PER_CHILD is set","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"sat6_goferd_amqps_conn_fail|SAT_GOFERD_AMQPS_CONN_FAIL","description":"Operations failure on Satellite client when connection error to Satellite 6 server occurs in goferd due to a bug in satellite-tools","category":"Availability","severity":"WARN","hitCount":0,"summary":"Operations fail on Satellite client when connection error to Satellite 6 server occurs in goferd due to a bug in satellite-tools.\n","summary_html":"<p>Operations fail on Satellite client when connection error to Satellite 6 server occurs in goferd due to a bug in satellite-tools.</p>\n","plugin":"sat6_goferd_amqps_conn_fail","error_key":"SAT_GOFERD_AMQPS_CONN_FAIL","plugin_name":"Operations failure on Satellite client when connection error to Satellite 6 server occurs in goferd, due to a bug in satellite-tools","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_candelpin_logs_selinux_logrotate_fail|CANDLEPIN_ROTATE_FAIL_ERROR","description":"Logs under /var/log/candlepin/ cannot be rotated normally with SELinux enabled which results in low available disk space, causing decreased stability and performance","category":"Availability","severity":"WARN","hitCount":0,"summary":"Log files under /var/log/candlepin/ cannot be normally rotated with SELinux enabled due to a known bug. This results in decreased stability and performance due to low available disk space.\n","summary_html":"<p>Log files under /var/log/candlepin/ cannot be normally rotated with SELinux enabled due to a known bug. This results in decreased stability and performance due to low available disk space.</p>\n","plugin":"sat6_candelpin_logs_selinux_logrotate_fail","error_key":"CANDLEPIN_ROTATE_FAIL_ERROR","plugin_name":"Decreased stability and performance when the available disk space is low in Satellite 6 due to logs under /var/log/candlepin/ cannot be normally rotated with SELinux enabled","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat6_basic_check|SAT6_BASIC_SYSTEM_REQUIREMENT_LOST","description":"Decreased stability and performance due to insufficient basic system requirements in Satellite 6","category":"Availability","severity":"WARN","hitCount":1,"summary":"Basic system requirements for Satellite 6 are not met, which results in decreased stability and performance, or even key services failures.\n","summary_html":"<p>Basic system requirements for Satellite 6 are not met, which results in decreased stability and performance, or even key services failures.</p>\n","plugin":"sat6_basic_check","error_key":"SAT6_BASIC_SYSTEM_REQUIREMENT_LOST","plugin_name":"Decreased stability and performance due to insufficient basic system requirements in Satellite 6","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"sat6_apache_tune|SAT6_TUNE_APACHE","description":"Decreased performance or key services failure when Satellite 6 is running without recommended Apache service configuration","category":"Availability","severity":"WARN","hitCount":0,"summary":"Running Satellite 6 without recommended Apache service configuration causes decreased performance or key services failure.\n","summary_html":"<p>Running Satellite 6 without recommended Apache service configuration causes decreased performance or key services failure.</p>\n","plugin":"sat6_apache_tune","error_key":"SAT6_TUNE_APACHE","plugin_name":"Decreased performance or key services failure when Satellite 6 is running without recommended Apache service configuration","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"sat5_taskomatic_running|SAT5_TASKOMATIC_RUNNING","description":"Newly synced content will not be available to clients due to taskomatic service not running","category":"Availability","severity":"WARN","hitCount":0,"summary":"The Taskomatic service is required, for example, for repodata regeneration or calculation of applicable errata for Red Hat Satellite.\n","summary_html":"<p>The Taskomatic service is required, for example, for repodata regeneration or calculation of applicable errata for Red Hat Satellite.</p>\n","plugin":"sat5_taskomatic_running","error_key":"SAT5_TASKOMATIC_RUNNING","plugin_name":"Newly synced content will not be available to clients due to taskomatic service not running","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sat5_osad_duplicate_clients|OSAD_DUPLICATE_CLIENTS","description":"Decreased performance when clients with duplicate OSAD IDs connect to the Satellite server","category":"Performance","severity":"WARN","hitCount":0,"summary":"When multiple clients using a common OSAD ID connect to a Satellite 5 system, the Satellite server can become overloaded and deadlock the database.\n","summary_html":"<p>When multiple clients using a common OSAD ID connect to a Satellite 5 system, the Satellite server can become overloaded and deadlock the database.</p>\n","plugin":"sat5_osad_duplicate_clients","error_key":"OSAD_DUPLICATE_CLIENTS","plugin_name":"Decreased performance when clients with duplicate OSAD IDs connect to the Satellite server","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":4,"acked":false},{"rule_id":"sat5_kickstart_profile_ISE_issue|SAT5_KICKSTART_PROFILE_ISE_WARN","description":"Kickstart profile page inaccessible in Satellite web UI due to incorrect configuration.","category":"Availability","severity":"WARN","hitCount":0,"summary":"Kickstart profile page inaccessible in web UI due to incorrect configuration.\n","summary_html":"<p>Kickstart profile page inaccessible in web UI due to incorrect configuration.</p>\n","plugin":"sat5_kickstart_profile_ISE_issue","error_key":"SAT5_KICKSTART_PROFILE_ISE_WARN","plugin_name":"Internal Server Error when accessing kickstarts in Satellite web UI","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"sat5_kickstart_profile_ISE_issue|SAT5_KICKSTART_PROFILE_ISE_ERROR","description":"Kickstart profile page inaccessible in the Satellite web UI due to incorrect configuration.","category":"Availability","severity":"WARN","hitCount":0,"summary":"Kickstart profile page inaccessible in web UI due to incorrect configuration.\n","summary_html":"<p>Kickstart profile page inaccessible in web UI due to incorrect configuration.</p>\n","plugin":"sat5_kickstart_profile_ISE_issue","error_key":"SAT5_KICKSTART_PROFILE_ISE_ERROR","plugin_name":"Internal Server Error when accessing kickstarts in Satellite web UI","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"sat5_cert_expire|SAT5_CERT_RESTRICTED","description":"Satellite 5 will be disabled because subscription certificate has expired (in restricted period)","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the Satellite 5 subscription certificate has expired for more than 7 days and less than 31 days (in restricted period), the server will no longer be able to download updates from the Red Hat CDN and clients of the Satellite server will stop receiving updates.\n","summary_html":"<p>When the Satellite 5 subscription certificate has expired for more than 7 days and less than 31 days (in restricted period), the server will no longer be able to download updates from the Red Hat CDN and clients of the Satellite server will stop receiving updates.</p>\n","plugin":"sat5_cert_expire","error_key":"SAT5_CERT_RESTRICTED","plugin_name":"Satellite 5 subscription certificate has expired","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"sat5_cert_expire|SAT5_CERT_INACTIVE","description":"Satellite 5 is disabled because subscription certificate has expired for more than one month","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the Satellite 5 subscription certificate has expired and Satellite is disabled, the server is no longer be able to download updates from the Red Hat CDN and clients of the Satellite server stop receive updates.\n","summary_html":"<p>When the Satellite 5 subscription certificate has expired and Satellite is disabled, the server is no longer be able to download updates from the Red Hat CDN and clients of the Satellite server stop receive updates.</p>\n","plugin":"sat5_cert_expire","error_key":"SAT5_CERT_INACTIVE","plugin_name":"Satellite 5 subscription certificate has expired","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"sat5_cert_expire|SAT5_CERT_GRACE_PERIOD","description":"Satellite 5 will be disabled because subscription certificate has expired (in grace period)","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the Satellite 5 subscription certificate has expired for less than 7 days (in grace period), the server will no longer be able to download updates from the Red Hat CDN and clients of the Satellite server will stop receiving updates.\n","summary_html":"<p>When the Satellite 5 subscription certificate has expired for less than 7 days (in grace period), the server will no longer be able to download updates from the Red Hat CDN and clients of the Satellite server will stop receiving updates.</p>\n","plugin":"sat5_cert_expire","error_key":"SAT5_CERT_GRACE_PERIOD","plugin_name":"Satellite 5 subscription certificate has expired","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"sat56_upgrade_database_space|SATELLITE_5_NOT_ENOUGH_SPACE_TO_UPGRADE","description":"Database migration will fail when upgrading to Satellite 5.8 due to insufficient space","category":"Availability","severity":"WARN","hitCount":0,"summary":"Satellite 5.6 and 5.7 will be EOL (End Of Life) on Jan 31, 2019, so it is necessary to upgrade to Satellite 5.8 before that. Database migration will fail when upgrading to Satellite 5.8 due to insufficient space.\n","summary_html":"<p>Satellite 5.6 and 5.7 will be EOL (End Of Life) on Jan 31, 2019, so it is necessary to upgrade to Satellite 5.8 before that. Database migration will fail when upgrading to Satellite 5.8 due to insufficient space.</p>\n","plugin":"sat56_upgrade_database_space","error_key":"SATELLITE_5_NOT_ENOUGH_SPACE_TO_UPGRADE","plugin_name":"Database migration will fail when upgrading to Satellite 5.8 due to insufficient space","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"sap_tuned_netweaver|SAP_TUNED_DISABLED","description":"Decreased application performance when not running sap-netweaver tuned profile with SAP applications","category":"Performance","severity":"WARN","hitCount":0,"summary":"Application performance decreased when not running sap-netweaver tuned profile with SAP applications.\n","summary_html":"<p>Application performance decreased when not running sap-netweaver tuned profile with SAP applications.</p>\n","plugin":"sap_tuned_netweaver","error_key":"SAP_TUNED_DISABLED","plugin_name":"Decreased application performance when not running sap-netweaver tuned profile with SAP applications","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"sap_limits|SAP_LIMITS_UNCONFIGURED","description":"SAP application performance degradation when file handler limits do not meet SAP requirements","category":"Performance","severity":"WARN","hitCount":0,"summary":"SAP applications require certain file handler in order to keep the applications running properly. When file handler limits do not meet SAP requirements, the application performance degrades.\n","summary_html":"<p>SAP applications require certain file handler in order to keep the applications running properly. When file handler limits do not meet SAP requirements, the application performance degrades.</p>\n","plugin":"sap_limits","error_key":"SAP_LIMITS_UNCONFIGURED","plugin_name":"SAP application performance degradation when file handler limits do not meet SAP requirements","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"sap_kernel_params|SAP_KERNEL_PARAMS_INCORRECT","description":"Decreased SAP application performance when using incorrect kernel parameters","category":"Performance","severity":"WARN","hitCount":0,"summary":"SAP application performance decreased when incompatible kernel parameters are used.\n","summary_html":"<p>SAP application performance decreased when incompatible kernel parameters are used.</p>\n","plugin":"sap_kernel_params","error_key":"SAP_KERNEL_PARAMS_INCORRECT","plugin_name":"Decreased SAP application performance when using incorrect kernel parameters","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"sanity_check_fstab|FSTAB_MISC_ERRORS","description":"Devices will not be mounted properly when /etc/fstab is misconfigured","category":"Availability","severity":"WARN","hitCount":0,"summary":"System reboot fails or devices fail to be mounted when /etc/fstab is missing or misconfigured. \n","summary_html":"<p>System reboot fails or devices fail to be mounted when /etc/fstab is missing or misconfigured. </p>\n","plugin":"sanity_check_fstab","error_key":"FSTAB_MISC_ERRORS","plugin_name":"System reboot fails or devices fail to be mounted when /etc/fstab is missing or misconfigured","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"sanity_check_fstab|FSTAB_IS_EMPTY","description":"System will not reboot successfully when /etc/fstab is misconfigured or missing","category":"Availability","severity":"WARN","hitCount":0,"summary":"System reboot fails or devices fail to be mounted when /etc/fstab is missing or misconfigured. \n","summary_html":"<p>System reboot fails or devices fail to be mounted when /etc/fstab is missing or misconfigured. </p>\n","plugin":"sanity_check_fstab","error_key":"FSTAB_IS_EMPTY","plugin_name":"System reboot fails or devices fail to be mounted when /etc/fstab is missing or misconfigured","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"rx_packet_drops|RX_PACKET_DROPS","description":"Interfaces dropping packets when ring buffer is fill up","category":"Performance","severity":"WARN","hitCount":0,"summary":"Interfaces dropping packets when ring buffer is fill up.\n","summary_html":"<p>Interfaces dropping packets when ring buffer is fill up.</p>\n","plugin":"rx_packet_drops","error_key":"RX_PACKET_DROPS","plugin_name":"Interfaces dropping packets when ring buffer is fill up","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"rx_crc_errors|RX_CRC_ERRORS","description":"Network interfaces get RX CRC errors and packet loss due to some hardware problems","category":"Performance","severity":"WARN","hitCount":0,"summary":"Network interfaces get large numbers of RX Cycle Redundancy Check errors (CRC errors) and packet loss due to some hardware problems on network, cable, switch or NIC.\n","summary_html":"<p>Network interfaces get large numbers of RX Cycle Redundancy Check errors (CRC errors) and packet loss due to some hardware problems on network, cable, switch or NIC.</p>\n","plugin":"rx_crc_errors","error_key":"RX_CRC_ERRORS","plugin_name":"Network interfaces get RX CRC errors and packet loss due to some hardware problems","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"rsyslogd_shutdown_crash|RSYSLOGD_SHUTDOWN_CRASH","description":"Rsyslogd crashing during shutdown when regex module is used with other external modules","category":"Stability","severity":"WARN","hitCount":0,"summary":"Rsyslogd crashing during shutdown when regex module is used with other external modules.\n","summary_html":"<p>Rsyslogd crashing during shutdown when regex module is used with other external modules.</p>\n","plugin":"rsyslogd_shutdown_crash","error_key":"RSYSLOGD_SHUTDOWN_CRASH","plugin_name":"Rsyslogd crashing during shutdown when regex module is used with other external modules","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"rport_delete_crash|RPORT_DELETE_CRASH","description":"Kernel panic on SCSI host removal","category":"Stability","severity":"WARN","hitCount":0,"summary":"In the Red Hat Enterprise Linux 6.6 kernel, a bug was introduced whereby a kernel panic might occur on fiber channel SCSI host removal.","summary_html":"<p>In the Red Hat Enterprise Linux 6.6 kernel, a bug was introduced whereby a kernel panic might occur on fiber channel SCSI host removal.</p>\n","plugin":"rport_delete_crash","error_key":"RPORT_DELETE_CRASH","plugin_name":"Kernel panic on SCSI host removal","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"rpm_database_problems|RPM_DATABASE_PROBLEMS_V1","description":"The rpm commands break after update nss-softokn to version 3.14.3-19.el6_6.","category":"Availability","severity":"WARN","hitCount":0,"summary":"Update to nss-softokn-3.14.3-19 can break RPM operations.","summary_html":"<p>Update to nss-softokn-3.14.3-19 can break RPM operations.</p>\n","plugin":"rpm_database_problems","error_key":"RPM_DATABASE_PROBLEMS_V1","plugin_name":"The rpm commands break after update nss-softokn to version 3.14.3-19.el6_6.","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"rpmdb_corrupt|RPMDB_CORRUPT","description":"RPM database corruption","category":"Stability","severity":"WARN","hitCount":0,"summary":"The `rpmdbNextIterator` error occurs in the `rpm -qa` command output, which suggests a RPM database corruption.","summary_html":"<p>The <code>rpmdbNextIterator</code> error occurs in the <code>rpm -qa</code> command output, which suggests a RPM database corruption.</p>\n","plugin":"rpmdb_corrupt","error_key":"RPMDB_CORRUPT","plugin_name":"RPM database corruption","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"root_pv_is_filtered_out|ROOT_PV_IS_FILTERED_OUT_V4","description":"Boot failure when root PV is filtered out","category":"Availability","severity":"WARN","hitCount":0,"summary":"If the `/etc/lvm/lvm.conf` file filters out PVs that are part of the VG hosting the root LV, it will cause the system not to boot successfully.\n","summary_html":"<p>If the <code>/etc/lvm/lvm.conf</code> file filters out PVs that are part of the VG hosting the root LV, it will cause the system not to boot successfully.</p>\n","plugin":"root_pv_is_filtered_out","error_key":"ROOT_PV_IS_FILTERED_OUT_V4","plugin_name":"Boot failure when root PV is filtered out","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":4,"acked":false},{"rule_id":"rhn_twilight_check|RHN_TWILIGHT_ALERT","description":"Systems registered to RHN will no longer be able to update after 4 April 2018","category":"Availability","severity":"WARN","hitCount":0,"summary":"Red Hat will shut RHN down on 4 April 2018 so all systems registered to RHN Classic will no longer be able to update after 4 April 2018.\n","summary_html":"<p>Red Hat will shut RHN down on 4 April 2018 so all systems registered to RHN Classic will no longer be able to update after 4 April 2018.</p>\n","plugin":"rhn_twilight_check","error_key":"RHN_TWILIGHT_ALERT","plugin_name":"Systems registered to RHN will no longer be able to update after 4 April 2018","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"rhev_unable_to_connect_to_vm_serial_console_over_ssh|UNABLE_TO_CONNECT_TO_VM_SERIAL_CONSOLE","description":"Failure to connect to VM serial console via SSH when certificates are not enrolled","category":"Availability","severity":"WARN","hitCount":0,"summary":"SSH connections to VM serial console fail when certificates in /etc/pki/ovirt-vmconsole/ are missing.\n","summary_html":"<p>SSH connections to VM serial console fail when certificates in /etc/pki/ovirt-vmconsole/ are missing.</p>\n","plugin":"rhev_unable_to_connect_to_vm_serial_console_over_ssh","error_key":"UNABLE_TO_CONNECT_TO_VM_SERIAL_CONSOLE","plugin_name":"Failure to connect to VM serial console via SSH when certificates are not enrolled","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"rhev_lro_on|RHEV_BRIDGE_LRO_ON_ERROR","description":"Loss of network connectivity when LRO is enabled on a host in RHV environment","category":"Stability","severity":"WARN","hitCount":0,"summary":"Due to a kernel bug Large Receive Offload (LRO) flag disabling is not being propagated downwards from above devices in vlan and bond hierarchy, breaking the flow of traffic.\n","summary_html":"<p>Due to a kernel bug Large Receive Offload (LRO) flag disabling is not being propagated downwards from above devices in vlan and bond hierarchy, breaking the flow of traffic.</p>\n","plugin":"rhev_lro_on","error_key":"RHEV_BRIDGE_LRO_ON_ERROR","plugin_name":"Loss of network connectivity when LRO is enabled on a host in RHV environment","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"rename_network_interface|RENAME_NETWORK_INTERFACE_WARN","description":"Network connectivity loss when setting extra whitespaces in network configuration files (this issue may happen in the future)","category":"Availability","severity":"WARN","hitCount":0,"summary":"Network connectivity loss when setting extra whitespaces after the final double-quote character (`\"`) in the line of NIC device name in network configuration files (`/etc/sysconfig/network-scripts/ifcfg*`) and using initscripts package version prior to `initscripts-9.03.53-1.el6.x86_64`.\n","summary_html":"<p>Network connectivity loss when setting extra whitespaces after the final double-quote character (<code>&quot;</code>) in the line of NIC device name in network configuration files (<code>/etc/sysconfig/network-scripts/ifcfg*</code>) and using initscripts package version prior to <code>initscripts-9.03.53-1.el6.x86_64</code>.</p>\n","plugin":"rename_network_interface","error_key":"RENAME_NETWORK_INTERFACE_WARN","plugin_name":"Network connectivity loss when setting extra whitespaces in network configuration files","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"rename_network_interface|RENAME_NETWORK_INTERFACE_ERROR","description":"Network connectivity loss when setting extra whitespaces in network configuration files (the issue has happened)","category":"Availability","severity":"WARN","hitCount":0,"summary":"Network connectivity loss when setting extra whitespaces after the final double-quote character (`\"`) in the line of NIC device name in network configuration files (`/etc/sysconfig/network-scripts/ifcfg*`) and using initscripts package version prior to `initscripts-9.03.53-1.el6.x86_64`.\n","summary_html":"<p>Network connectivity loss when setting extra whitespaces after the final double-quote character (<code>&quot;</code>) in the line of NIC device name in network configuration files (<code>/etc/sysconfig/network-scripts/ifcfg*</code>) and using initscripts package version prior to <code>initscripts-9.03.53-1.el6.x86_64</code>.</p>\n","plugin":"rename_network_interface","error_key":"RENAME_NETWORK_INTERFACE_ERROR","plugin_name":"Network connectivity loss when setting extra whitespaces in network configuration files","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"rabbitmq_queue_size|RABBITMQ_MSG_THRESHOLD","description":"OpenStack operations are slow due to growing RabbitMQ message queues without consumers","category":"Stability","severity":"WARN","hitCount":0,"summary":"In an OpenStack environment, if rabbmitmq message consumers go down, the rabbitmq queue can continue to grow even if there is no message consumer.","summary_html":"<p>In an OpenStack environment, if rabbmitmq message consumers go down, the rabbitmq queue can continue to grow even if there is no message consumer.</p>\n","plugin":"rabbitmq_queue_size","error_key":"RABBITMQ_MSG_THRESHOLD","plugin_name":"OpenStack operations are slow due to growing RabbitMQ message queues without consumers","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"rabbitmq_file_limit|RABBITMQ_FILE_DESCRIPTOR_LIMIT_SYMPTOM","description":"OpenStack tasks failure when RabbitMQ reached file descriptor limit (this issue has happened)","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the RabbitMQ server hits its file descriptor limit, it cannot open new file descriptors to process more messages and refuses those connections. As a result, the associated OpenStack tasks fail.\n","summary_html":"<p>When the RabbitMQ server hits its file descriptor limit, it cannot open new file descriptors to process more messages and refuses those connections. As a result, the associated OpenStack tasks fail.</p>\n","plugin":"rabbitmq_file_limit","error_key":"RABBITMQ_FILE_DESCRIPTOR_LIMIT_SYMPTOM","plugin_name":"OpenStack tasks failure when RabbitMQ reached file descriptor limit","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"rabbitmq_file_limit|RABBITMQ_FILE_DESCRIPTOR_LIMIT","description":"OpenStack tasks failure when RabbitMQ reached file descriptor limit (this issue may happen in the future)","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the RabbitMQ server hits its file descriptor limit, it cannot open new file descriptors to process more messages and refuses those connections. As a result, the associated OpenStack tasks fail.\n","summary_html":"<p>When the RabbitMQ server hits its file descriptor limit, it cannot open new file descriptors to process more messages and refuses those connections. As a result, the associated OpenStack tasks fail.</p>\n","plugin":"rabbitmq_file_limit","error_key":"RABBITMQ_FILE_DESCRIPTOR_LIMIT","plugin_name":"OpenStack tasks failure when RabbitMQ reached file descriptor limit","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"rabbitmq_cluster_new_users|RABBITMQ_CLUSTER_NEW_USERS","description":"Manually created RabbitMQ users are lost after RabbitMQ resource is restarted by pacemaker","category":"Stability","severity":"WARN","hitCount":0,"summary":"Manually created RabbitMQ users and their associated user permissions are automatically deleted after RabbitMQ resource restarts.\n","summary_html":"<p>Manually created RabbitMQ users and their associated user permissions are automatically deleted after RabbitMQ resource restarts.</p>\n","plugin":"rabbitmq_cluster_new_users","error_key":"RABBITMQ_CLUSTER_NEW_USERS","plugin_name":"Manually created RabbitMQ users are lost after RabbitMQ resource is restarted by pacemaker","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"qlcnic_old_firmware|QLCNIC_OLD_FIRMWARE","description":"Kernel panic when running qlcnic firmware versions earlier than 4.12.00","category":"Stability","severity":"WARN","hitCount":0,"summary":"Kernel panic when running qlcnic firmware versions earlier than 4.12.00.\n","summary_html":"<p>Kernel panic when running qlcnic firmware versions earlier than 4.12.00.</p>\n","plugin":"qlcnic_old_firmware","error_key":"QLCNIC_OLD_FIRMWARE","plugin_name":"Kernel panic when running qlcnic firmware versions earlier than 4.12.00","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"qlcnic_driver_crash|QLCNIC_DRIVER_CRASH","description":"System crash when running ethtool against NIC with qlcnic driver","category":"Availability","severity":"WARN","hitCount":0,"summary":"Running \"ethtool -S\" or \"sosreport\" against **HP NC523SFP** or **HP NC526FLR** adapters with the qlcnic driver causes the system to crash due to a known bug in the qlcnic driver.\n","summary_html":"<p>Running &quot;ethtool -S&quot; or &quot;sosreport&quot; against <strong>HP NC523SFP</strong> or <strong>HP NC526FLR</strong> adapters with the qlcnic driver causes the system to crash due to a known bug in the qlcnic driver.</p>\n","plugin":"qlcnic_driver_crash","error_key":"QLCNIC_DRIVER_CRASH","plugin_name":"System crash when running ethtool against NIC with qlcnic driver","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"qla2xxx_request_stuck|QLA2XXX_REQUEST_STUCK","description":"Request queued and never completes due to qla2xxx race condition during port loss","category":"Stability","severity":"WARN","hitCount":0,"summary":"There is a possible race condition in the qla2xxx driver when marking multiple ports as lost can result in an inconsistent state between the SCSI layer and the qla2xxx driver.","summary_html":"<p>There is a possible race condition in the qla2xxx driver when marking multiple ports as lost can result in an inconsistent state between the SCSI layer and the qla2xxx driver.</p>\n","plugin":"qla2xxx_request_stuck","error_key":"QLA2XXX_REQUEST_STUCK","plugin_name":"Request queued and never completes due to qla2xxx race condition during port loss","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"postgres_max_connections|PGSQL_MAX_CONNECTIONS_WARN","description":"PostgreSQL connection is refused more than 5000 times per day because connection limit is exceeded in PostgreSQL service","category":"Performance","severity":"WARN","hitCount":0,"summary":"Connections are refused because the number of concurrent connections to PostgreSQL exceeds the **max_connections** value in `/var/lib/pgsql/data/postgresql.conf`.\n","summary_html":"<p>Connections are refused because the number of concurrent connections to PostgreSQL exceeds the <strong>max_connections</strong> value in <code>/var/lib/pgsql/data/postgresql.conf</code>.</p>\n","plugin":"postgres_max_connections","error_key":"PGSQL_MAX_CONNECTIONS_WARN","plugin_name":"Connections to PostgreSQL database fail due to max conncetion exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"postgres_max_connections|PGSQL_MAX_CONNECTIONS_SATELLITE_WARN","description":"PostgreSQL connection is refused more than 5000 times per day when connection limit is exceeded in Satellite's PostgreSQL service","category":"Performance","severity":"WARN","hitCount":0,"summary":"Connections are refused because the number of concurrent connections to PostgreSQL exceeds the **max_connections** value in `/var/lib/pgsql/data/postgresql.conf`.\n","summary_html":"<p>Connections are refused because the number of concurrent connections to PostgreSQL exceeds the <strong>max_connections</strong> value in <code>/var/lib/pgsql/data/postgresql.conf</code>.</p>\n","plugin":"postgres_max_connections","error_key":"PGSQL_MAX_CONNECTIONS_SATELLITE_WARN","plugin_name":"Connections to PostgreSQL database fail due to max conncetion exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"postgres_max_connections|PGSQL_MAX_CONNECTIONS_SATELLITE_INFO","description":"PostgreSQL connection is refused more than 1000 times per day because connection limit is exceeded in Satellite's PostgreSQL service","category":"Performance","severity":"WARN","hitCount":0,"summary":"Connections are refused because the number of concurrent connections to PostgreSQL exceeds the **max_connections** value in `/var/lib/pgsql/data/postgresql.conf`.\n","summary_html":"<p>Connections are refused because the number of concurrent connections to PostgreSQL exceeds the <strong>max_connections</strong> value in <code>/var/lib/pgsql/data/postgresql.conf</code>.</p>\n","plugin":"postgres_max_connections","error_key":"PGSQL_MAX_CONNECTIONS_SATELLITE_INFO","plugin_name":"Connections to PostgreSQL database fail due to max conncetion exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"postgres_max_connections|PGSQL_MAX_CONNECTIONS_RHVM_WARN","description":"PostgreSQL connection is refused more than 5000 times per day because connection limit is exceeded in RHV-M's PostgreSQL service","category":"Performance","severity":"WARN","hitCount":0,"summary":"Connections are refused because the number of concurrent connections to PostgreSQL exceeds the **max_connections** value in `/var/lib/pgsql/data/postgresql.conf`.\n","summary_html":"<p>Connections are refused because the number of concurrent connections to PostgreSQL exceeds the <strong>max_connections</strong> value in <code>/var/lib/pgsql/data/postgresql.conf</code>.</p>\n","plugin":"postgres_max_connections","error_key":"PGSQL_MAX_CONNECTIONS_RHVM_WARN","plugin_name":"Connections to PostgreSQL database fail due to max conncetion exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"postgres_max_connections|PGSQL_MAX_CONNECTIONS_RHVM_INFO","description":"PostgreSQL connection is refused more than 1000 times per day because connection limit is exceeded in RHV-M's PostgreSQL service","category":"Performance","severity":"WARN","hitCount":0,"summary":"Connections are refused because the number of concurrent connections to PostgreSQL exceeds the **max_connections** value in `/var/lib/pgsql/data/postgresql.conf`.\n","summary_html":"<p>Connections are refused because the number of concurrent connections to PostgreSQL exceeds the <strong>max_connections</strong> value in <code>/var/lib/pgsql/data/postgresql.conf</code>.</p>\n","plugin":"postgres_max_connections","error_key":"PGSQL_MAX_CONNECTIONS_RHVM_INFO","plugin_name":"Connections to PostgreSQL database fail due to max conncetion exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"postgres_max_connections|PGSQL_MAX_CONNECTIONS_INFO","description":"PostgreSQL connection is refused more than 1000 times per day because connection limit is exceeded in PostgreSQL service","category":"Performance","severity":"WARN","hitCount":0,"summary":"Connections are refused because the number of concurrent connections to PostgreSQL exceeds the **max_connections** value in `/var/lib/pgsql/data/postgresql.conf`.\n","summary_html":"<p>Connections are refused because the number of concurrent connections to PostgreSQL exceeds the <strong>max_connections</strong> value in <code>/var/lib/pgsql/data/postgresql.conf</code>.</p>\n","plugin":"postgres_max_connections","error_key":"PGSQL_MAX_CONNECTIONS_INFO","plugin_name":"Connections to PostgreSQL database fail due to max conncetion exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"postgres_frequent_checkpoints|PGSQL_FREQUENT_CHECKPOINTS","description":"Decreased PostgreSQL database performance when checkpoints are occurring too frequently","category":"Performance","severity":"WARN","hitCount":0,"summary":"PostgreSQL engine is performing checkpoints too frequently, resulting in decreased database performance.\n","summary_html":"<p>PostgreSQL engine is performing checkpoints too frequently, resulting in decreased database performance.</p>\n","plugin":"postgres_frequent_checkpoints","error_key":"PGSQL_FREQUENT_CHECKPOINTS","plugin_name":"Decreased PostgreSQL database performance when checkpoints are occurring too frequently","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"pcs_systemd|PACEMAKER_SYSTEMD_BUG","description":"Pacemaker fails to manage systemd resources when using specific version of pacemaker","category":"Stability","severity":"WARN","hitCount":0,"summary":"Earlier versions of Pacemaker are incompatible with systemd. This issue causes systemd resource management to fail and the managed resources to become unresponsive.","summary_html":"<p>Earlier versions of Pacemaker are incompatible with systemd. This issue causes systemd resource management to fail and the managed resources to become unresponsive.</p>\n","plugin":"pcs_systemd","error_key":"PACEMAKER_SYSTEMD_BUG","plugin_name":"Pacemaker fails to manage systemd resources when using specific version of pacemaker","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"panic_be2net_driver|PANIC_BE2NET_DRIVER","description":"Kernel panic when using an Emulex network interface with GRO enabled in be2net driver","category":"Stability","severity":"WARN","hitCount":0,"summary":"Emulex network interface card using a be2net driver and setting GRO to \"on\" can lead to kernel panic.","summary_html":"<p>Emulex network interface card using a be2net driver and setting GRO to &quot;on&quot; can lead to kernel panic.</p>\n","plugin":"panic_be2net_driver","error_key":"PANIC_BE2NET_DRIVER","plugin_name":"Kernel panic when using an Emulex network interface with GRO enabled in be2net driver","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"panic_anon_vma_degree|PANIC_ANON_VMA_DEGREE_NUMA","description":"Kernel panic in unlink_anon_vmas() on a NUMA server","category":"Stability","severity":"WARN","hitCount":0,"summary":"On a NUMA server, a known issue has been found in the unlink_anon_vma() function of the kernel, which could lead to a kernel panic.","summary_html":"<p>On a NUMA server, a known issue has been found in the unlink_anon_vma() function of the kernel, which could lead to a kernel panic.</p>\n","plugin":"panic_anon_vma_degree","error_key":"PANIC_ANON_VMA_DEGREE_NUMA","plugin_name":"Kernel panic in unlink_anon_vmas() while freeing anonymous pages","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"panic_anon_vma_degree|PANIC_ANON_VMA_DEGREE_KSM","description":"Kernel panic in unlink_anon_vmas() when KSM is running on the system","category":"Stability","severity":"WARN","hitCount":0,"summary":"When KSM is running on the system, a known issue in the unlink_anon_vma() function of the kernel could lead to a kernel panic.","summary_html":"<p>When KSM is running on the system, a known issue in the unlink_anon_vma() function of the kernel could lead to a kernel panic.</p>\n","plugin":"panic_anon_vma_degree","error_key":"PANIC_ANON_VMA_DEGREE_KSM","plugin_name":"Kernel panic in unlink_anon_vmas() while freeing anonymous pages","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"panic_anon_vma_degree|PANIC_ANON_VMA_DEGREE_BOTH","description":"Kernel panic in unlink_anon_vmas() while freeing anonymous pages","category":"Stability","severity":"WARN","hitCount":0,"summary":"When KSM is running on the system or the server architecture is NUMA, a known issue in the unlink_anon_vma() function of kernel could lead to a kernel panic.","summary_html":"<p>When KSM is running on the system or the server architecture is NUMA, a known issue in the unlink_anon_vma() function of kernel could lead to a kernel panic.</p>\n","plugin":"panic_anon_vma_degree","error_key":"PANIC_ANON_VMA_DEGREE_BOTH","plugin_name":"Kernel panic in unlink_anon_vmas() while freeing anonymous pages","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"pacemaker_corosync_service_status|PACEMAKER_COROSYNC_SERVICE_STATUS_CONFILCT","description":"Pacemaker-managed resources are unmanageable when rebooting a node with cman/corosync enabled and pacemaker disabled","category":"Stability","severity":"WARN","hitCount":0,"summary":"When rebooting a node with cman enabled (Red Hat Enterprise Linux 6) or Corosync enabled (Red Hat Enterprise Linux 7) and Pacemaker disabled, the node will be in a \"pending\" state, with other nodes waiting on it to fully come online.","summary_html":"<p>When rebooting a node with cman enabled (Red Hat Enterprise Linux 6) or Corosync enabled (Red Hat Enterprise Linux 7) and Pacemaker disabled, the node will be in a &quot;pending&quot; state, with other nodes waiting on it to fully come online.</p>\n","plugin":"pacemaker_corosync_service_status","error_key":"PACEMAKER_COROSYNC_SERVICE_STATUS_CONFILCT","plugin_name":"Pacemaker-managed resources are unmanageable when rebooting a node with cman/corosync enabled and pacemaker disabled","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"osp_ntp_check|OSP_NTP_ISSUE","description":"OpenStack and messaging services will malfunction when time is out of sync on all nodes","category":"Stability","severity":"WARN","hitCount":0,"summary":"For Red Hat Openstack Platform, system times on all nodes must be synchronized. Various Openstack services and messaging services will malfunction if the time on the controller and compute nodes are not in sync.\n","summary_html":"<p>For Red Hat Openstack Platform, system times on all nodes must be synchronized. Various Openstack services and messaging services will malfunction if the time on the controller and compute nodes are not in sync.</p>\n","plugin":"osp_ntp_check","error_key":"OSP_NTP_ISSUE","plugin_name":"OpenStack and messaging services will malfunction when time is out of sync on all nodes","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"osp_mariadb_target_check|SYSTEMCTL_MARIADB_SERVICE_TARGET_CHECK","description":"MariaDB service start failure on reboot when configured with a specific IP address and \"After=network.target\" is set","category":"Availability","severity":"WARN","hitCount":0,"summary":"Based on the current configuration, MariaDB will fail to start after reboot.\n","summary_html":"<p>Based on the current configuration, MariaDB will fail to start after reboot.</p>\n","plugin":"osp_mariadb_target_check","error_key":"SYSTEMCTL_MARIADB_SERVICE_TARGET_CHECK","plugin_name":"MariaDB service start failure after reboot when it is configured to a specific ip address and service is set to start after network service","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"osp_dpdk_socket_memory|OSP_DPDK_SOCKET_LOW_MEMORY","description":"Memory allocation for DPDK interface fails when value of dpdk-socket-mem is low","category":"Performance","severity":"WARN","hitCount":0,"summary":"Memory allocation for DPDK interface fails when value of dpdk-socket-mem is low.\n","summary_html":"<p>Memory allocation for DPDK interface fails when value of dpdk-socket-mem is low.</p>\n","plugin":"osp_dpdk_socket_memory","error_key":"OSP_DPDK_SOCKET_LOW_MEMORY","plugin_name":"Memory allocation for DPDK interface fails when value of dpdk-socket-mem is low","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"osp_director_high_memory_by_heat|OSP_HEAT_ENGINE_HIGH_MEMORY","description":"High memory utilization when the number of heat workers is not limited on director node","category":"Stability","severity":"WARN","hitCount":0,"summary":"High memory utilization occurs when the number of heat workers is not limited on the director node. Many heat-engine processes consume an excessive amount of memory.\n","summary_html":"<p>High memory utilization occurs when the number of heat workers is not limited on the director node. Many heat-engine processes consume an excessive amount of memory.</p>\n","plugin":"osp_director_high_memory_by_heat","error_key":"OSP_HEAT_ENGINE_HIGH_MEMORY","plugin_name":"High memory utilization when the number of heat workers is not limited on director node","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"osp_cinder_backend_host|OSP_CINDER_BACKEND_HOST","description":"Failure to shift cinder volume service when the host value in the cinder.conf file are different among each node","category":"Availability","severity":"WARN","hitCount":0,"summary":"Cluster fails to shift cinder volume service to other nodes when the host value in the `cinder.conf` file is different for each node.\n","summary_html":"<p>Cluster fails to shift cinder volume service to other nodes when the host value in the <code>cinder.conf</code> file is different for each node.</p>\n","plugin":"osp_cinder_backend_host","error_key":"OSP_CINDER_BACKEND_HOST","plugin_name":"Failure to shift cinder volume service when the host value in the cinder.conf file are different among each node","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"osp_certificates_expiration|OSP_CERTIFICATES_EXPIRE_WARN","description":"Galera certificates will expire in 7 days in OpenStack","category":"Availability","severity":"WARN","hitCount":0,"summary":"Connectivity failure between the OpenStack components will cause OpenStack to be down when Galera certificates have expired.\n","summary_html":"<p>Connectivity failure between the OpenStack components will cause OpenStack to be down when Galera certificates have expired.</p>\n","plugin":"osp_certificates_expiration","error_key":"OSP_CERTIFICATES_EXPIRE_WARN","plugin_name":"Connectivity failure when Galera certificates have expired in OpenStack","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"osp_certificates_expiration|OSP_CERTIFICATES_EXPIRE_INFO","description":"Galera certificates will expire in 30 days in OpenStack","category":"Availability","severity":"WARN","hitCount":0,"summary":"Connectivity failure between the OpenStack components will cause OpenStack to be down when Galera certificates have expired.\n","summary_html":"<p>Connectivity failure between the OpenStack components will cause OpenStack to be down when Galera certificates have expired.</p>\n","plugin":"osp_certificates_expiration","error_key":"OSP_CERTIFICATES_EXPIRE_INFO","plugin_name":"Connectivity failure when Galera certificates have expired in OpenStack","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"osp_certificates_expiration|OSP_CERTIFICATES_EXPIRE_ERROR","description":"Connectivity failure when Galera certificates have expired in OpenStack","category":"Availability","severity":"WARN","hitCount":0,"summary":"Connectivity failure between the OpenStack components will cause OpenStack to be down when Galera certificates have expired.\n","summary_html":"<p>Connectivity failure between the OpenStack components will cause OpenStack to be down when Galera certificates have expired.</p>\n","plugin":"osp_certificates_expiration","error_key":"OSP_CERTIFICATES_EXPIRE_ERROR","plugin_name":"Connectivity failure when Galera certificates have expired in OpenStack","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"osp7_undercloud_sync_power_issue|NOVA_SYNC_POWER_STATE","description":"Undercloud keeps shutting down overcloud nodes as the conflict of synchronizing the power state","category":"Availability","severity":"WARN","hitCount":0,"summary":"Undercloud keeps shutting down overcloud nodes as the result of synchronizing the power state after restarting all nodes with the `sync_power_state_interval` option not set to `-1`. \n\n","summary_html":"<p>Undercloud keeps shutting down overcloud nodes as the result of synchronizing the power state after restarting all nodes with the <code>sync_power_state_interval</code> option not set to <code>-1</code>. </p>\n","plugin":"osp7_undercloud_sync_power_issue","error_key":"NOVA_SYNC_POWER_STATE","plugin_name":"Undercloud keeps shutting down overcloud nodes as the conflict of synchronizing the power state","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"osp7_oslo_messaging_bug|PYTHON-OSLO-MESSAGING_BUG","description":"Openstack is inoperative due to rabbitmq disconnection when restarting nodes","category":"Availability","severity":"WARN","hitCount":0,"summary":"Clients of oslo.messaging get stuck due to rabbitmq disconnection, this issue occures when messaging service cannot handle channel errors effectively, This is a known bug (oslo.messaging holds connections when replies fail) in python-oslo-messaging rpm package.\n","summary_html":"<p>Clients of oslo.messaging get stuck due to rabbitmq disconnection, this issue occures when messaging service cannot handle channel errors effectively, This is a known bug (oslo.messaging holds connections when replies fail) in python-oslo-messaging rpm package.</p>\n","plugin":"osp7_oslo_messaging_bug","error_key":"PYTHON-OSLO-MESSAGING_BUG","plugin_name":"Clients of oslo.messaging get stuck when it cannot handle channel errors gracefully","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"osp7_oslo_messaging_bug|OSLO_MESSAGING_HOLD_CONNECTIONS_BUG","description":"Clients of oslo.messaging get stuck when it cannot handle channel errors gracefully","category":"Availability","severity":"WARN","hitCount":0,"summary":"Clients of oslo.messaging get stuck due to rabbitmq disconnection, this issue occures when messaging service cannot handle channel errors effectively, This is a known bug (oslo.messaging holds connections when replies fail) in python-oslo-messaging rpm package.\n","summary_html":"<p>Clients of oslo.messaging get stuck due to rabbitmq disconnection, this issue occures when messaging service cannot handle channel errors effectively, This is a known bug (oslo.messaging holds connections when replies fail) in python-oslo-messaging rpm package.</p>\n","plugin":"osp7_oslo_messaging_bug","error_key":"OSLO_MESSAGING_HOLD_CONNECTIONS_BUG","plugin_name":"Clients of oslo.messaging get stuck when it cannot handle channel errors gracefully","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"oracle_tuning|ORACLE_UNTUNED_WARN","description":"Decreased Oracle database performance when using untuned configurations (swap has been used over 30%)","category":"Performance","severity":"WARN","hitCount":0,"summary":"Decreased Oracle database performance when using untuned configurations.\n","summary_html":"<p>Decreased Oracle database performance when using untuned configurations.</p>\n","plugin":"oracle_tuning","error_key":"ORACLE_UNTUNED_WARN","plugin_name":"Decreased Oracle database performance when using untuned configurations","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"oracle_tuning|ORACLE_UNTUNED_INFO","description":"Decreased Oracle database performance when using untuned configurations","category":"Performance","severity":"WARN","hitCount":0,"summary":"Decreased Oracle database performance when using untuned configurations.\n","summary_html":"<p>Decreased Oracle database performance when using untuned configurations.</p>\n","plugin":"oracle_tuning","error_key":"ORACLE_UNTUNED_INFO","plugin_name":"Decreased Oracle database performance when using untuned configurations","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"oracle_storage_tuning|ORACLE_STORAGE_UNTUNED","description":"Decreased Oracle database performance when using untuned storage configurations","category":"Performance","severity":"WARN","hitCount":0,"summary":"When Oracle's storage tuning recommendations are not followed, Oracle databases will experience decreased performance.\n","summary_html":"<p>When Oracle&#39;s storage tuning recommendations are not followed, Oracle databases will experience decreased performance.</p>\n","plugin":"oracle_storage_tuning","error_key":"ORACLE_STORAGE_UNTUNED","plugin_name":"Decreased Oracle database performance when using untuned storage configurations","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"oracle_java_se_access_not_available|ORACLE_JAVA_SE_ACCESS_NOT_AVAILABLE","description":"Oracle Java SE will not receive support and updates after November 30, 2018","category":"Availability","severity":"WARN","hitCount":0,"summary":"Oracle Java SE will not receive support and updates after November 30, 2018.","summary_html":"<p>Oracle Java SE will not receive support and updates after November 30, 2018.</p>\n","plugin":"oracle_java_se_access_not_available","error_key":"ORACLE_JAVA_SE_ACCESS_NOT_AVAILABLE","plugin_name":"Oracle Java SE will not receive support and updates after November 30, 2018","ansible":0,"rec_impact":1,"rec_likelihood":4,"resolution_risk":1,"acked":false},{"rule_id":"oracle_io_scheduler|IMPROPER_IO_SCHEDULER","description":"Performance degraded when using improper I/O scheduler on systems running Oracle databases","category":"Performance","severity":"WARN","hitCount":0,"summary":"Oracle Database systems can see performance benefits from using an optimized I/O scheduler\n","summary_html":"<p>Oracle Database systems can see performance benefits from using an optimized I/O scheduler</p>\n","plugin":"oracle_io_scheduler","error_key":"IMPROPER_IO_SCHEDULER","plugin_name":"Performance degraded when using improper I/O scheduler on systems running Oracle databases","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"oracle_cluster_avahi_daemon|DETECTED_ORACLE_RAC_AVAHI_CONFLICT","description":"Oracle RAC cluster node rebooted unexpectedly due to avahi-daemon affected Oracle RAC's multicast heartbeat","category":"Availability","severity":"WARN","hitCount":0,"summary":"Oracle RAC cluster node rebooted unexpectedly due to avahi-daemon affected Oracle RAC's multicast heartbeat\n","summary_html":"<p>Oracle RAC cluster node rebooted unexpectedly due to avahi-daemon affected Oracle RAC&#39;s multicast heartbeat</p>\n","plugin":"oracle_cluster_avahi_daemon","error_key":"DETECTED_ORACLE_RAC_AVAHI_CONFLICT","plugin_name":"Oracle RAC cluster node rebooted unexpectedly due to avahi-daemon affected Oracle RAC's multicast heartbeat","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"openvswitch_vlans|OPENVSWITCH_VLANS","description":"Kernel panic when OpenVSwitch handles multiple VLANs","category":"Stability","severity":"WARN","hitCount":0,"summary":"Certain versions of the openvswitch driver do not handle frames that contain multiple VLAN headers correctly, which could result in a kernel panic.","summary_html":"<p>Certain versions of the openvswitch driver do not handle frames that contain multiple VLAN headers correctly, which could result in a kernel panic.</p>\n","plugin":"openvswitch_vlans","error_key":"OPENVSWITCH_VLANS","plugin_name":"Kernel panic when OpenVSwitch handles multiple VLANs","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"openvswitch_dpdk_pmd_isolcpu|OVS_DPDK_PMD_ISOLCPUS_ERROR","description":"Network performance degradation when PMD cores are not present in isolcpus list","category":"Performance","severity":"WARN","hitCount":0,"summary":"All Pull Mode Driver (PMD) masked CPU cores should be isolated from the host OS so that the CPU cores will not be used by the host OS. Otherwise Data Plane Development Kit (DPDK) network performance will be degraded. \n","summary_html":"<p>All Pull Mode Driver (PMD) masked CPU cores should be isolated from the host OS so that the CPU cores will not be used by the host OS. Otherwise Data Plane Development Kit (DPDK) network performance will be degraded. </p>\n","plugin":"openvswitch_dpdk_pmd_isolcpu","error_key":"OVS_DPDK_PMD_ISOLCPUS_ERROR","plugin_name":"Network performance degradation when PMD cores are not present in isolcpus list","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_wrong_memory_resource_limit|WRONG_MEMORY_LIMIT","description":"Project build failure when memory limit is configured with decimal unit prefix in the build configuration files","category":"Availability","severity":"WARN","hitCount":0,"summary":"Project build fails when memory limit is configured with decimal unit prefix in the build configuration files.","summary_html":"<p>Project build fails when memory limit is configured with decimal unit prefix in the build configuration files.</p>\n","plugin":"ocp_wrong_memory_resource_limit","error_key":"WRONG_MEMORY_LIMIT","plugin_name":"Project build failure when memory limit is configured incorrectly","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_shared_target_removed|SHARED_TARGET_REMOVED","description":"LUNs on iSCSI target inaccessible when deleting a pod using any LUN on the same iSCSI target","category":"Availability","severity":"WARN","hitCount":0,"summary":"Shared iSCSI target gets deleted when one OpenShift pod deletes a LUN it contains.\n","summary_html":"<p>Shared iSCSI target gets deleted when one OpenShift pod deletes a LUN it contains.</p>\n","plugin":"ocp_shared_target_removed","error_key":"SHARED_TARGET_REMOVED","plugin_name":"OCP shared iscsi target disappeared","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_service_denial|EXCEED_API_QUOTA","description":"Denial of service caused by volume deletion requests exceeding API limits","category":"Availability","severity":"WARN","hitCount":0,"summary":"Failed AWS delete volume actions will result in denial of service for API calls due to a bug in OpenShift.\n","summary_html":"<p>Failed AWS delete volume actions will result in denial of service for API calls due to a bug in OpenShift.</p>\n","plugin":"ocp_service_denial","error_key":"EXCEED_API_QUOTA","plugin_name":"Denial of service caused by volume deletion requests exceeding API limits","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_service_check|OSE_SERVICE_NOTRUNNING","description":"OpenShift functionality degraded when critical services are not running","category":"Availability","severity":"WARN","hitCount":0,"summary":"OpenShift will not function as expected when critical services are not running.\n","summary_html":"<p>OpenShift will not function as expected when critical services are not running.</p>\n","plugin":"ocp_service_check","error_key":"OSE_SERVICE_NOTRUNNING","plugin_name":"OpenShift functionality degraded when critical services are not running","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_prune_request_timeout|OCP_PRUNE_REQUEST_TIMEOUT","description":"Docker registry pruning failure when the value of request-timeout value is too low","category":"Performance","severity":"WARN","hitCount":0,"summary":"Docker registry pruning fails when the value of request-time is too low where network connection is slow or dataset is large.\n","summary_html":"<p>Docker registry pruning fails when the value of request-time is too low where network connection is slow or dataset is large.</p>\n","plugin":"ocp_prune_request_timeout","error_key":"OCP_PRUNE_REQUEST_TIMEOUT","plugin_name":"Docker registry pruning failure when the value of request-timeout value is too low","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_project_invisible|PROJECT_BECOME_INVISIBLE","description":"Projects are unlisted when there is a rolebinding to a nonexistent role","category":"Availability","severity":"WARN","hitCount":0,"summary":"Project visibility calculation fails if it encounters a role binding that references a nonexistent role. Projects containing a role binding that references a missing role will not appear when listing projects via the API.\n","summary_html":"<p>Project visibility calculation fails if it encounters a role binding that references a nonexistent role. Projects containing a role binding that references a missing role will not appear when listing projects via the API.</p>\n","plugin":"ocp_project_invisible","error_key":"PROJECT_BECOME_INVISIBLE","plugin_name":"Projects are unlisted when there is a rolebinding to a nonexistent role","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_pod_number_limit|POD_NUMBER_LIMIT_INCIDENT","description":"Pod scheduling failed because pod limit has been exceeded","category":"Availability","severity":"WARN","hitCount":0,"summary":"Pod scheduling fails when creating more pods than pod limit which is configured by pods-per-core and max-pods in /etc/origin/node/node-config.yaml.\n","summary_html":"<p>Pod scheduling fails when creating more pods than pod limit which is configured by pods-per-core and max-pods in /etc/origin/node/node-config.yaml.</p>\n","plugin":"ocp_pod_number_limit","error_key":"POD_NUMBER_LIMIT_INCIDENT","plugin_name":"Failure to schedule pods when pod limit is exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_pod_number_limit|POD_NUMBER_LIMIT","description":"Pod scheduling will fail when pod limit is exceeded","category":"Availability","severity":"WARN","hitCount":0,"summary":"Pod scheduling fails when creating more pods than pod limit which is configured by pods-per-core and max-pods in /etc/origin/node/node-config.yaml.\n","summary_html":"<p>Pod scheduling fails when creating more pods than pod limit which is configured by pods-per-core and max-pods in /etc/origin/node/node-config.yaml.</p>\n","plugin":"ocp_pod_number_limit","error_key":"POD_NUMBER_LIMIT","plugin_name":"Failure to schedule pods when pod limit is exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_pods_cannot_resolve_dns|OCP_PODS_CANNOT_RESOLVE_DNS_V1","description":"DNS resolution fails within an OpenShift Pod when DNS server address is set incorrectly","category":"Availability","severity":"WARN","hitCount":0,"summary":"DNS resolution fails within an OpenShift pod when DNS server address is set incorrectly.\n","summary_html":"<p>DNS resolution fails within an OpenShift pod when DNS server address is set incorrectly.</p>\n","plugin":"ocp_pods_cannot_resolve_dns","error_key":"OCP_PODS_CANNOT_RESOLVE_DNS_V1","plugin_name":"DNS resolution fails within an OpenShift pod when DNS server address is set incorrectly","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_null_repotag|OCP_NULL_REPOTAG","description":"Failure to prune image via \"openshift ex\" command when image with \"null\" RepoTags exists","category":"Availability","severity":"WARN","hitCount":0,"summary":"Failure to prune images via \"openshift ex dockergc encounter\" command when image with \"null\" RepoTags exists.\n","summary_html":"<p>Failure to prune images via &quot;openshift ex dockergc encounter&quot; command when image with &quot;null&quot; RepoTags exists.</p>\n","plugin":"ocp_null_repotag","error_key":"OCP_NULL_REPOTAG","plugin_name":"Failure to prune image via \"openshift ex\" command when image with \"null\" RepoTags exists","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_node_selector_not_match_node_label|NODE_SELECTOR_NOT_MATCH_NODE_LABELS","description":"Failure to deploy pods when nodeSelector does not match any node's label","category":"Availability","severity":"WARN","hitCount":0,"summary":"Pods deployment fails when nodeSelector does not match any node's label.\n","summary_html":"<p>Pods deployment fails when nodeSelector does not match any node&#39;s label.</p>\n","plugin":"ocp_node_selector_not_match_node_label","error_key":"NODE_SELECTOR_NOT_MATCH_NODE_LABELS","plugin_name":"Failure to deploy pods when nodeSelector does not match any node's label","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_memory_exhausted_by_in_memory_volumes|MEMORY_EXHAUSTED_BY_IN_MEMORY_VOLUMES","description":"Memory used by containers is not limited when Openshift pods are using in-memory EmptyDir volumes","category":"Performance","severity":"WARN","hitCount":0,"summary":"Memory used by containers is not limited when Openshift pods are using in-memory EmptyDir volumes. In this situation, the host memory could be used up by containers.\n","summary_html":"<p>Memory used by containers is not limited when Openshift pods are using in-memory EmptyDir volumes. In this situation, the host memory could be used up by containers.</p>\n","plugin":"ocp_memory_exhausted_by_in_memory_volumes","error_key":"MEMORY_EXHAUSTED_BY_IN_MEMORY_VOLUMES","plugin_name":"Memory used by containers is not limited when Openshift pods are using in-memory EmptyDir volumes","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_long_pvdeletion|LONG_PV_DELETION","description":"Degraded performance when deleting dynamically provisioned persistent volumes with \"Delete\" reclaim policy","category":"Availability","severity":"WARN","hitCount":0,"summary":"Due to a bug in OpenShift 3.2 and earlier, it will take an excessively long time to remove a persistent volume that has a Delete reclaim policy.\n","summary_html":"<p>Due to a bug in OpenShift 3.2 and earlier, it will take an excessively long time to remove a persistent volume that has a Delete reclaim policy.</p>\n","plugin":"ocp_long_pvdeletion","error_key":"LONG_PV_DELETION","plugin_name":"Degraded performance when deleting dynamically provisioned persistent volumes with \"Delete\" reclaim policy","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_localhost_route_freeze|OCP_LOCALHOST_ROUTE_FREEZE","description":"Router does not work when deleting route with host set to \"localhost\"","category":"Availability","severity":"WARN","hitCount":0,"summary":"Router does not work because reloading fails when deleting route with host set to \"localhost\".\n","summary_html":"<p>Router does not work because reloading fails when deleting route with host set to &quot;localhost&quot;.</p>\n","plugin":"ocp_localhost_route_freeze","error_key":"OCP_LOCALHOST_ROUTE_FREEZE","plugin_name":"Router does not work when deleting route with host set to \"localhost\"","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_large_number_route|LARGE_NUMBER_ROUTE","description":"Excessive load time for new routes when a large number of routes exist","category":"Performance","severity":"WARN","hitCount":0,"summary":"Excessive load time for new routes will be encountered when a large number of routes exist.\n","summary_html":"<p>Excessive load time for new routes will be encountered when a large number of routes exist.</p>\n","plugin":"ocp_large_number_route","error_key":"LARGE_NUMBER_ROUTE","plugin_name":"Excessive load time for new routes when a large number of routes exist","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_iptables_restore_high_cpu|IPTABLES_RESTORE_HIGH_CPU","description":"Pod creation fails when is under high load due to iptables-restore process","category":"Availability","severity":"WARN","hitCount":0,"summary":"Pod creation fails when is under high load due to iptables-restore process.\n","summary_html":"<p>Pod creation fails when is under high load due to iptables-restore process.</p>\n","plugin":"ocp_iptables_restore_high_cpu","error_key":"IPTABLES_RESTORE_HIGH_CPU","plugin_name":"Pod creation fails when is under high load due to iptables-restore process","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_invalid_no_proxy_list|INVALID_NO_PROXY_SETTING","description":"Inaccessible pods when using invalid clauses in the NO_PROXY setting","category":"Availability","severity":"WARN","hitCount":0,"summary":"Pods become inaccessible when using invalid clauses in the NO_PROXY setting.\n","summary_html":"<p>Pods become inaccessible when using invalid clauses in the NO_PROXY setting.</p>\n","plugin":"ocp_invalid_no_proxy_list","error_key":"INVALID_NO_PROXY_SETTING","plugin_name":"Inaccessible pods when using invalid clauses in the NO_PROXY setting","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"ocp_headless_service|OCP_HEADLESS_SERVICE_INCIDENT","description":"Master controller fails to start when changes are made to the SDN plugin if there are headless services in the cluster","category":"Availability","severity":"WARN","hitCount":0,"summary":"When changes are made to the SDN plugin, the master controller will fail to start if there are headless services in the cluster. \n","summary_html":"<p>When changes are made to the SDN plugin, the master controller will fail to start if there are headless services in the cluster. </p>\n","plugin":"ocp_headless_service","error_key":"OCP_HEADLESS_SERVICE_INCIDENT","plugin_name":"Master controller fails to start when changes are made to the SDN plugin if there are headless services in the cluster","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_glusterfs_service_restart|OCP_GLUSTERFS_SERVICE_RESTART","description":"GlusterFS storage disconnects from pods when restarting atomic-openshift-node server","category":"Availability","severity":"WARN","hitCount":0,"summary":"When a restart of the atomic-openshift-node.service is done, all pods that have an active PV Claim (PVC) mounted will lose glusterfs mounts.\n","summary_html":"<p>When a restart of the atomic-openshift-node.service is done, all pods that have an active PV Claim (PVC) mounted will lose glusterfs mounts.</p>\n","plugin":"ocp_glusterfs_service_restart","error_key":"OCP_GLUSTERFS_SERVICE_RESTART","plugin_name":"GlusterFS storage disconnects from pods when restarting atomic-openshift-node server","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ocp_docker_version_incompatible|OCP_OPENSHIFT_VERSION_INCOMPATIBLE","description":"Failed api connection between docker and OpenShift when version of docker and openshift are incompatible","category":"Performance","severity":"WARN","hitCount":0,"summary":"Due to the use of a new API between docker 1.12 and OpenShift 3.4, if Docker 1.12+ was installed on any new or existing OSE 3.2 or 3.3 hosts there is a high possibility that connection between docker and OpenShift will not function properly, leading to performance issues such as a hung Docker process and OpenShift error messages.\n","summary_html":"<p>Due to the use of a new API between docker 1.12 and OpenShift 3.4, if Docker 1.12+ was installed on any new or existing OSE 3.2 or 3.3 hosts there is a high possibility that connection between docker and OpenShift will not function properly, leading to performance issues such as a hung Docker process and OpenShift error messages.</p>\n","plugin":"ocp_docker_version_incompatible","error_key":"OCP_OPENSHIFT_VERSION_INCOMPATIBLE","plugin_name":"Failed api connection between docker and OpenShift when version of docker and openshift are incompatible","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_docker_registry_restart|DOCKER_REGISTRY_POD_RESTART","description":"Docker registry pod restarts occasionally when liveness and readiness probes collide","category":"Availability","severity":"WARN","hitCount":0,"summary":"Docker registry pod restarts occasionally when liveness and readiness probes collide because both liveness and readiness probes are invoked at the same time by GET method, and use the same host:port and transport.\n","summary_html":"<p>Docker registry pod restarts occasionally when liveness and readiness probes collide because both liveness and readiness probes are invoked at the same time by GET method, and use the same host:port and transport.</p>\n","plugin":"ocp_docker_registry_restart","error_key":"DOCKER_REGISTRY_POD_RESTART","plugin_name":"Docker registry pod restarts occasionally when liveness and readiness probes collide","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_concurrent_pod_without_ip|OCP_CONCURRENT_POD_WITHOUT_IP","description":"Image build failure when creating a large number of concurrent builds","category":"Availability","severity":"WARN","hitCount":0,"summary":"When creating a large number of concurrent builds, image builds will fail due to some pods without IP address being allocated.\n","summary_html":"<p>When creating a large number of concurrent builds, image builds will fail due to some pods without IP address being allocated.</p>\n","plugin":"ocp_concurrent_pod_without_ip","error_key":"OCP_CONCURRENT_POD_WITHOUT_IP","plugin_name":"Image build failure when creating a large number of concurrent builds","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ocp_bc_resource_override_failure|WARN_OCP_BUILD_CONFIGURATION_RESOURCE_OVERRIDE_FAILURE","description":"Global build default resource requests and limits not honoured by build configuration","category":"Availability","severity":"WARN","hitCount":0,"summary":"Specified resources in the build default configuration are not applied to the build pods.\n","summary_html":"<p>Specified resources in the build default configuration are not applied to the build pods.</p>\n","plugin":"ocp_bc_resource_override_failure","error_key":"WARN_OCP_BUILD_CONFIGURATION_RESOURCE_OVERRIDE_FAILURE","plugin_name":"Global build default resource requests and limits not honoured by build configuration","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"ntpd_slew_mode|NTPD_SLEW_MODE_FAILED_WARN","description":"The system clock changes instantaneously when a leap second event occurs in a NTP system, even though slew mode is configured.","category":"Stability","severity":"WARN","hitCount":0,"summary":"The system clock changes instantaneously when a leap second event occurs in a NTP system, even though -x option (slew mode) is configured.\n","summary_html":"<p>The system clock changes instantaneously when a leap second event occurs in a NTP system, even though -x option (slew mode) is configured.</p>\n","plugin":"ntpd_slew_mode","error_key":"NTPD_SLEW_MODE_FAILED_WARN","plugin_name":"The system clock changes instantaneously when a leap second event occurs in a NTP system, even though slew mode is configured.","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ntpd_not_reset_leap_status|LEAP_STATUS_NOT_RESET_ERROR","description":"The ntpd leap status is not reset after inserting a leap second due to a bug of ntp package","category":"Stability","severity":"WARN","hitCount":0,"summary":"Due to a known bug in the ntp package, the `ntpd` service does not reset leap status when finishing a leap second insertion in an NTP server and continues announcing an upcoming leap second to its clients.\n","summary_html":"<p>Due to a known bug in the ntp package, the <code>ntpd</code> service does not reset leap status when finishing a leap second insertion in an NTP server and continues announcing an upcoming leap second to its clients.</p>\n","plugin":"ntpd_not_reset_leap_status","error_key":"LEAP_STATUS_NOT_RESET_ERROR","plugin_name":"The ntpd leap status is not reset after inserting a leap second due to a bug of ntp package","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"nproc_common_entries|NPROC_COMMON_ENTRIES","description":"Unexpected behavior occurs when duplicate entries with different settings in limits.conf and limits.d files","category":"Stability","severity":"WARN","hitCount":0,"summary":"Unexpected behavior occurs when duplicate entries in limits.conf and limits.d files.\n","summary_html":"<p>Unexpected behavior occurs when duplicate entries in limits.conf and limits.d files.</p>\n","plugin":"nproc_common_entries","error_key":"NPROC_COMMON_ENTRIES","plugin_name":"Unexpected behavior occurs when duplicate entries in limits.conf and limits.d files","ansible":0,"rec_impact":1,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"nova_versioned_notifications_no_consumer|NOVA_NOTIFICATION_NOT_UNVERSIONED","description":"Nova commands timeout when the growing versioned notifications queues have no consumers","category":"Availability","severity":"WARN","hitCount":0,"summary":"Nova commands timeout because growing versioned notifications queues have no consumers.\n","summary_html":"<p>Nova commands timeout because growing versioned notifications queues have no consumers.</p>\n","plugin":"nova_versioned_notifications_no_consumer","error_key":"NOVA_NOTIFICATION_NOT_UNVERSIONED","plugin_name":"Nova commands timeout when the growing versioned notifications queues have no consumers","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nova_scheduler_incompatible_filters|NOVA_SCHEDULER_INCOMPATIBLE_FILTERS_ERROR","description":"VM migration failure when incompatible filters are used in nova.conf as scheduler_default_filters","category":"Availability","severity":"WARN","hitCount":0,"summary":"VM migration fails when `ComputeCapabilitiesFilter` and `AggregateInstanceExtraSpecsFilter` are both used in `nova.conf` as `scheduler_default_filters`.\n","summary_html":"<p>VM migration fails when <code>ComputeCapabilitiesFilter</code> and <code>AggregateInstanceExtraSpecsFilter</code> are both used in <code>nova.conf</code> as <code>scheduler_default_filters</code>.</p>\n","plugin":"nova_scheduler_incompatible_filters","error_key":"NOVA_SCHEDULER_INCOMPATIBLE_FILTERS_ERROR","plugin_name":"VM migration failure when incompatible filters used in nova.conf as scheduler_default_filters","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nova_noop_firewall_driver_check|NOVA_NOOPFIREWALLDRIVER_NO_SETTING","description":"VM network intermittently unavailable when incorrect firewall driver is configured in nova","category":"Stability","severity":"WARN","hitCount":0,"summary":"When \"firewall_driver\" in nova.conf file is misconfigured, Nova and Neutron will try to manage security group rules at the same time which will cause conflicts, leading to Nova VM network intermittently unavailable. \n","summary_html":"<p>When &quot;firewall_driver&quot; in nova.conf file is misconfigured, Nova and Neutron will try to manage security group rules at the same time which will cause conflicts, leading to Nova VM network intermittently unavailable. </p>\n","plugin":"nova_noop_firewall_driver_check","error_key":"NOVA_NOOPFIREWALLDRIVER_NO_SETTING","plugin_name":"VM network intermittently unavailable when incorrect firewall driver is configured in nova","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nova_migration_fail_as_ssh|NOVA_MIGRATION_FAILED_AS_SSHD_SYMPTOM","description":"Nova live migration has previously failed because SSH needs password authentication","category":"Availability","severity":"WARN","hitCount":0,"summary":"Nova live migration ever failed due to SSH need password authentication \n","summary_html":"<p>Nova live migration ever failed due to SSH need password authentication </p>\n","plugin":"nova_migration_fail_as_ssh","error_key":"NOVA_MIGRATION_FAILED_AS_SSHD_SYMPTOM","plugin_name":"Nova live migration failure when SSH needs password authentication","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nova_migration_fail_as_ssh|NOVA_MIGRATION_FAILED_AS_SSHD","description":"Nova live migration will fail when SSH needs password authentication","category":"Availability","severity":"WARN","hitCount":0,"summary":"Nova live migration fails when SSH needs password authentication. \n","summary_html":"<p>Nova live migration fails when SSH needs password authentication. </p>\n","plugin":"nova_migration_fail_as_ssh","error_key":"NOVA_MIGRATION_FAILED_AS_SSHD","plugin_name":"Nova live migration failure when SSH needs password authentication","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nova_migration_fail_as_selinux|NOVA_MIGRATION_AS_SELINUX_ERROR","description":"Live migration fails when security_driver is set to none on OSP compute nodes","category":"Availability","severity":"WARN","hitCount":0,"summary":"Live migration fails when `security_driver` is set to **none** in `/etc/libvirt/qemu.conf`\n","summary_html":"<p>Live migration fails when <code>security_driver</code> is set to <strong>none</strong> in <code>/etc/libvirt/qemu.conf</code></p>\n","plugin":"nova_migration_fail_as_selinux","error_key":"NOVA_MIGRATION_AS_SELINUX_ERROR","plugin_name":"Live migration failed when security_driver is set to none","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nic_speed_issue|NIC_MAX_PERF_ISSUE","description":"Network interface card is not operating at maximum speed due to faulty cable, network interface card, switchport, SFP, etc.","category":"Performance","severity":"WARN","hitCount":0,"summary":"The network interface card is not operating at the proper speed. The `ethtool` command shows the card speed is less than Advertised and Supported link modes.\n","summary_html":"<p>The network interface card is not operating at the proper speed. The <code>ethtool</code> command shows the card speed is less than Advertised and Supported link modes.</p>\n","plugin":"nic_speed_issue","error_key":"NIC_MAX_PERF_ISSUE","plugin_name":"Network interface card is not operating at maximum speed due to faulty cable, network interface card, switchport, SFP, etc.","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"nic_rx_perf_issue|NIC_RX_PERF_ISSUE","description":"Poor TCP performance for NICs with RX acceleration due to a known kernel bug","category":"Performance","severity":"WARN","hitCount":0,"summary":"Network interface cards (NICs) with receive (RX) acceleration (GRO, LRO, TPA, etc.) can suffer from poor network performance due to a known issue in the Red Hat Enterprise Linux 6.4 kernel prior to 2.6.32-358.el6.\n","summary_html":"<p>Network interface cards (NICs) with receive (RX) acceleration (GRO, LRO, TPA, etc.) can suffer from poor network performance due to a known issue in the Red Hat Enterprise Linux 6.4 kernel prior to 2.6.32-358.el6.</p>\n","plugin":"nic_rx_perf_issue","error_key":"NIC_RX_PERF_ISSUE","plugin_name":"Poor TCP performance for NICs with RX acceleration due to a known kernel bug","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"nf_conntrack_table_full|NF_CONNTRACK_TABLE_FULL","description":"Packet loss when maximum number of entries in connections table is exceeded","category":"Stability","severity":"WARN","hitCount":0,"summary":"Packages that are traversing the system firewall are dropped when maximum number of entries of connection table is exceeded.\n","summary_html":"<p>Packages that are traversing the system firewall are dropped when maximum number of entries of connection table is exceeded.</p>\n","plugin":"nf_conntrack_table_full","error_key":"NF_CONNTRACK_TABLE_FULL","plugin_name":"Packet loss when maximum number of entries in connections table is exceeded","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"nfs4_client_hang|NFS4_CLIENT_HANG_INFO","description":"NFSv4 client hangs when trying to open files against NFSv4 server","category":"Stability","severity":"WARN","hitCount":0,"summary":"NFSv4 client hangs due to never finishing state recovery when trying to open files against an NFSv4 server.\n","summary_html":"<p>NFSv4 client hangs due to never finishing state recovery when trying to open files against an NFSv4 server.</p>\n","plugin":"nfs4_client_hang","error_key":"NFS4_CLIENT_HANG_INFO","plugin_name":"NFSv4 client hangs when trying to open files against NFSv4 server","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"neutron_tenant_vlan_limit|NEUTRON_VLAN_4094_LIMIT","description":"Failure to create VLAN tenant network when total VLAN networks reaches 4094","category":"Availability","severity":"WARN","hitCount":0,"summary":"When the number of 802.1q VLAN IDs reaches the limit of 4094, VLAN tenant network creation will fail.\n","summary_html":"<p>When the number of 802.1q VLAN IDs reaches the limit of 4094, VLAN tenant network creation will fail.</p>\n","plugin":"neutron_tenant_vlan_limit","error_key":"NEUTRON_VLAN_4094_LIMIT","plugin_name":"Failure to create VLAN tenant network when total VLAN networks reaches 4094","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"neutron_osp11_ha_bug|NEUTRON_OSP11_HA_BUG","description":"Openstack tenant network outage when network node is restarted, due to a known bug","category":"Availability","severity":"WARN","hitCount":0,"summary":"Due to a known bug, OpenStack tenant network outage occurs when one of network nodes shuts down or reboots.\n","summary_html":"<p>Due to a known bug, OpenStack tenant network outage occurs when one of network nodes shuts down or reboots.</p>\n","plugin":"neutron_osp11_ha_bug","error_key":"NEUTRON_OSP11_HA_BUG","plugin_name":"OpenStack tenant network outage when network node is restarted, due to a known bug","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"neutron_duplicate_iptables_warning|NEUTRON_HUGE_DUPLICATE_IPTABLE_MSG","description":"Excessive storage utilization when bug in openstack-neutron creates large log files","category":"Availability","severity":"WARN","hitCount":0,"summary":"Due to a known bug, a huge amount of **\"Duplicate iptables rule detected\"** warning messages are generated by L3-agent in l3-agent.log when multiple security groups with same security group rules are applied while spawning an instance.\n","summary_html":"<p>Due to a known bug, a huge amount of <strong>&quot;Duplicate iptables rule detected&quot;</strong> warning messages are generated by L3-agent in l3-agent.log when multiple security groups with same security group rules are applied while spawning an instance.</p>\n","plugin":"neutron_duplicate_iptables_warning","error_key":"NEUTRON_HUGE_DUPLICATE_IPTABLE_MSG","plugin_name":"Excessive storage utilization when bug in openstack-neutron creates large log files","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"network_vmxnet3_dropped_udp|INSTALLED_VMXNET3_MODULE_DROPPED_PACKETS_POSSIBLE","description":"VMXNET3 drops small UDP packets due to a bug in its driver in specific RHEL6.2 kernels","category":"Stability","severity":"WARN","hitCount":0,"summary":"The `vmxnet3` driver shipped within specific Red Hat Enterprise Linux (RHEL) 6.2\nkernel versions that are earlier than `2.6.32-220.13.1.el6` has a bug that can cause\npacket and data loss.\n","summary_html":"<p>The <code>vmxnet3</code> driver shipped within specific Red Hat Enterprise Linux (RHEL) 6.2\nkernel versions that are earlier than <code>2.6.32-220.13.1.el6</code> has a bug that can cause\npacket and data loss.</p>\n","plugin":"network_vmxnet3_dropped_udp","error_key":"INSTALLED_VMXNET3_MODULE_DROPPED_PACKETS_POSSIBLE","plugin_name":"VMXNET3 drops small UDP packets due to a bug in its driver in specific RHEL6.2 kernels","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"network_vmxnet3_discard_ipv6|WARN_MODULE_VMXNET3_DISCARD_IPV6_PACKETS","description":"Reduced network performance on heavy IPv6 workload due to a bug in vmxnet3 driver","category":"Performance","severity":"WARN","hitCount":0,"summary":"The vmxnet3 driver discards IPv6 packets due to a known bug leading to reduced network performance.\n","summary_html":"<p>The vmxnet3 driver discards IPv6 packets due to a known bug leading to reduced network performance.</p>\n","plugin":"network_vmxnet3_discard_ipv6","error_key":"WARN_MODULE_VMXNET3_DISCARD_IPV6_PACKETS","plugin_name":"Reduced network performance due to a bug in vmxnet3 driver","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"network_vmxnet3_discard_ipv6|INFO_MODULE_VMXNET3_DISCARD_IPV6_PACKETS","description":"Reduced IPv6 network performance due to a bug in vmxnet3 driver","category":"Performance","severity":"WARN","hitCount":0,"summary":"The vmxnet3 driver discards IPv6 packets due to a known bug leading to reduced network performance.\n","summary_html":"<p>The vmxnet3 driver discards IPv6 packets due to a known bug leading to reduced network performance.</p>\n","plugin":"network_vmxnet3_discard_ipv6","error_key":"INFO_MODULE_VMXNET3_DISCARD_IPV6_PACKETS","plugin_name":"Reduced network performance due to a bug in vmxnet3 driver","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"network_multiple_lo_ip_addr|NETWORK_MULTIPLE_LO_IPADDR","description":"IPv4 connection unstable when lo is misconfigured","category":"Stability","severity":"WARN","hitCount":0,"summary":"When the IP address assigned to the loopback interface is in the same subnet as that of any primary interface, the network connection is down.","summary_html":"<p>When the IP address assigned to the loopback interface is in the same subnet as that of any primary interface, the network connection is down.</p>\n","plugin":"network_multiple_lo_ip_addr","error_key":"NETWORK_MULTIPLE_LO_IPADDR","plugin_name":"IPv4 connection unstable when lo is misconfigured","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"network_bridge_multicast_fail|BRIDGE_NEED_MULTICAST_QUERIER","description":"Network bridges fail to forward multicast traffic on specific kernel version","category":"Stability","severity":"WARN","hitCount":0,"summary":"Network bridges will fail to forward multicast traffic on some specific kernel version due to a known bug that disables multicast querying by default.\n","summary_html":"<p>Network bridges will fail to forward multicast traffic on some specific kernel version due to a known bug that disables multicast querying by default.</p>\n","plugin":"network_bridge_multicast_fail","error_key":"BRIDGE_NEED_MULTICAST_QUERIER","plugin_name":"Network bridges fail to forward multicast traffic on specific kernel version","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"network_blue_flame|NETWORK_MELLANOX_MLX4_ISSUE","description":"Network performance loss when using a Mellanox network card or mlx4 driver on certain RHEL versions","category":"Stability","severity":"WARN","hitCount":0,"summary":"Network performance loss or intermittent connectivity issue can happen when using a Mellanox network card or mlx4 driver on certain RHEL.\n","summary_html":"<p>Network performance loss or intermittent connectivity issue can happen when using a Mellanox network card or mlx4 driver on certain RHEL.</p>\n","plugin":"network_blue_flame","error_key":"NETWORK_MELLANOX_MLX4_ISSUE","plugin_name":"Network performance loss when using a Mellanox network card or mlx4 driver on certain RHEL versions","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"neighbour_table_overflow|NEIGHBOUR_TABLE_OVERFLOW_WARN","description":"Network connectivity will fail when neighbour table nears capacity","category":"Stability","severity":"WARN","hitCount":0,"summary":"Network connectivity fails when neighbour table is close to full.\n","summary_html":"<p>Network connectivity fails when neighbour table is close to full.</p>\n","plugin":"neighbour_table_overflow","error_key":"NEIGHBOUR_TABLE_OVERFLOW_WARN","plugin_name":"Network connectivity fails when neighbour table is full","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"neighbour_table_overflow|NEIGHBOUR_TABLE_OVERFLOW_ERROR","description":"Network connectivity fails when neighbour table is full","category":"Stability","severity":"WARN","hitCount":0,"summary":"Network connectivity fails when neighbour table is full.\n","summary_html":"<p>Network connectivity fails when neighbour table is full.</p>\n","plugin":"neighbour_table_overflow","error_key":"NEIGHBOUR_TABLE_OVERFLOW_ERROR","plugin_name":"Network connectivity fails when neighbour table is full","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"mysql_odbc_memory_leak|MYSQL_ODBC_MEMORY_LEAK","description":"Memory leak occurs when using drivers with SSPS in MySQL ODBC connector due to a bug in mysql-connector-odbc","category":"Availability","severity":"WARN","hitCount":0,"summary":"MySQL ODBC connector has a memory leak in the function SQLPrepare due to a bug in mysql-connector-odbc.\n","summary_html":"<p>MySQL ODBC connector has a memory leak in the function SQLPrepare due to a bug in mysql-connector-odbc.</p>\n","plugin":"mysql_odbc_memory_leak","error_key":"MYSQL_ODBC_MEMORY_LEAK","plugin_name":"Memory leak occurs when using drivers with SSPS in MySQL ODBC connector due to a bug in mysql-connector-odbc","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"mysql_md5_forbidden_in_fips_mode|MYSQL_MD5_FORBIDDEN_IN_FIPS_MODE","description":"The mysqld daemon failed when FIPS mode is enabled","category":"Availability","severity":"WARN","hitCount":0,"summary":"The mysqld daemon failed with error messages when FIPS mode is enabled.\n","summary_html":"<p>The mysqld daemon failed with error messages when FIPS mode is enabled.</p>\n","plugin":"mysql_md5_forbidden_in_fips_mode","error_key":"MYSQL_MD5_FORBIDDEN_IN_FIPS_MODE","plugin_name":"The mysqld daemon failed when FIPS mode is enabled","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"multipathd_start_failed|MULTIPATHD_START_FAILED_WARN","description":"Multipathd service fails to start at boot time (this issue may happen in the future)","category":"Stability","severity":"WARN","hitCount":0,"summary":"Package `device-mapper-multipath` prior to `device-mapper-multipath-0.4.9-77.el7_1.1` can cause the multipathd service to fail in starting at boot time. ","summary_html":"<p>Package <code>device-mapper-multipath</code> prior to <code>device-mapper-multipath-0.4.9-77.el7_1.1</code> can cause the multipathd service to fail in starting at boot time. </p>\n","plugin":"multipathd_start_failed","error_key":"MULTIPATHD_START_FAILED_WARN","plugin_name":"Multipathd service fails to start at boot time","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"multipathd_start_failed|MULTIPATHD_START_FAILED_ERROR","description":"Multipathd service fails to start at boot time (the issue has happened)","category":"Stability","severity":"WARN","hitCount":0,"summary":"Package `device-mapper-multipath` prior to `device-mapper-multipath-0.4.9-77.el7_1.1` can cause the multipathd service to fail in starting at boot time. ","summary_html":"<p>Package <code>device-mapper-multipath</code> prior to <code>device-mapper-multipath-0.4.9-77.el7_1.1</code> can cause the multipathd service to fail in starting at boot time. </p>\n","plugin":"multipathd_start_failed","error_key":"MULTIPATHD_START_FAILED_ERROR","plugin_name":"Multipathd service fails to start at boot time","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"missing_grub_symlink|MISSED_GRUB_SYMLINK_ISSUE","description":"The grub's configuration file is not getting updated after installing new kernel when symlink missed","category":"Stability","severity":"WARN","hitCount":0,"summary":"The grub's configuration file is not getting updated after installing new kernel when symlink missed.\n","summary_html":"<p>The grub&#39;s configuration file is not getting updated after installing new kernel when symlink missed.</p>\n","plugin":"missing_grub_symlink","error_key":"MISSED_GRUB_SYMLINK_ISSUE","plugin_name":"The grub's configuration file is not getting updated after installing new kernel when symlink missed","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"missing_boot_files|MISSING_BOOT_FILES","description":"System fails to reboot when missing kernel or initial ramdisk files","category":"Stability","severity":"WARN","hitCount":0,"summary":"When the grub referenced kernel or initial ramdisk files do not exist in the system, grub will not be able to load these files. This will result in reboot failure.\n","summary_html":"<p>When the grub referenced kernel or initial ramdisk files do not exist in the system, grub will not be able to load these files. This will result in reboot failure.</p>\n","plugin":"missing_boot_files","error_key":"MISSING_BOOT_FILES","plugin_name":"System fails to reboot when grub file misconfiguration","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":2,"acked":false},{"rule_id":"missing_boot_files|GRUB_CONFIG_ISSUE","description":"System fails to reboot when grub file misconfiguration","category":"Stability","severity":"WARN","hitCount":0,"summary":"When the grub configuration file is corrupt or no items for a kernel and initial ramdisk to boot into, the system will not be able to reboot normally. \n","summary_html":"<p>When the grub configuration file is corrupt or no items for a kernel and initial ramdisk to boot into, the system will not be able to reboot normally. </p>\n","plugin":"missing_boot_files","error_key":"GRUB_CONFIG_ISSUE","plugin_name":"System fails to reboot when grub file misconfiguration","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":4,"acked":false},{"rule_id":"min_free_kbytes|MIN_FREE_KBYTES_HIGH","description":"Possibility of performance issues is too high when vm.min_free_kbytes is high","category":"Performance","severity":"WARN","hitCount":0,"summary":"Possibility of performance issues is too high when vm.min_free_kbytes is high.\n","summary_html":"<p>Possibility of performance issues is too high when vm.min_free_kbytes is high.</p>\n","plugin":"min_free_kbytes","error_key":"MIN_FREE_KBYTES_HIGH","plugin_name":"Possibility of performance issues is too high when vm.min_free_kbytes is high","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"mariadb_open_file_limits|MARIADB_OPEN_FILE_LIMITS","description":"OpenStack services stop working because file descriptor limit is reached in MariaDB service","category":"Availability","severity":"WARN","hitCount":0,"summary":"OpenStack services stop working because file descriptor limit is reached in MariaDB service.\n","summary_html":"<p>OpenStack services stop working because file descriptor limit is reached in MariaDB service.</p>\n","plugin":"mariadb_open_file_limits","error_key":"MARIADB_OPEN_FILE_LIMITS","plugin_name":"OpenStack services stop working because file descriptor limit is reached in MariaDB service","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"leapsec_hrtimer_expire|LEAPSEC_HRTIMER_EXPIRE_EARLY_OTHER_WARN","description":"System hang, high CPU usage, or application crash during a leap second event","category":"Stability","severity":"WARN","hitCount":0,"summary":"High CPU utilization and system crashes occur when leap second events occur due to a bug in the kernel.\n","summary_html":"<p>High CPU utilization and system crashes occur when leap second events occur due to a bug in the kernel.</p>\n","plugin":"leapsec_hrtimer_expire","error_key":"LEAPSEC_HRTIMER_EXPIRE_EARLY_OTHER_WARN","plugin_name":"System hang, high CPU usage, or application crash during a leap second event","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"leapsec_hrtimer_expire|LEAPSEC_HRTIMER_EXPIRE_EARLY_NTPD_WARN","description":"System hang, high CPU usage, or application crash during a leap second event with ntpd","category":"Stability","severity":"WARN","hitCount":0,"summary":"High CPU utilization and system crashes occur when leap second events occur due to a bug in the kernel.\n","summary_html":"<p>High CPU utilization and system crashes occur when leap second events occur due to a bug in the kernel.</p>\n","plugin":"leapsec_hrtimer_expire","error_key":"LEAPSEC_HRTIMER_EXPIRE_EARLY_NTPD_WARN","plugin_name":"System hang, high CPU usage, or application crash during a leap second event","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"leapsec_hrtimer_expire|LEAPSEC_HRTIMER_EXPIRE_EARLY_CHRONY_WARN","description":"System hang, high CPU usage, or application crash during a leap second event with chonyd","category":"Stability","severity":"WARN","hitCount":0,"summary":"High CPU utilization and system crashes occur when leap second events occur due to a bug in the kernel.\n","summary_html":"<p>High CPU utilization and system crashes occur when leap second events occur due to a bug in the kernel.</p>\n","plugin":"leapsec_hrtimer_expire","error_key":"LEAPSEC_HRTIMER_EXPIRE_EARLY_CHRONY_WARN","plugin_name":"System hang, high CPU usage, or application crash during a leap second event","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"ldap_auth_fail|LDAP_AUTH_VSFTPD_FAIL","description":"LDAP users fail to authenticate against vsftpd due to bug in vsftpd","category":"Stability","severity":"WARN","hitCount":0,"summary":"LDAP users fail to authenticate against vsftpd due to bug in vsftpd.\n","summary_html":"<p>LDAP users fail to authenticate against vsftpd due to bug in vsftpd.</p>\n","plugin":"ldap_auth_fail","error_key":"LDAP_AUTH_VSFTPD_FAIL","plugin_name":"LDAP users fail to authenticate against vsftpd due to bug in vsftpd","ansible":1,"rec_impact":1,"rec_likelihood":4,"resolution_risk":1,"acked":false},{"rule_id":"kdump_iommu|KDUMP_INTEL_IOMMU_ENABLED","description":"Failure to generate a vmcore when Intel IOMMU extensions are enabled","category":"Stability","severity":"WARN","hitCount":0,"summary":"Failure to generate a vmcore when Intel IOMMU extensions are enabled.\n","summary_html":"<p>Failure to generate a vmcore when Intel IOMMU extensions are enabled.</p>\n","plugin":"kdump_iommu","error_key":"KDUMP_INTEL_IOMMU_ENABLED","plugin_name":"Failure to generate a vmcore when Intel IOMMU extensions are enabled","ansible":1,"rec_impact":1,"rec_likelihood":4,"resolution_risk":3,"acked":false},{"rule_id":"kdump_hang_hp_bl460c_g7|HP_BL460C_G7_KDUMP_ISSUE","description":"Failure to generate a vmcore when using older P410i/P220i controller firmware versions on HP BL460C G7 systems","category":"Stability","severity":"WARN","hitCount":0,"summary":"Crash kernel can hang while initializing P410i or P220i controllers with older firmware versions on HP DL460c G7 server.","summary_html":"<p>Crash kernel can hang while initializing P410i or P220i controllers with older firmware versions on HP DL460c G7 server.</p>\n","plugin":"kdump_hang_hp_bl460c_g7","error_key":"HP_BL460C_G7_KDUMP_ISSUE","plugin_name":"Failure to generate a vmcore when using older P410i/P220i controller firmware versions on HP BL460C G7 systems","ansible":0,"rec_impact":1,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"isdn_hangup_issue|ISDN_HANGUP_ISSUE","description":"ISDN hangup causes spinlock","category":"Availability","severity":"WARN","hitCount":0,"summary":"The Integrated Services Digital Network(ISDN) device can hang due to a kernel bug.","summary_html":"<p>The Integrated Services Digital Network(ISDN) device can hang due to a kernel bug.</p>\n","plugin":"isdn_hangup_issue","error_key":"ISDN_HANGUP_ISSUE","plugin_name":"ISDN hangup causes spinlock","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"ip_fragment_packet_drop|IP_FRAGMENT_PACKET_DROP_WARN","description":"Network performance loss when IP fragmentation reaches its max threshold on multi-cpu hosts with a specific kernel version","category":"Availability","severity":"WARN","hitCount":0,"summary":"Multi-processor hosts running specific kernel versions have packet reassmebly issues when IP fragmentation reaches its max threshold.\n","summary_html":"<p>Multi-processor hosts running specific kernel versions have packet reassmebly issues when IP fragmentation reaches its max threshold.</p>\n","plugin":"ip_fragment_packet_drop","error_key":"IP_FRAGMENT_PACKET_DROP_WARN","plugin_name":"Packet reassemble failure when IP fragmentation reaches its max threshold in a multi-processor host where specific version of kernel is used","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ip_fragment_packet_drop|IP_FRAGMENT_PACKET_DROP_ERROR","description":"Network performance degredation when IP fragmentation reaches its max threshold on multi-cpu hosts with a specific kernel version","category":"Availability","severity":"WARN","hitCount":0,"summary":"Multi-processor hosts running specific kernel versions have packet reassmebly issues when IP fragmentation reaches its max threshold.\n","summary_html":"<p>Multi-processor hosts running specific kernel versions have packet reassmebly issues when IP fragmentation reaches its max threshold.</p>\n","plugin":"ip_fragment_packet_drop","error_key":"IP_FRAGMENT_PACKET_DROP_ERROR","plugin_name":"Packet reassemble failure when IP fragmentation reaches its max threshold in a multi-processor host where specific version of kernel is used","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"ipmi_list_corruption_crash|IPMI_LIST_CORRUPTION_CRASH","description":"Kernel panic occurs when running ipmitool command with specific kernels","category":"Stability","severity":"WARN","hitCount":0,"summary":"Kernel occasionally panics when running `ipmitool` command due to a bug in the ipmi message handler.\n","summary_html":"<p>Kernel occasionally panics when running <code>ipmitool</code> command due to a bug in the ipmi message handler.</p>\n","plugin":"ipmi_list_corruption_crash","error_key":"IPMI_LIST_CORRUPTION_CRASH","plugin_name":"Kernel panic occurs when running ipmitool command with specific kernels","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"intel5500_interrupt_remapping_failure|HAS_INTERRUPT_REMAPPING_PROBLEM","description":"Network instability due to intremap bug in Intel 5520 and 5500 chipsets","category":"Stability","severity":"WARN","hitCount":0,"summary":"Network instability due to intremap bug in Intel 5520 and 5500 chipsets.\n","summary_html":"<p>Network instability due to intremap bug in Intel 5520 and 5500 chipsets.</p>\n","plugin":"intel5500_interrupt_remapping_failure","error_key":"HAS_INTERRUPT_REMAPPING_PROBLEM","plugin_name":"Network instability due to intremap bug in Intel 5520 and 5500 chipsets","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"increase_nsslapd_cachesize_warning|INCREASE_NSSLAPD_CACHESIZE","description":"Reduced performance for LDAP queries when entry cache size is exceeded in 389 Directory Server","category":"Performance","severity":"WARN","hitCount":0,"summary":"389 Directory Server's entry cache is not large enough to hold the entire entry database in memory.\n","summary_html":"<p>389 Directory Server&#39;s entry cache is not large enough to hold the entire entry database in memory.</p>\n","plugin":"increase_nsslapd_cachesize_warning","error_key":"INCREASE_NSSLAPD_CACHESIZE","plugin_name":"Reduced performance for LDAP queries when entry cache size is exceeded in 389 Directory Server","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"incorrect_process_utimes|INCORRECT_PROCESS_UTIMES","description":"Incorrect process utimes due to a kernel bug","category":"Stability","severity":"WARN","hitCount":0,"summary":"A bug in RHEL6 kernels causes the \"utime\" (shown as the time field in ps) to be shown as a very long time when this is not the case. \n","summary_html":"<p>A bug in RHEL6 kernels causes the &quot;utime&quot; (shown as the time field in ps) to be shown as a very long time when this is not the case. </p>\n","plugin":"incorrect_process_utimes","error_key":"INCORRECT_PROCESS_UTIMES","plugin_name":"Incorrect process utimes due to a kernel bug","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"ilo|HP_ILO_ISSUE","description":"Memory corruption and subsequent kernel panics when hp-health service running on HP ProLiant G7 systems with specific iLO 3 firmware version","category":"Stability","severity":"WARN","hitCount":0,"summary":"Memory corruption and subsequent kernel panics happens when hp-health service running on HP ProLiant G7 systems with iLO 3 firmware versions 1.80 and earlier.\n","summary_html":"<p>Memory corruption and subsequent kernel panics happens when hp-health service running on HP ProLiant G7 systems with iLO 3 firmware versions 1.80 and earlier.</p>\n","plugin":"ilo","error_key":"HP_ILO_ISSUE","plugin_name":"Memory corruption and subsequent kernel panics when hp-health service running on HP ProLiant G7 systems with specific iLO 3 firmware version","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"ifdown_alias_interface|ALIAS_INTERFACE_IP_REMOVE","description":"Executing ifdown on alias interfaces can remove all IPv4 addresses when using the specific versions of initscripts package","category":"Availability","severity":"WARN","hitCount":0,"summary":"Executing `ifdown` on alias interfaces can remove all IPv4 addresses from the primary network interfaces when using `initscripts` package prior to **initscripts-9.03.50-1.el6**.\n","summary_html":"<p>Executing <code>ifdown</code> on alias interfaces can remove all IPv4 addresses from the primary network interfaces when using <code>initscripts</code> package prior to <strong>initscripts-9.03.50-1.el6</strong>.</p>\n","plugin":"ifdown_alias_interface","error_key":"ALIAS_INTERFACE_IP_REMOVE","plugin_name":"Executing ifdown on alias interfaces can remove all IPv4 addresses when using the specific versions of initscripts package","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"idm_smbv1_windows_unsupport|IDM_SMBV1_WINDOWS_UNSUPPORT_WARN","description":"SMBv1 is not supported by Windows since version 1709 and SMBv2 is not supported in current kernel","category":"Availability","severity":"WARN","hitCount":0,"summary":"SMBv1 is not supported by Windows since version 1709.\n","summary_html":"<p>SMBv1 is not supported by Windows since version 1709.</p>\n","plugin":"idm_smbv1_windows_unsupport","error_key":"IDM_SMBV1_WINDOWS_UNSUPPORT_WARN","plugin_name":"SMBv1 is not supported by Windows since version 1709","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"idm_smbv1_windows_unsupport|IDM_SMBV1_WINDOWS_UNSUPPORT_INFO","description":"SMBv1 is not supported by Windows since version 1709 but SMBv2 is supported in current kernel","category":"Availability","severity":"WARN","hitCount":0,"summary":"SMBv1 is not supported by Windows since version 1709.\n","summary_html":"<p>SMBv1 is not supported by Windows since version 1709.</p>\n","plugin":"idm_smbv1_windows_unsupport","error_key":"IDM_SMBV1_WINDOWS_UNSUPPORT_INFO","plugin_name":"SMBv1 is not supported by Windows since version 1709","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"idm_kvno_version_mismatch|KVNO_VERSION_MISMATCH","description":"Samba authentication fails when krb5.keytab kvno version does not match secrets.tdb","category":"Availability","severity":"WARN","hitCount":0,"summary":"Samba authentication will fail when the kerberos `krb5.keytab` kvno version does not match `secrets.tdb`\n","summary_html":"<p>Samba authentication will fail when the kerberos <code>krb5.keytab</code> kvno version does not match <code>secrets.tdb</code></p>\n","plugin":"idm_kvno_version_mismatch","error_key":"KVNO_VERSION_MISMATCH","plugin_name":"Samba authentication fails when krb5.keytab kvno version mismatches secrets.tdb","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"idm_ipv6_disabled|IPA_IPV6_DISABLED_INFO","description":"IPA server upgrade will fail when IPv6 is disabled","category":"Availability","severity":"WARN","hitCount":0,"summary":"IPA server upgrade fails when IPv6 is disabled.\n","summary_html":"<p>IPA server upgrade fails when IPv6 is disabled.</p>\n","plugin":"idm_ipv6_disabled","error_key":"IPA_IPV6_DISABLED_INFO","plugin_name":"Failure to upgrade IPA server when IPv6 is disabled","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"hypervisor_bridge_bonding_mode|HYPERVISOR_UNEXPECTED_BONDING_MODE","description":"Virtual Network will not work when network bridge is on bonding mode 5 or 6","category":"Availability","severity":"WARN","hitCount":0,"summary":"Virtual network will not work as expected when the virtual network port is attached to the network bridge which is on bonding interface with a bonding mode 5 or 6.\n","summary_html":"<p>Virtual network will not work as expected when the virtual network port is attached to the network bridge which is on bonding interface with a bonding mode 5 or 6.</p>\n","plugin":"hypervisor_bridge_bonding_mode","error_key":"HYPERVISOR_UNEXPECTED_BONDING_MODE","plugin_name":"Virtual network will not work as expected when using an incorrect networking bonding mode","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"hugepage_race_condition_issue|HUGETLB_ISSUE","description":"System crash when HugePages active due to race condition in kernel","category":"Stability","severity":"WARN","hitCount":0,"summary":"A race condition in hugetlb code in specific versions of the kernel in Red Hat Enterprise Linux 6 and 7 will cause a system crash when HugePages is activated and in use on the system, due to regression in the kernel.\n","summary_html":"<p>A race condition in hugetlb code in specific versions of the kernel in Red Hat Enterprise Linux 6 and 7 will cause a system crash when HugePages is activated and in use on the system, due to regression in the kernel.</p>\n","plugin":"hugepage_race_condition_issue","error_key":"HUGETLB_ISSUE","plugin_name":"System crash when HugePages active due to race condition in kernel","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"httpd_weak_ciphers|HTTPD_WEAK_CIPHERS_2","description":"Decreased security in httpd when using weak ciphers","category":"Security","severity":"WARN","hitCount":0,"summary":"Earlier ciphers in Apache `httpd` are considered weak and pose a security risk. Those ciphers should be disabled in favor of newer algorithms.","summary_html":"<p>Earlier ciphers in Apache <code>httpd</code> are considered weak and pose a security risk. Those ciphers should be disabled in favor of newer algorithms.</p>\n","plugin":"httpd_weak_ciphers","error_key":"HTTPD_WEAK_CIPHERS_2","plugin_name":"Decreased security in httpd when using weak ciphers","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"httpd_scoreboard_full|HTTPD_SCOREBOARD_FULL_WARN","description":"Unresponsive httpd service when encountering heavy traffic with the event MPM mode","category":"Availability","severity":"WARN","hitCount":0,"summary":"The httpd service becomes unresponsive when encountering heavy traffic with the **event** MPM mode.\n","summary_html":"<p>The httpd service becomes unresponsive when encountering heavy traffic with the <strong>event</strong> MPM mode.</p>\n","plugin":"httpd_scoreboard_full","error_key":"HTTPD_SCOREBOARD_FULL_WARN","plugin_name":"Unresponsive httpd service when encountering heavy traffic with the event MPM mode","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"httpd_restart_segfault_after_reload|HTTPD_RESTART_SEGFAULT_AFTER_RELOAD","description":"Graceful restart of httpd service fails with segmentation faults when logrotate is invoked","category":"Availability","severity":"WARN","hitCount":0,"summary":"Graceful restart of httpd service fails with segmentation faults when logrotate is invoked.\n","summary_html":"<p>Graceful restart of httpd service fails with segmentation faults when logrotate is invoked.</p>\n","plugin":"httpd_restart_segfault_after_reload","error_key":"HTTPD_RESTART_SEGFAULT_AFTER_RELOAD","plugin_name":"Graceful restart of httpd service fails with segmentation faults when logrotate is invoked","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"httpd_graceful_restart_too_long|HTTPD_GRACEFUL_RESTART_TOO_LONG_TIME_WARN","description":"Slow restarts for httpd when the StartServers parameter is set to a large value","category":"Availability","severity":"WARN","hitCount":0,"summary":"Slow restarts of the **httpd** service occur when the `StartServers` parameter is set to **64** or greater \n","summary_html":"<p>Slow restarts of the <strong>httpd</strong> service occur when the <code>StartServers</code> parameter is set to <strong>64</strong> or greater </p>\n","plugin":"httpd_graceful_restart_too_long","error_key":"HTTPD_GRACEFUL_RESTART_TOO_LONG_TIME_WARN","plugin_name":"Slow restart of the httpd service when StartServers is set to a large value in httpd config","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"httpd_fails_no_space_left|HTTPD_LOW_SEMAPHORES_ERROR","description":"The httpd service fails to start when semaphores are exhausted","category":"Stability","severity":"WARN","hitCount":0,"summary":"The httpd service fails to start when semaphores are exhausted.\n","summary_html":"<p>The httpd service fails to start when semaphores are exhausted.</p>\n","plugin":"httpd_fails_no_space_left","error_key":"HTTPD_LOW_SEMAPHORES_ERROR","plugin_name":"Orphaned semaphores have been detected in the system","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"httpd_fails_no_space_left|HTTPD_LOW_SEMAPHORES","description":"The httpd service will fail to start when semaphores are nealy exhausted","category":"Stability","severity":"WARN","hitCount":0,"summary":"System semaphores are nearly exhausted which will cause the httpd service to fail at startup.\n","summary_html":"<p>System semaphores are nearly exhausted which will cause the httpd service to fail at startup.</p>\n","plugin":"httpd_fails_no_space_left","error_key":"HTTPD_LOW_SEMAPHORES","plugin_name":"Orphaned semaphores have been detected in the system","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"httpd24_deprecated_order|DEPRECATED_ORDER_USED_INFO","description":"Unexpected behavior when using deprecated access control directives in httpd 2.4","category":"Availability","severity":"WARN","hitCount":1,"summary":"The httpd service does not work as expected when using old directives in httpd-2.4.\n","summary_html":"<p>The httpd service does not work as expected when using old directives in httpd-2.4.</p>\n","plugin":"httpd24_deprecated_order","error_key":"DEPRECATED_ORDER_USED_INFO","plugin_name":"Old directives (Order, Allow and Deny) don't work as expected in httpd-2.4","ansible":0,"rec_impact":1,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"hp_ilo4_nmi|HP_ILO4_NMI_ISSUE_DETECTED","description":"Kernel panic when using HP Proliant Gen8 servers with older iLO4 firmware versions","category":"Stability","severity":"WARN","hitCount":0,"summary":"Some combinations of iLO 4 firmware and the HP Watchdog daemon (hpwdt) can cause a kernel panic.","summary_html":"<p>Some combinations of iLO 4 firmware and the HP Watchdog daemon (hpwdt) can cause a kernel panic.</p>\n","plugin":"hp_ilo4_nmi","error_key":"HP_ILO4_NMI_ISSUE_DETECTED","plugin_name":"Kernel panic when using HP Proliant Gen8 servers with older iLO4 firmware versions","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"hp_dl380_g6_reboot|HP_DL380_G6_REBOOT","description":"HP ProLiant DL380 G6 unexpected reboot","category":"Stability","severity":"WARN","hitCount":0,"summary":"A limited number of Proliant DL380 G6 servers configured with assembly number 451277-001 system boards with a revision number earlier than 0 S (zero S) that were shipped prior to 16 November 2009 and that have a serial number earlier than xxx 947 xxxx might reboot unexpectedly.","summary_html":"<p>A limited number of Proliant DL380 G6 servers configured with assembly number 451277-001 system boards with a revision number earlier than 0 S (zero S) that were shipped prior to 16 November 2009 and that have a serial number earlier than xxx 947 xxxx might reboot unexpectedly.</p>\n","plugin":"hp_dl380_g6_reboot","error_key":"HP_DL380_G6_REBOOT","plugin_name":"HP ProLiant DL380 G6 unexpected reboot","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"hpsa_task_blocked|HPSA_TASK_BLOCKED_V1","description":"Hung tasks due to bug in HPSA driver","category":"Stability","severity":"WARN","hitCount":0,"summary":"Certain versions of the HP Smart Array (HPSA) driver can block the CPU's work queue for up to ten minutes which can lead to kernel panics, system hangs, or hung tasks.","summary_html":"<p>Certain versions of the HP Smart Array (HPSA) driver can block the CPU&#39;s work queue for up to ten minutes which can lead to kernel panics, system hangs, or hung tasks.</p>\n","plugin":"hpsa_task_blocked","error_key":"HPSA_TASK_BLOCKED_V1","plugin_name":"Hung tasks due to bug in HPSA driver","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"hpsa_target_reset_issue|HPSA_TARGET_RESET_ISSUE","description":"HP Smart Array logical drives was taken offline when the target reset failed","category":"Stability","severity":"WARN","hitCount":0,"summary":"HP Smart Array logical drives was taken offline when the target reset failed due to these drives do not support target reset.\n","summary_html":"<p>HP Smart Array logical drives was taken offline when the target reset failed due to these drives do not support target reset.</p>\n","plugin":"hpsa_target_reset_issue","error_key":"HPSA_TARGET_RESET_ISSUE","plugin_name":"HP Smart Array logical drives was taken offline when the target reset failed","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"heartbleed|USING_HEARTBLEED_SO","description":"OpenSSL with active processes vulnerable to information disclosure (CVE-2014-0160/Heartbleed)","category":"Security","severity":"WARN","hitCount":0,"summary":"The Heartbleed [CVE-2014-0160](https://access.redhat.com/security/cve/CVE-2014-0160) exploit allows an attacker to remotely obtain privileged regions of memory.","summary_html":"<p>The Heartbleed <a href=\"https://access.redhat.com/security/cve/CVE-2014-0160\">CVE-2014-0160</a> exploit allows an attacker to remotely obtain privileged regions of memory.</p>\n","plugin":"heartbleed","error_key":"USING_HEARTBLEED_SO","plugin_name":"OpenSSL vulnerable to information disclosure (CVE-2014-0160/Heartbleed)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"heartbleed|HAS_HEARTBLEED","description":"OpenSSL vulnerable to information disclosure (CVE-2014-0160/Heartbleed)","category":"Security","severity":"WARN","hitCount":2,"summary":"The Heartbleed [CVE-2014-0160](https://access.redhat.com/security/cve/CVE-2014-0160) exploit allows an attacker to remotely obtain privileged regions of memory.","summary_html":"<p>The Heartbleed <a href=\"https://access.redhat.com/security/cve/CVE-2014-0160\">CVE-2014-0160</a> exploit allows an attacker to remotely obtain privileged regions of memory.</p>\n","plugin":"heartbleed","error_key":"HAS_HEARTBLEED","plugin_name":"OpenSSL vulnerable to information disclosure (CVE-2014-0160/Heartbleed)","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"ha_node_fenced_issue|HA_FENCED_NO_IPMAPPING","description":"DNS is single point of failure in the cluster as there is no IP/hostnamemapping","category":"Stability","severity":"WARN","hitCount":0,"summary":"Cluster nodes are fenced frequently because DHCP and DNS is used for corosync interconnection. The DHCP client removes and re-adds an IP address periodically or DNS server fails to resolve domain name which results in corosync connection failure. \n","summary_html":"<p>Cluster nodes are fenced frequently because DHCP and DNS is used for corosync interconnection. The DHCP client removes and re-adds an IP address periodically or DNS server fails to resolve domain name which results in corosync connection failure. </p>\n","plugin":"ha_node_fenced_issue","error_key":"HA_FENCED_NO_IPMAPPING","plugin_name":"Cluster nodes are fenced frequently as DHCP is used for corosync interconnection","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"ha_node_fenced_issue|HA_FENCED_DHCP_ISSUE","description":"Cluster nodes are fenced frequently as DHCP is used for corosync interconnection","category":"Stability","severity":"WARN","hitCount":0,"summary":"Cluster nodes are fenced frequently because DHCP and DNS is used for corosync interconnection. The DHCP client removes and re-adds an IP address periodically or DNS server fails to resolve domain name which results in corosync connection failure. \n","summary_html":"<p>Cluster nodes are fenced frequently because DHCP and DNS is used for corosync interconnection. The DHCP client removes and re-adds an IP address periodically or DNS server fails to resolve domain name which results in corosync connection failure. </p>\n","plugin":"ha_node_fenced_issue","error_key":"HA_FENCED_DHCP_ISSUE","plugin_name":"Cluster nodes are fenced frequently as DHCP is used for corosync interconnection","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":true},{"rule_id":"guardium_super_prune_error|GUARDIUM_SUPER_PRUNE_ERROR","description":"Kernel crashes due to an issue of STAP v8.2","category":"Stability","severity":"WARN","hitCount":0,"summary":"IBM Guardium STAP version 8.2 can cause system crashes on Oracle RAC clusters.\n","summary_html":"<p>IBM Guardium STAP version 8.2 can cause system crashes on Oracle RAC clusters.</p>\n","plugin":"guardium_super_prune_error","error_key":"GUARDIUM_SUPER_PRUNE_ERROR","plugin_name":"Kernel crashes due to an issue of STAP v8.2","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"fs_resize2fs_bug|RESIZE2FS_BUG_INFO","description":"Failure to reduce ext4 file system size due to a bug in resize2fs utility (this issue may happen in the future)","category":"Stability","severity":"WARN","hitCount":0,"summary":"This host is running `e2fsprogs` package prior to `e2fsprogs-1.41.10-10.2.el6_5.1` that has a known issue when reducing the size of an ext4 file system. Eventually, any access into the file system will hang necessitating a reboot.\n","summary_html":"<p>This host is running <code>e2fsprogs</code> package prior to <code>e2fsprogs-1.41.10-10.2.el6_5.1</code> that has a known issue when reducing the size of an ext4 file system. Eventually, any access into the file system will hang necessitating a reboot.</p>\n","plugin":"fs_resize2fs_bug","error_key":"RESIZE2FS_BUG_INFO","plugin_name":"Failure to reduce ext4 file system size due to a bug in resize2fs utility","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"freeipmi_reboot|FREEIPMI_REBOOT","description":"RHEL6.5 system reboots repeatedly due to a bug of freeipmi (this issue may happen in the future)","category":"Stability","severity":"WARN","hitCount":0,"summary":"Systems running an earlier version of freeipmi are prone to resetting after running for an extended time.\n","summary_html":"<p>Systems running an earlier version of freeipmi are prone to resetting after running for an extended time.</p>\n","plugin":"freeipmi_reboot","error_key":"FREEIPMI_REBOOT","plugin_name":"RHEL system reboots abnormally due to a bug of freeipmi","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"filesystem_capacity|FILESYSTEM_CAPACITY","description":"Decreased stability and/or performance due to filesystem over 95% capacity","category":"Stability","severity":"WARN","hitCount":0,"summary":"File systems nearing full capacity can cause performance issues because blocks must be used from different block groups. \nBesides, file systems at or exceeding capacity will have stability issues because applications will no longer be able to write to the file system.\n","summary_html":"<p>File systems nearing full capacity can cause performance issues because blocks must be used from different block groups. \nBesides, file systems at or exceeding capacity will have stability issues because applications will no longer be able to write to the file system.</p>\n","plugin":"filesystem_capacity","error_key":"FILESYSTEM_CAPACITY","plugin_name":"Decreased stability and/or performance due to filesystem over 95% capacity","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"expand_maximum_resources_per_stack|EXPAND_MAXIMUM_RESOURCES_PER_STACK","description":"Deployment of additional compute nodes fails when exceeding maximum resources allowed per top-level stack","category":"Availability","severity":"WARN","hitCount":0,"summary":"Heat counts the number of resources in all nested stacks toward the max_resources_per_stack limit (default limit is 1000). Deploying additional compute nodes failure when max_resources_per_stack in heat.conf is exceeded.\n","summary_html":"<p>Heat counts the number of resources in all nested stacks toward the max_resources_per_stack limit (default limit is 1000). Deploying additional compute nodes failure when max_resources_per_stack in heat.conf is exceeded.</p>\n","plugin":"expand_maximum_resources_per_stack","error_key":"EXPAND_MAXIMUM_RESOURCES_PER_STACK","plugin_name":"Deployment of additional compute nodes fails when exceeding maximum resources allowed per top-level stack","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"docker_volumes_available|OSE_PERSISTENT_VOLUMES_UNAVAILABLE","description":"Docker storage access failure when volume listed in container mount list are missing in volumes directory","category":"Availability","severity":"WARN","hitCount":0,"summary":"Docker volume access failure when volumes listed in the container mount list are missing in volumes directory.\n","summary_html":"<p>Docker volume access failure when volumes listed in the container mount list are missing in volumes directory.</p>\n","plugin":"docker_volumes_available","error_key":"OSE_PERSISTENT_VOLUMES_UNAVAILABLE","plugin_name":"Storage access failure when volumes in container mount list are missing from volumes directory","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":2,"acked":false},{"rule_id":"docker_systemd_cgroup|SYSTEMD_IMAGE_CGROUP","description":"Containers that run systemd fail to run when container_manage_cgroup is off","category":"Availability","severity":"WARN","hitCount":0,"summary":"Containers that run systemd fail to run when container_manage_cgroup is off.\n","summary_html":"<p>Containers that run systemd fail to run when container_manage_cgroup is off.</p>\n","plugin":"docker_systemd_cgroup","error_key":"SYSTEMD_IMAGE_CGROUP","plugin_name":"Containers that run systemd fail to run when container_manage_cgroup is off","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_selinux_disabled|DOCKER_SELINUX_DISABLED","description":"Security and isolation between containers are weakened when SELinux is disabled","category":"Availability","severity":"WARN","hitCount":0,"summary":"SELinux provides a much stronger security posture for containers than cgroups and namespaces do alone. Security and isolation between containers are weakened when SELinux is disabled. Therefore, Red Hat strongly recommends enabling SELinux in enforcing mode at all times.\n","summary_html":"<p>SELinux provides a much stronger security posture for containers than cgroups and namespaces do alone. Security and isolation between containers are weakened when SELinux is disabled. Therefore, Red Hat strongly recommends enabling SELinux in enforcing mode at all times.</p>\n","plugin":"docker_selinux_disabled","error_key":"DOCKER_SELINUX_DISABLED","plugin_name":"Security and isolation between containers are weakened when SELinux is disabled","ansible":1,"rec_impact":1,"rec_likelihood":3,"resolution_risk":4,"acked":false},{"rule_id":"docker_run_mount|INHERIT_SUBSCRIPTION_FAILED","description":"Host entitlements not available to container due to bug in oci-systemd-hook","category":"Availability","severity":"WARN","hitCount":0,"summary":"Host entitlements not available to container due to bug in oci-systemd-hook.\n","summary_html":"<p>Host entitlements not available to container due to bug in oci-systemd-hook.</p>\n","plugin":"docker_run_mount","error_key":"INHERIT_SUBSCRIPTION_FAILED","plugin_name":"Host entitlements not available to container due to bug in oci-systemd-hook","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_restart_abnormal|DOCKER_UNIT_FILE_RESTART_POLICY","description":"Docker daemon continually restarts when systemd restart option is set to \"on-failure\"","category":"Availability","severity":"WARN","hitCount":0,"summary":"In the Docker daemon file /usr/lib/systemd/system/docker.service, when \"Restart\" is set as \"on-failure\" the Docker daemon is forced to restart even in cases where it couldn't be started because of configuration or other issues. This situation forces unnecessary restarts of the docker-storage-setup service in a loop, and also causes real error messages to be lost due to excessive restarts.","summary_html":"<p>In the Docker daemon file /usr/lib/systemd/system/docker.service, when &quot;Restart&quot; is set as &quot;on-failure&quot; the Docker daemon is forced to restart even in cases where it couldn&#39;t be started because of configuration or other issues. This situation forces unnecessary restarts of the docker-storage-setup service in a loop, and also causes real error messages to be lost due to excessive restarts.</p>\n","plugin":"docker_restart_abnormal","error_key":"DOCKER_UNIT_FILE_RESTART_POLICY","plugin_name":"Docker daemon continually restarts when systemd restart option is set to \"on-failure\"","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"docker_original_netns|ORIGINAL_NETNS_STOPPED","description":"Container network fails when referenced container is stopped","category":"Availability","severity":"WARN","hitCount":0,"summary":"Container network fails when referenced container is stopped.\n","summary_html":"<p>Container network fails when referenced container is stopped.</p>\n","plugin":"docker_original_netns","error_key":"ORIGINAL_NETNS_STOPPED","plugin_name":"Container network fails when referenced container is stopped","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":2,"acked":false},{"rule_id":"docker_multicontainers_resolvconf|CONTAINER_RESOLVE_FAILED","description":"Failed DNS resolution when containers share network namespace","category":"Availability","severity":"WARN","hitCount":0,"summary":"Failed DNS resolution when containers share network namespace.\n","summary_html":"<p>Failed DNS resolution when containers share network namespace.</p>\n","plugin":"docker_multicontainers_resolvconf","error_key":"CONTAINER_RESOLVE_FAILED","plugin_name":"Failed DNS resolution when containers share network namespace","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_lvm_autoextension|PERSISTENT_AUTOEXTENSION_FAIL","description":"High CPU usage when extending Docker thin pool due to insufficient free space in Volume Group","category":"Performance","severity":"WARN","hitCount":0,"summary":"High CPU usage when extending Docker thin pool due to insufficient free space in Volume Group.\n","summary_html":"<p>High CPU usage when extending Docker thin pool due to insufficient free space in Volume Group.</p>\n","plugin":"docker_lvm_autoextension","error_key":"PERSISTENT_AUTOEXTENSION_FAIL","plugin_name":"High CPU usage when extending Docker thin pool due to insufficient free space in Volume Group","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_logs_rotate|DOCKER_LOGS_ROTATE2","description":"Docker daemon is inoperative when docker registry fills up due to no log rotation","category":"Stability","severity":"WARN","hitCount":0,"summary":"Docker daemon is inoperative when docker registry fills up due to no log rotation\n","summary_html":"<p>Docker daemon is inoperative when docker registry fills up due to no log rotation</p>\n","plugin":"docker_logs_rotate","error_key":"DOCKER_LOGS_ROTATE2","plugin_name":"Docker daemon is inoperative when docker registry fills up due to no log rotation","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_journald_stop|DOCKER_CRASH_AFTER_JOURNALD_TERMINATES_INFO","description":"Docker daemon crashes when journald is stopped due to issue in golang","category":"Availability","severity":"WARN","hitCount":0,"summary":"Docker daemon crashes when journald is stopped due to issue in golang\n","summary_html":"<p>Docker daemon crashes when journald is stopped due to issue in golang</p>\n","plugin":"docker_journald_stop","error_key":"DOCKER_CRASH_AFTER_JOURNALD_TERMINATES_INFO","plugin_name":"Docker daemon crashes when journald is stopped due to issue in golang","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_hanging_highload|LOW_EVENT-TIMEOUT","description":"Docker daemon hangs under high I/O load when udev.event-timeout is not large enough in kernel","category":"Stability","severity":"WARN","hitCount":0,"summary":"Docker process hangs when there is high IO load caused by other IO actions.\n","summary_html":"<p>Docker process hangs when there is high IO load caused by other IO actions.</p>\n","plugin":"docker_hanging_highload","error_key":"LOW_EVENT-TIMEOUT","plugin_name":"Docker daemon hangs under high I/O load when udev.event-timeout is not large enough in kernel","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"docker_devicemapper_busy|DEVICEMAPPER_REMOVAL_FAILED","description":"Failed to remove container when its devicemapper device is referenced by other containers","category":"Stability","severity":"WARN","hitCount":0,"summary":"Failed to remove container when its devicemapper device is referenced by other containers.\n","summary_html":"<p>Failed to remove container when its devicemapper device is referenced by other containers.</p>\n","plugin":"docker_devicemapper_busy","error_key":"DEVICEMAPPER_REMOVAL_FAILED","plugin_name":"Failed to remove container when its devicemapper device is referenced by other containers","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"docker_deferred_remove|DEFERRED_REMOVAL_FAILED_INFO","description":"Running container fails when a race between device-mapper and docker daemon itself occurs (the issue may happen in the future)","category":"Availability","severity":"WARN","hitCount":0,"summary":"Running container fails when a race between device-mapper and docker daemon itself occurs due to a known bug of docker package.\n","summary_html":"<p>Running container fails when a race between device-mapper and docker daemon itself occurs due to a known bug of docker package.</p>\n","plugin":"docker_deferred_remove","error_key":"DEFERRED_REMOVAL_FAILED_INFO","plugin_name":"Running container fails when a race between device-mapper and docker daemon itself occurs","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_deferred_remove|DEFERRED_REMOVAL_FAILED_ERROR","description":"Running container fails when a race between device-mapper and docker daemon itself occurs (the issue has happened)","category":"Availability","severity":"WARN","hitCount":0,"summary":"Running container fails when a race between device-mapper and docker daemon itself occurs due to a known bug of docker package.\n","summary_html":"<p>Running container fails when a race between device-mapper and docker daemon itself occurs due to a known bug of docker package.</p>\n","plugin":"docker_deferred_remove","error_key":"DEFERRED_REMOVAL_FAILED_ERROR","plugin_name":"Running container fails when a race between device-mapper and docker daemon itself occurs","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"docker_daemon_huge_memory_usage|DOCKER_DAEMON_HUGE_MEMORY_USAGE","description":"Excessive memory consumption by the docker daemon on versions between docker-1.12.6-16 and docker-1.12.6-41","category":"Performance","severity":"WARN","hitCount":0,"summary":"Excessive memory consumption by the docker daemon on versions between docker-1.12.6-16 and docker-1.12.6-41.\n","summary_html":"<p>Excessive memory consumption by the docker daemon on versions between docker-1.12.6-16 and docker-1.12.6-41.</p>\n","plugin":"docker_daemon_huge_memory_usage","error_key":"DOCKER_DAEMON_HUGE_MEMORY_USAGE","plugin_name":"Docker daemon consumes huge amounts of memory when the docker version is between docker-1.12.6-16 and docker-1.12.6-41","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"disabled_monitoring_bond|DISABLED_MONITORING_BOND","description":"Link is not monitored when arp* or miimon* directive does not exist in BONDING_OPTS","category":"Availability","severity":"WARN","hitCount":0,"summary":"Link is not monitored when arp* or miimon* directive does not exist in BONDING_OPTS.\n","summary_html":"<p>Link is not monitored when arp* or miimon* directive does not exist in BONDING_OPTS.</p>\n","plugin":"disabled_monitoring_bond","error_key":"DISABLED_MONITORING_BOND","plugin_name":"Link is not monitored when arp* or miimon* directive does not exist in BONDING_OPTS","ansible":1,"rec_impact":1,"rec_likelihood":4,"resolution_risk":3,"acked":false},{"rule_id":"df_used_negative|DF_USED_NEGATIVE","description":"File system corruption detected when df command reports negative disk space","category":"Stability","severity":"WARN","hitCount":0,"summary":"File system corruption detected when df command reports negative disk space.\n","summary_html":"<p>File system corruption detected when df command reports negative disk space.</p>\n","plugin":"df_used_negative","error_key":"DF_USED_NEGATIVE","plugin_name":"File system corruption detected when df command reports negative disk space","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":4,"acked":false},{"rule_id":"degraded_software_raid|DEGRADED_SOFTWARE_RAID","description":"Software RAID set operating in a degraded state when a disk failure occurs on a redundant software RAID array","category":"Stability","severity":"WARN","hitCount":0,"summary":"Software RAID set operating in a degraded state when a disk failure occurs on a redundant software RAID array.","summary_html":"<p>Software RAID set operating in a degraded state when a disk failure occurs on a redundant software RAID array.</p>\n","plugin":"degraded_software_raid","error_key":"DEGRADED_SOFTWARE_RAID","plugin_name":"Software RAID set operating in a degraded state when a disk failure occurs on a redundant software RAID array","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2018_8897_kernel_popss|KERNEL_CVE_2018_8897_VULNERABLE_2","description":"Kernel vulnerable to local privilege escalation via exceptions triggered after the POP SS and MOV to SS instructions (CVE-2018-8897, CVE-2018-1087)","category":"Security","severity":"WARN","hitCount":8,"summary":"A flaw was found in the way the Linux kernel's KVM hypervisor handles exceptions triggered after the POP SS and MOV to SS instructions. It has been assigned [CVE-2018-8897](https://access.redhat.com/security/cve/CVE-2018-8897) and [CVE-2018-1087](https://access.redhat.com/security/cve/CVE-2018-1087). These issues could lead to denial of service for unpatched systems. These instructions hold delivery of interrupts, data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction.\n\nAn unprivileged KVM guest user could use this flaw to crash the guest or potentially escalate their privileges in the guest.\n\n","summary_html":"<p>A flaw was found in the way the Linux kernel&#39;s KVM hypervisor handles exceptions triggered after the POP SS and MOV to SS instructions. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2018-8897\">CVE-2018-8897</a> and <a href=\"https://access.redhat.com/security/cve/CVE-2018-1087\">CVE-2018-1087</a>. These issues could lead to denial of service for unpatched systems. These instructions hold delivery of interrupts, data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction.</p>\n<p>An unprivileged KVM guest user could use this flaw to crash the guest or potentially escalate their privileges in the guest.</p>\n","plugin":"CVE_2018_8897_kernel_popss","error_key":"KERNEL_CVE_2018_8897_VULNERABLE_2","plugin_name":"Kernel vulnerable to local privilege escalation via exceptions triggered after the POP SS and MOV to SS instructions (CVE-2018-8897, CVE-2018-1087)","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2018_3639_cpu_kernel|CVE_2018_3639_CPU_BAD_MICROCODE","description":"Kernel vulnerable to side-channel attacks in modern microprocessors using Speculative Store Bypass when CPU microcode is outdated (CVE-2018-3639)","category":"Security","severity":"WARN","hitCount":4,"summary":"An industry-wide issue was found in the manner in which many modern microprocessors have implemented speculative execution of instructions. It has been assigned [CVE-2018-3639](https://access.redhat.com/security/cve/CVE-2018-3639). Mitigations for this vulnerability require firmware/microcode updates from hardware vendors.\n\nAn unprivileged attacker can use this flaw to bypass restrictions in order to gain read access to privileged memory that would otherwise be inaccessible.\n","summary_html":"<p>An industry-wide issue was found in the manner in which many modern microprocessors have implemented speculative execution of instructions. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2018-3639\">CVE-2018-3639</a>. Mitigations for this vulnerability require firmware/microcode updates from hardware vendors.</p>\n<p>An unprivileged attacker can use this flaw to bypass restrictions in order to gain read access to privileged memory that would otherwise be inaccessible.</p>\n","plugin":"CVE_2018_3639_cpu_kernel","error_key":"CVE_2018_3639_CPU_BAD_MICROCODE","plugin_name":"Kernel vulnerable to side-channel attacks in modern microprocessors using Speculative Store Bypass (CVE-2018-3639)","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2018_3639_cpu_kernel|CVE_2018_3639_CPU_BAD_KERNEL","description":"Kernel vulnerable to side-channel attacks in modern microprocessors using Speculative Store Bypass (CVE-2018-3639)","category":"Security","severity":"WARN","hitCount":10,"summary":"An industry-wide issue was found in the manner in which many modern microprocessors have implemented speculative execution of instructions. It has been assigned [CVE-2018-3639](https://access.redhat.com/security/cve/CVE-2018-3639). Mitigations for this vulnerability require firmware/microcode updates from hardware vendors.\n\nAn unprivileged attacker can use this flaw to bypass restrictions in order to gain read access to privileged memory that would otherwise be inaccessible.\n","summary_html":"<p>An industry-wide issue was found in the manner in which many modern microprocessors have implemented speculative execution of instructions. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2018-3639\">CVE-2018-3639</a>. Mitigations for this vulnerability require firmware/microcode updates from hardware vendors.</p>\n<p>An unprivileged attacker can use this flaw to bypass restrictions in order to gain read access to privileged memory that would otherwise be inaccessible.</p>\n","plugin":"CVE_2018_3639_cpu_kernel","error_key":"CVE_2018_3639_CPU_BAD_KERNEL","plugin_name":"Kernel vulnerable to side-channel attacks in modern microprocessors using Speculative Store Bypass (CVE-2018-3639)","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2018_3639_cpu_kernel|CVE_2018_3639_CPU_BAD_CMDLINE","description":"Kernel vulnerable to side-channel attacks in modern microprocessors using Speculative Store Bypass when mitigation is disabled (CVE-2018-3639)","category":"Security","severity":"WARN","hitCount":0,"summary":"An industry-wide issue was found in the manner in which many modern microprocessors have implemented speculative execution of instructions. It has been assigned [CVE-2018-3639](https://access.redhat.com/security/cve/CVE-2018-3639). Mitigations for this vulnerability require firmware/microcode updates from hardware vendors.\n\nAn unprivileged attacker can use this flaw to bypass restrictions in order to gain read access to privileged memory that would otherwise be inaccessible.\n","summary_html":"<p>An industry-wide issue was found in the manner in which many modern microprocessors have implemented speculative execution of instructions. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2018-3639\">CVE-2018-3639</a>. Mitigations for this vulnerability require firmware/microcode updates from hardware vendors.</p>\n<p>An unprivileged attacker can use this flaw to bypass restrictions in order to gain read access to privileged memory that would otherwise be inaccessible.</p>\n","plugin":"CVE_2018_3639_cpu_kernel","error_key":"CVE_2018_3639_CPU_BAD_CMDLINE","plugin_name":"Kernel vulnerable to side-channel attacks in modern microprocessors using Speculative Store Bypass (CVE-2018-3639)","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"CVE_2018_1112_glusterfs|GLUSTERFS_CVE_2018_1112","description":"glusterfs vulnerable to unrestricted access (CVE-2018-1112)","category":"Security","severity":"WARN","hitCount":0,"summary":"Security fix for CVE-2018-1088 introduced vulnerability in glusterfs server which allows any unauthenticated gluster client from any network which can access gluster servers to mount gluster volumes.\n","summary_html":"<p>Security fix for CVE-2018-1088 introduced vulnerability in glusterfs server which allows any unauthenticated gluster client from any network which can access gluster servers to mount gluster volumes.</p>\n","plugin":"CVE_2018_1112_glusterfs","error_key":"GLUSTERFS_CVE_2018_1112","plugin_name":"glusterfs vulnerable to unrestricted access (CVE-2018-1112)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2018_1111_dhcp|WARN_CVE_2018_1111_DHCP_2","description":"NetworkManager DHCP potentially vulnerable to remote code execution (CVE-2018-1111)","category":"Security","severity":"WARN","hitCount":7,"summary":"A command injection vulnerability was found in the DHCP script provided by `dhclient`, located in `/etc/NetworkManager/dispatcher.d/11-dhclient`. An attacker on the local network who is able to spoof DHCP responses or run a malicious DHCP server can execute arbitrary commands with root privileges on DHCP client systems by exploiting this vulnerability.\n","summary_html":"<p>A command injection vulnerability was found in the DHCP script provided by <code>dhclient</code>, located in <code>/etc/NetworkManager/dispatcher.d/11-dhclient</code>. An attacker on the local network who is able to spoof DHCP responses or run a malicious DHCP server can execute arbitrary commands with root privileges on DHCP client systems by exploiting this vulnerability.</p>\n","plugin":"CVE_2018_1111_dhcp","error_key":"WARN_CVE_2018_1111_DHCP_2","plugin_name":"NetworkManager DHCP script vulnerable to remote code execution (CVE-2018-1111)","ansible":0,"rec_impact":4,"rec_likelihood":1,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2017_8779_rpc|CVE_2017_8779_WARN","description":"RPC vulnerable to memory leak via crafted XDR strings with mitigation (CVE-2017-8779/rpcbomb)","category":"Security","severity":"WARN","hitCount":0,"summary":"A bug was found in XDR parsing code in libtirpc, libntirpc and glibc which could lead to memory being allocated and never freed. Depending on the setting of sysctl vm.overcommit_memory, the process could exhaust virtual memory, causing a denial of service.\n","summary_html":"<p>A bug was found in XDR parsing code in libtirpc, libntirpc and glibc which could lead to memory being allocated and never freed. Depending on the setting of sysctl vm.overcommit_memory, the process could exhaust virtual memory, causing a denial of service.</p>\n","plugin":"CVE_2017_8779_rpc","error_key":"CVE_2017_8779_WARN","plugin_name":"RPC vulnerable to memory leak via crafted XDR strings (CVE-2017-8779/rpcbomb)","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2017_8779_rpc|CVE_2017_8779_VULNERABLE","description":"RPC vulnerable to memory leak via crafted XDR strings (CVE-2017-8779/rpcbomb)","category":"Security","severity":"WARN","hitCount":0,"summary":"A bug was found in XDR parsing code in libtirpc, libntirpc and glibc which could lead to memory being allocated and never freed. Depending on the setting of sysctl vm.overcommit_memory, the process could exhaust virtual memory, causing a denial of service.\n","summary_html":"<p>A bug was found in XDR parsing code in libtirpc, libntirpc and glibc which could lead to memory being allocated and never freed. Depending on the setting of sysctl vm.overcommit_memory, the process could exhaust virtual memory, causing a denial of service.</p>\n","plugin":"CVE_2017_8779_rpc","error_key":"CVE_2017_8779_VULNERABLE","plugin_name":"RPC vulnerable to memory leak via crafted XDR strings (CVE-2017-8779/rpcbomb)","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2017_7184_kernel|KERNEL_CVE_2017_7184_EXPLOITABLE_2","description":"Kernel vulnerable to local privilege escalation via CAP_NET_ADMIN capability (CVE-2017-7184)","category":"Security","severity":"WARN","hitCount":0,"summary":"An out-of-bounds flaw was found within the Linux kernel xfrm framework for transforming IP packets. It has been assigned [CVE-2017-7184](https://access.redhat.com/security/cve/CVE-2017-7184). An unprivileged local user could use this flaw to execute arbitrary code in kernel memory and increase their privileges on the system.","summary_html":"<p>An out-of-bounds flaw was found within the Linux kernel xfrm framework for transforming IP packets. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2017-7184\">CVE-2017-7184</a>. An unprivileged local user could use this flaw to execute arbitrary code in kernel memory and increase their privileges on the system.</p>\n","plugin":"CVE_2017_7184_kernel","error_key":"KERNEL_CVE_2017_7184_EXPLOITABLE_2","plugin_name":"Kernel vulnerable to local privilege escalation via CAP_NET_ADMIN capability (CVE-2017-7184)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_6074_kernel|KERNEL_CVE_2017_6074","description":"Kernel vulnerable to local privilege escalation via DCCP module (CVE-2017-6074)","category":"Security","severity":"WARN","hitCount":1,"summary":"A use-after-free flaw was found in the Linux kernel IPv6 DCCP network protocol code. It has been assigned [CVE-2017-6074](https://access.redhat.com/security/cve/CVE-2017-6074). An unprivileged local user could use this flaw to execute arbitrary code in kernel memory and increase their privileges on the system.\n","summary_html":"<p>A use-after-free flaw was found in the Linux kernel IPv6 DCCP network protocol code. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2017-6074\">CVE-2017-6074</a>. An unprivileged local user could use this flaw to execute arbitrary code in kernel memory and increase their privileges on the system.</p>\n","plugin":"CVE_2017_6074_kernel","error_key":"KERNEL_CVE_2017_6074","plugin_name":"Kernel vulnerable to local privilege escalation via DCCP module (CVE-2017-6074)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_5753_4_cpu_kernel|KERNEL_CVE_2017_5753_4_CPU_ERROR_3","description":"Kernel vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5753/Spectre, CVE-2017-5715/Spectre, CVE-2017-5754/Meltdown)","category":"Security","severity":"WARN","hitCount":11,"summary":"A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as [CVE-2017-5753 / CVE-2017-5715 / Spectre](https://access.redhat.com/security/cve/CVE-2017-5753) and [CVE-2017-5754 / Meltdown](https://access.redhat.com/security/cve/CVE-2017-5754).\n","summary_html":"<p>A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-5753\">CVE-2017-5753 / CVE-2017-5715 / Spectre</a> and <a href=\"https://access.redhat.com/security/cve/CVE-2017-5754\">CVE-2017-5754 / Meltdown</a>.</p>\n","plugin":"CVE_2017_5753_4_cpu_kernel","error_key":"KERNEL_CVE_2017_5753_4_CPU_ERROR_3","plugin_name":"Kernel vulnerable to side-channel attacks in modern microprocessors (Meltdown, Spectre)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_VIRTKERNEL","description":"Virtualization and kernel vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","category":"Security","severity":"WARN","hitCount":1,"summary":"A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as [CVE-2017-5715 / Spectre](https://access.redhat.com/security/cve/CVE-2017-5715).\n","summary_html":"<p>A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-5715\">CVE-2017-5715 / Spectre</a>.</p>\n","plugin":"CVE_2017_5715_cpu_virt","error_key":"VIRT_CVE_2017_5715_CPU_3_VIRTKERNEL","plugin_name":"Virtualization vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_ONLYVIRT","description":"Virtualization vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","category":"Security","severity":"WARN","hitCount":0,"summary":"A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as [CVE-2017-5715 / Spectre](https://access.redhat.com/security/cve/CVE-2017-5715).\n","summary_html":"<p>A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-5715\">CVE-2017-5715 / Spectre</a>.</p>\n","plugin":"CVE_2017_5715_cpu_virt","error_key":"VIRT_CVE_2017_5715_CPU_3_ONLYVIRT","plugin_name":"Virtualization vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_ONLYKERNEL","description":"Kernel vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","category":"Security","severity":"WARN","hitCount":5,"summary":"A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as [CVE-2017-5715 / Spectre](https://access.redhat.com/security/cve/CVE-2017-5715).\n","summary_html":"<p>A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-5715\">CVE-2017-5715 / Spectre</a>.</p>\n","plugin":"CVE_2017_5715_cpu_virt","error_key":"VIRT_CVE_2017_5715_CPU_3_ONLYKERNEL","plugin_name":"Virtualization vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_ONLYDRACUT","description":"Dracut vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","category":"Security","severity":"WARN","hitCount":0,"summary":"A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as [CVE-2017-5715 / Spectre](https://access.redhat.com/security/cve/CVE-2017-5715).\n","summary_html":"<p>A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-5715\">CVE-2017-5715 / Spectre</a>.</p>\n","plugin":"CVE_2017_5715_cpu_virt","error_key":"VIRT_CVE_2017_5715_CPU_3_ONLYDRACUT","plugin_name":"Virtualization vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2017_5715_cpu_virt|VIRT_CVE_2017_5715_CPU_3_DRACUTKERNEL","description":"Dracut and kernel vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","category":"Security","severity":"WARN","hitCount":0,"summary":"A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as [CVE-2017-5715 / Spectre](https://access.redhat.com/security/cve/CVE-2017-5715).\n","summary_html":"<p>A vulnerability was discovered in modern microprocessors supported by the kernel, whereby an unprivileged attacker can use this flaw to bypass restrictions to gain read access to privileged memory.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-5715\">CVE-2017-5715 / Spectre</a>.</p>\n","plugin":"CVE_2017_5715_cpu_virt","error_key":"VIRT_CVE_2017_5715_CPU_3_DRACUTKERNEL","plugin_name":"Virtualization vulnerable to side-channel attacks in modern microprocessors (CVE-2017-5715/Spectre)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_2636_kernel|KERNEL_CVE_2017_2636","description":"Kernel vulnerable to local privilege escalation via n_hdlc module (CVE-2017-2636)","category":"Security","severity":"WARN","hitCount":3,"summary":"A vulnerability in the Linux kernel allowing local privilege escalation was discovered.\nThe issue was reported as [CVE-2017-2636](https://access.redhat.com/security/cve/CVE-2017-2636).\n","summary_html":"<p>A vulnerability in the Linux kernel allowing local privilege escalation was discovered.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-2636\">CVE-2017-2636</a>.</p>\n","plugin":"CVE_2017_2636_kernel","error_key":"KERNEL_CVE_2017_2636","plugin_name":"Kernel vulnerable to local privilege escalation via n_hdlc module (CVE-2017-2636)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_12616_tomcat|TOMCAT_CVE_2017_12616_USED_2","description":"Tomcat with running process vulnerable to information disclosure when using VirtualDirContext (CVE-2017-12616)","category":"Security","severity":"WARN","hitCount":0,"summary":"A flaw was found in the Apache Tomcat. It is possible to bypass security constraints for resources served by the VirtualDirContext using a specially crafted request.\n","summary_html":"<p>A flaw was found in the Apache Tomcat. It is possible to bypass security constraints for resources served by the VirtualDirContext using a specially crafted request.</p>\n","plugin":"CVE_2017_12616_tomcat","error_key":"TOMCAT_CVE_2017_12616_USED_2","plugin_name":"Tomcat vulnerable to information disclosure when using VirtualDirContext (CVE-2017-12616)","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_12616_tomcat|TOMCAT_CVE_2017_12616_2","description":"Tomcat vulnerable to information disclosure when using VirtualDirContext (CVE-2017-12616)","category":"Security","severity":"WARN","hitCount":0,"summary":"A flaw was found in the Apache Tomcat. It is possible to bypass security constraints for resources served by the VirtualDirContext using a specially crafted request.\n","summary_html":"<p>A flaw was found in the Apache Tomcat. It is possible to bypass security constraints for resources served by the VirtualDirContext using a specially crafted request.</p>\n","plugin":"CVE_2017_12616_tomcat","error_key":"TOMCAT_CVE_2017_12616_2","plugin_name":"Tomcat vulnerable to information disclosure when using VirtualDirContext (CVE-2017-12616)","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_1000405_kernel|KERNEL_CVE_2017_1000405_VULNERABLE","description":"Kernel vulnerable to memory corruption via permission bypass (CVE-2017-1000405)","category":"Security","severity":"WARN","hitCount":0,"summary":"A flaw was found in the Linux kernel's memory subsystem. An unprivileged local user could use this flaw to gain write access to otherwise read-only memory mappings, and modify read-only pages which can influence how userspace applications behave.\n","summary_html":"<p>A flaw was found in the Linux kernel&#39;s memory subsystem. An unprivileged local user could use this flaw to gain write access to otherwise read-only memory mappings, and modify read-only pages which can influence how userspace applications behave.</p>\n","plugin":"CVE_2017_1000405_kernel","error_key":"KERNEL_CVE_2017_1000405_VULNERABLE","plugin_name":"Kernel vulnerable to memory corruption via permission bypass (CVE-2017-1000405)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_1000368_sudo|CVE_2017_1000368_SUDO_2","description":"sudo vulnerable to local privilege escalation via process TTY name parsing (CVE-2017-1000368) impact: Local Privilege Escalation","category":"Security","severity":"WARN","hitCount":3,"summary":"A local privilege escalation flaw was found in `sudo`. A local user having sudo access on the system,\ncould use this flaw to execute arbitrary commands as root. This issue was reported as\n[CVE-2017-1000368](https://access.redhat.com/security/cve/CVE-2017-1000368)\n","summary_html":"<p>A local privilege escalation flaw was found in <code>sudo</code>. A local user having sudo access on the system,\ncould use this flaw to execute arbitrary commands as root. This issue was reported as\n<a href=\"https://access.redhat.com/security/cve/CVE-2017-1000368\">CVE-2017-1000368</a></p>\n","plugin":"CVE_2017_1000368_sudo","error_key":"CVE_2017_1000368_SUDO_2","plugin_name":"sudo vulnerable to local privilege escalation via process TTY name parsing (CVE-2017-1000368) impact: Local Privilege Escalation","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2017_1000366_glibc|CVE_2017_1000364_KERNEL_CVE_2017_1000366_GLIBC_EXPLOITABLE","description":"Kernel and glibc vulnerable to local privilege escalation via stack and heap memory clash (CVE-2017-1000364 and CVE-2017-1000366)","category":"Security","severity":"WARN","hitCount":3,"summary":"A flaw was found in the way memory is being allocated on the stack for user space binaries. It has been assigned [CVE-2017-1000364](https://access.redhat.com/security/cve/CVE-2017-1000364) and [CVE-2017-1000366](https://access.redhat.com/security/cve/CVE-2017-1000366). An unprivileged local user can use this flaw to execute arbitrary code as root and increase their privileges on the system.\n","summary_html":"<p>A flaw was found in the way memory is being allocated on the stack for user space binaries. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2017-1000364\">CVE-2017-1000364</a> and <a href=\"https://access.redhat.com/security/cve/CVE-2017-1000366\">CVE-2017-1000366</a>. An unprivileged local user can use this flaw to execute arbitrary code as root and increase their privileges on the system.</p>\n","plugin":"CVE_2017_1000366_glibc","error_key":"CVE_2017_1000364_KERNEL_CVE_2017_1000366_GLIBC_EXPLOITABLE","plugin_name":"Kernel and glibc vulnerable to local privilege escalation via stack and heap memory clash (CVE-2017-1000364 and CVE-2017-1000366)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_1000253_loadelf|KERNEL_CVE_2017_1000253_VULNERABLE","description":"Kernel is vulnerable to memory corruption or local privilege escalation (CVE-2017-1000253)","category":"Security","severity":"WARN","hitCount":4,"summary":"A flaw was found in the Linux kernel's implementation of ELF loading for binaries with large data segments. This can lead to memory corruption and privilege escalation when a malicious binary is mapped.\n","summary_html":"<p>A flaw was found in the Linux kernel&#39;s implementation of ELF loading for binaries with large data segments. This can lead to memory corruption and privilege escalation when a malicious binary is mapped.</p>\n","plugin":"CVE_2017_1000253_loadelf","error_key":"KERNEL_CVE_2017_1000253_VULNERABLE","plugin_name":"Kernel vulnerable to memory corruption or local privilege escalation","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_1000253_loadelf|KERNEL_CVE_2017_1000253_MITIGATION_INCOMPLETE","description":"Kernel is vulnerable to memory corruption or local privilege escalation  with incomplete mitigation (CVE-2017-1000253)","category":"Security","severity":"WARN","hitCount":0,"summary":"A flaw was found in the Linux kernel's implementation of ELF loading for binaries with large data segments. This can lead to memory corruption and privilege escalation when a malicious binary is mapped.\n","summary_html":"<p>A flaw was found in the Linux kernel&#39;s implementation of ELF loading for binaries with large data segments. This can lead to memory corruption and privilege escalation when a malicious binary is mapped.</p>\n","plugin":"CVE_2017_1000253_loadelf","error_key":"KERNEL_CVE_2017_1000253_MITIGATION_INCOMPLETE","plugin_name":"Kernel vulnerable to memory corruption or local privilege escalation","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"CVE_2017_1000251_kernel_blueborne|KERNEL_CVE_2017_1000251_POSSIBLE_RCE","description":"Kernel vulnerable to remote code execution via Bluetooth stack (CVE-2017-1000251/Blueborne)","category":"Security","severity":"WARN","hitCount":0,"summary":"A vulnerability in the Linux kernel allowing denial of service, or potentially remote code execution, through Bluetooth stack has been discovered.\nThe issue was reported as [CVE-2017-1000251](https://access.redhat.com/security/cve/CVE-2017-1000251).\n","summary_html":"<p>A vulnerability in the Linux kernel allowing denial of service, or potentially remote code execution, through Bluetooth stack has been discovered.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-1000251\">CVE-2017-1000251</a>.</p>\n","plugin":"CVE_2017_1000251_kernel_blueborne","error_key":"KERNEL_CVE_2017_1000251_POSSIBLE_RCE","plugin_name":"Kernel vulnerable to denial of service or remote code execution via Bluetooth stack (Blueborne)","ansible":1,"rec_impact":4,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_1000251_kernel_blueborne|KERNEL_CVE_2017_1000251_POSSIBLE_DOS","description":"Kernel vulnerable to denial of service via Bluetooth stack (CVE-2017-1000251/Blueborne)","category":"Security","severity":"WARN","hitCount":4,"summary":"A vulnerability in the Linux kernel allowing denial of service, or potentially remote code execution, through Bluetooth stack has been discovered.\nThe issue was reported as [CVE-2017-1000251](https://access.redhat.com/security/cve/CVE-2017-1000251).\n","summary_html":"<p>A vulnerability in the Linux kernel allowing denial of service, or potentially remote code execution, through Bluetooth stack has been discovered.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-1000251\">CVE-2017-1000251</a>.</p>\n","plugin":"CVE_2017_1000251_kernel_blueborne","error_key":"KERNEL_CVE_2017_1000251_POSSIBLE_DOS","plugin_name":"Kernel vulnerable to denial of service or remote code execution via Bluetooth stack (Blueborne)","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2017_1000250_bluez|BLUEZ_CVE_2017_1000250","description":"Information disclosure vulnerability in BlueZ via crafted SDP requests (CVE-2017-1000250)","category":"Security","severity":"WARN","hitCount":0,"summary":"A vulnerability in the Bluetooth service allowing information disclosure through SDP requests has been discovered.\nThe issue was reported as [CVE-2017-1000250](https://access.redhat.com/security/cve/CVE-2017-1000250).\n","summary_html":"<p>A vulnerability in the Bluetooth service allowing information disclosure through SDP requests has been discovered.\nThe issue was reported as <a href=\"https://access.redhat.com/security/cve/CVE-2017-1000250\">CVE-2017-1000250</a>.</p>\n","plugin":"CVE_2017_1000250_bluez","error_key":"BLUEZ_CVE_2017_1000250","plugin_name":"Bluetooth service vulnerable to information disclosure","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2016_9962_docker|CVE_2015_9962_DOCKER_MORE_SEVERE_2","description":"Efficient local privilege escalation via ptrace using outdated Docker (CVE-2016-9962)","category":"Security","severity":"WARN","hitCount":0,"summary":"A flaw in Docker has been discovered that allows an attacker to escape out of an unprivileged container. It has been assigned [CVE-2016-9962](https://access.redhat.com/security/cve/CVE-2016-9962). The detected version of Docker is considerably outdated and can be exploited more easily than versions 1.10.3 and newer. The vulnerability is mitigated under the default SELinux configuration, but the detected configuration is vulnerable because SELinux is either disabled or set to permissive mode. \n","summary_html":"<p>A flaw in Docker has been discovered that allows an attacker to escape out of an unprivileged container. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2016-9962\">CVE-2016-9962</a>. The detected version of Docker is considerably outdated and can be exploited more easily than versions 1.10.3 and newer. The vulnerability is mitigated under the default SELinux configuration, but the detected configuration is vulnerable because SELinux is either disabled or set to permissive mode. </p>\n","plugin":"CVE_2016_9962_docker","error_key":"CVE_2015_9962_DOCKER_MORE_SEVERE_2","plugin_name":"Local privilege escalation via ptrace using Docker (CVE-2016-9962)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2016_9962_docker|CVE_2015_9962_DOCKER_LESS_SEVERE_2","description":"Local privilege escalation via ptrace using Docker (CVE-2016-9962)","category":"Security","severity":"WARN","hitCount":0,"summary":"A flaw in Docker has been discovered that allows an attacker to escape out of an unprivileged container. It has been assigned [CVE-2016-9962](https://access.redhat.com/security/cve/CVE-2016-9962). The vulnerability is mitigated under the default SELinux configuration, but the detected configuration is vulnerable because SELinux is either disabled or set to permissive mode. \n","summary_html":"<p>A flaw in Docker has been discovered that allows an attacker to escape out of an unprivileged container. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2016-9962\">CVE-2016-9962</a>. The vulnerability is mitigated under the default SELinux configuration, but the detected configuration is vulnerable because SELinux is either disabled or set to permissive mode. </p>\n","plugin":"CVE_2016_9962_docker","error_key":"CVE_2015_9962_DOCKER_LESS_SEVERE_2","plugin_name":"Local privilege escalation via ptrace using Docker (CVE-2016-9962)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"CVE_2016_5195_kernel|KERNEL_CVE_2016_5195_2","description":"Kernel vulnerable to privilege escalation via permission bypass (CVE-2016-5195)","category":"Security","severity":"WARN","hitCount":3,"summary":"A flaw was found in the Linux kernel's memory subsystem. An unprivileged local user could use this flaw to write to files they would normally only have read-only access to and thus increase their privileges on the system.","summary_html":"<p>A flaw was found in the Linux kernel&#39;s memory subsystem. An unprivileged local user could use this flaw to write to files they would normally only have read-only access to and thus increase their privileges on the system.</p>\n","plugin":"CVE_2016_5195_kernel","error_key":"KERNEL_CVE_2016_5195_2","plugin_name":"Kernel vulnerable to privilege escalation via permission bypass (CVE-2016-5195)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2016_0800_openssl_drown|OPENSSL_CVE_2016_0800_SPECIAL_DROWN","description":"OpenSSL vulnerable to very efficient session decryption (CVE-2016-0800/Special DROWN)","category":"Security","severity":"WARN","hitCount":0,"summary":"A new cross-protocol attack against SSLv2 protocol has been found. It has been assigned [CVE-2016-0800](https://access.redhat.com/security/cve/CVE-2016-0800) and is referred to as DROWN - Decrypting RSA using Obsolete and Weakened eNcryption. An attacker can decrypt passively collected TLS sessions between up-to-date client and server which supports SSLv2.","summary_html":"<p>A new cross-protocol attack against SSLv2 protocol has been found. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2016-0800\">CVE-2016-0800</a> and is referred to as DROWN - Decrypting RSA using Obsolete and Weakened eNcryption. An attacker can decrypt passively collected TLS sessions between up-to-date client and server which supports SSLv2.</p>\n","plugin":"CVE_2016_0800_openssl_drown","error_key":"OPENSSL_CVE_2016_0800_SPECIAL_DROWN","plugin_name":"OpenSSL vulnerable to session decryption (CVE-2016-0800/DROWN)","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"CVE_2016_0800_openssl_drown|OPENSSL_CVE_2016_0800_DROWN","description":"OpenSSL vulnerable to session decryption (CVE-2016-0800/DROWN)","category":"Security","severity":"WARN","hitCount":0,"summary":"A new cross-protocol attack against SSLv2 protocol has been found. It has been assigned [CVE-2016-0800](https://access.redhat.com/security/cve/CVE-2016-0800) and is referred to as DROWN - Decrypting RSA using Obsolete and Weakened eNcryption. An attacker can decrypt passively collected TLS sessions between up-to-date client and server which supports SSLv2.","summary_html":"<p>A new cross-protocol attack against SSLv2 protocol has been found. It has been assigned <a href=\"https://access.redhat.com/security/cve/CVE-2016-0800\">CVE-2016-0800</a> and is referred to as DROWN - Decrypting RSA using Obsolete and Weakened eNcryption. An attacker can decrypt passively collected TLS sessions between up-to-date client and server which supports SSLv2.</p>\n","plugin":"CVE_2016_0800_openssl_drown","error_key":"OPENSSL_CVE_2016_0800_DROWN","plugin_name":"OpenSSL vulnerable to session decryption (CVE-2016-0800/DROWN)","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"CVE_2016_0728_kernel|KERNEL_CVE-2016-0728","description":"Kernel key management subsystem vulnerable to local privilege escalation (CVE-2016-0728)","category":"Security","severity":"WARN","hitCount":1,"summary":"A vulnerability in the Linux kernel allowing local privilege escalation was discovered. The issue was reported as [CVE-2016-0728](https://access.redhat.com/security/cve/cve-2016-0728).","summary_html":"<p>A vulnerability in the Linux kernel allowing local privilege escalation was discovered. The issue was reported as <a href=\"https://access.redhat.com/security/cve/cve-2016-0728\">CVE-2016-0728</a>.</p>\n","plugin":"CVE_2016_0728_kernel","error_key":"KERNEL_CVE-2016-0728","plugin_name":"Kernel key management subsystem vulnerable to local privilege escalation (CVE-2016-0728)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"CVE_2015_3456|VENOM_CVE_2015_3456","description":"QEMU locally vulnerable via virtual Floppy Disk Controller (CVE-2015-3456/VENOM)","category":"Security","severity":"WARN","hitCount":0,"summary":"A security flaw has been discovered in the QEMU component of the KVM/QEMU and Xen hypervisors, which is documented in [CVE-2015-3456](https://access.redhat.com/security/cve/CVE-2015-3456).","summary_html":"<p>A security flaw has been discovered in the QEMU component of the KVM/QEMU and Xen hypervisors, which is documented in <a href=\"https://access.redhat.com/security/cve/CVE-2015-3456\">CVE-2015-3456</a>.</p>\n","plugin":"CVE_2015_3456","error_key":"VENOM_CVE_2015_3456","plugin_name":"QEMU locally vulnerable via virtual Floppy Disk Controller (CVE-2015-3456/VENOM)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"cronjob_to_clear_heat_raw_templates|NO_CRONJOB_TO_CLEAR_HEAT_RAW_TEMPLATES","description":"Openstack commands timeout when heat raw_templates table is too large on controller or director node","category":"Availability","severity":"WARN","hitCount":0,"summary":"Openstack commands timeout when heat raw_templates table is too large on controller or director node due to no cron job to clear heat.raw_teplates table.\n","summary_html":"<p>Openstack commands timeout when heat raw_templates table is too large on controller or director node due to no cron job to clear heat.raw_teplates table.</p>\n","plugin":"cronjob_to_clear_heat_raw_templates","error_key":"NO_CRONJOB_TO_CLEAR_HEAT_RAW_TEMPLATES","plugin_name":"Openstack commands timeout when heat raw_templates table is too large on controller or director node","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"cronjob_4_nova_soft_deleted_rows|NO_NOVA_CRONJOB_TO_CLEAR_SOFT_DELETED","description":"Disk space may be exceeded when soft deleted rows are not purged from Nova database","category":"Availability","severity":"WARN","hitCount":0,"summary":"The database tables in Nova that utilize the *soft delete* method, if unamanaged, may grow very large and the allocated disk may run out of space.\n","summary_html":"<p>The database tables in Nova that utilize the <em>soft delete</em> method, if unamanaged, may grow very large and the allocated disk may run out of space.</p>\n","plugin":"cronjob_4_nova_soft_deleted_rows","error_key":"NO_NOVA_CRONJOB_TO_CLEAR_SOFT_DELETED","plugin_name":"Disk space may be exceeded when soft deleted rows are not purged from Nova database","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":4,"acked":false},{"rule_id":"crashkernel_with_3w_modules|CRASHKERNEL_WITH_3W_MODULES_WARN","description":"Kernel crash with 3w-sas or 3w-9xxx modules loaded due to a bug in kernel","category":"Stability","severity":"WARN","hitCount":0,"summary":"Servers running a kernel prior to 3.10.0-327.28.2.el7 with 3w-sas or 3w-9xxx module loaded when I/O load is heavy due to a known bug in kernel.\n","summary_html":"<p>Servers running a kernel prior to 3.10.0-327.28.2.el7 with 3w-sas or 3w-9xxx module loaded when I/O load is heavy due to a known bug in kernel.</p>\n","plugin":"crashkernel_with_3w_modules","error_key":"CRASHKERNEL_WITH_3W_MODULES_WARN","plugin_name":"Kernel crash with 3w-sas or 3w-9xxx modules loaded due to a bug in kernel","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"crashkernel_reservation_failed|CRASHKERNEL_RESERVATION_FAILED","description":"Kdump crashkernel reservation failed due to improper configuration of crashkernel parameter","category":"Stability","severity":"WARN","hitCount":2,"summary":"The crashkernel configuration has failed to produce a working kdump environment. Configuration changes must be made to enable vmcore capture.\n","summary_html":"<p>The crashkernel configuration has failed to produce a working kdump environment. Configuration changes must be made to enable vmcore capture.</p>\n","plugin":"crashkernel_reservation_failed","error_key":"CRASHKERNEL_RESERVATION_FAILED","plugin_name":"Kdump crashkernel reservation failed due to improper configuration of crashkernel parameter","ansible":0,"rec_impact":1,"rec_likelihood":3,"resolution_risk":3,"acked":false},{"rule_id":"corosync_enable_rt_schedule|COROSYNC_NOT_ENABLE_RT","description":"Cluster nodes are frequently fenced as realtime is not enabled in corosync","category":"Stability","severity":"WARN","hitCount":0,"summary":"Cluster nodes are frequently fenced because the realtime scheduling priority is not enabled. This increases the possibility of corosync token losses.\n","summary_html":"<p>Cluster nodes are frequently fenced because the realtime scheduling priority is not enabled. This increases the possibility of corosync token losses.</p>\n","plugin":"corosync_enable_rt_schedule","error_key":"COROSYNC_NOT_ENABLE_RT","plugin_name":"Cluster nodes are frequently fenced as realtime is not enabled in corosync","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"cmirror_perf_issue|CMIRROR_PERF_ISSUE","description":"Performance or connectivity issues in RHEL when using LVM with cmirror","category":"Performance","severity":"WARN","hitCount":0,"summary":"In Red Hat Enterprise Linux (RHEL) 6 or 7 with the Resilient Storage Add On, when using LVM with cmirror, LVM mirroring takes an excessive amount of I/O capacity, slowing systems and severely impacting running applications.\n","summary_html":"<p>In Red Hat Enterprise Linux (RHEL) 6 or 7 with the Resilient Storage Add On, when using LVM with cmirror, LVM mirroring takes an excessive amount of I/O capacity, slowing systems and severely impacting running applications.</p>\n","plugin":"cmirror_perf_issue","error_key":"CMIRROR_PERF_ISSUE","plugin_name":"Performance or connectivity issues in RHEL when using LVM with cmirror","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"cinder_volume_file_limit|CINDER_VOLUME_INCREASE_OPEN_FILE_LIMIT","description":"Requests for cinder volume resources become unresponsive when exceeding the maximum number of open files","category":"Availability","severity":"WARN","hitCount":0,"summary":"Requests for cinder volumes are consistently unresponsive when issuing many tasks at once because the maximum number of opened files has been reached in cinder volume.\n","summary_html":"<p>Requests for cinder volumes are consistently unresponsive when issuing many tasks at once because the maximum number of opened files has been reached in cinder volume.</p>\n","plugin":"cinder_volume_file_limit","error_key":"CINDER_VOLUME_INCREASE_OPEN_FILE_LIMIT","plugin_name":"Requests for cinder volume resources become unresponsive when exceeding the maximum number of open files","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":2,"acked":false},{"rule_id":"cinder_logs_rabbitmq_password|CINDER_LOGS_RABBITMQ_PASSWORD","description":"RabbitMQ password is at risk of being logged in cinder debug log due to a known bug of openstack-cinder","category":"Security","severity":"WARN","hitCount":0,"summary":"Due to a known bug of openstack-cinder, cinder logs password of RabbitMQ into debug log when \"debug = true\" is set in cinder.conf.\n","summary_html":"<p>Due to a known bug of openstack-cinder, cinder logs password of RabbitMQ into debug log when &quot;debug = true&quot; is set in cinder.conf.</p>\n","plugin":"cinder_logs_rabbitmq_password","error_key":"CINDER_LOGS_RABBITMQ_PASSWORD","plugin_name":"Cinder logs password of RabbitMQ into debug log due to a known bug of openstack-cinder","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"cinder_logs_rabbitmq_password|CINDER_LOGGING_RABBITMQ_PASSWORD","description":"Cinder logs password of RabbitMQ into debug log due to a known bug of openstack-cinder","category":"Security","severity":"WARN","hitCount":0,"summary":"Due to a known bug of openstack-cinder, cinder logs password of RabbitMQ into debug log when \"debug = true\" is set in cinder.conf.\n","summary_html":"<p>Due to a known bug of openstack-cinder, cinder logs password of RabbitMQ into debug log when &quot;debug = true&quot; is set in cinder.conf.</p>\n","plugin":"cinder_logs_rabbitmq_password","error_key":"CINDER_LOGGING_RABBITMQ_PASSWORD","plugin_name":"Cinder logs password of RabbitMQ into debug log due to a known bug of openstack-cinder","ansible":1,"rec_impact":3,"rec_likelihood":1,"resolution_risk":1,"acked":false},{"rule_id":"cinder_clone_fail_with_netapp_nfs|CINDER_CLONE_FAIL_WITH_NETAPP_NFS_SYMPTOM","description":"Cinder cloning has failed when using NetApp driver with NFS protocol and missing mount options (this issue has happened)","category":"Stability","severity":"WARN","hitCount":0,"summary":"Cinder cloning operations are frequently unsuccessful when using NetApp driver with NFS protocol and missing mount options.\n","summary_html":"<p>Cinder cloning operations are frequently unsuccessful when using NetApp driver with NFS protocol and missing mount options.</p>\n","plugin":"cinder_clone_fail_with_netapp_nfs","error_key":"CINDER_CLONE_FAIL_WITH_NETAPP_NFS_SYMPTOM","plugin_name":"Cinder cloning failure when using NetApp driver with NFS protocol and missing mount options","ansible":1,"rec_impact":1,"rec_likelihood":4,"resolution_risk":2,"acked":false},{"rule_id":"cifs_share_lost|UPGRADE_KERNEL_CIFS_WARN","description":"CIFS share gets unmounted due to bug in kernel (this issue may happen in the future)","category":"Availability","severity":"WARN","hitCount":0,"summary":"A CIFS bug in kernel prior to `2.6.32-431.el6` can cause CIFS share to be lost even when there is no network issue.\n","summary_html":"<p>A CIFS bug in kernel prior to <code>2.6.32-431.el6</code> can cause CIFS share to be lost even when there is no network issue.</p>\n","plugin":"cifs_share_lost","error_key":"UPGRADE_KERNEL_CIFS_WARN","plugin_name":"CIFS share gets unmounted due to bug in kernel","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"ceilometer_time_to_live|CEILOMETER_TIME_TO_LIVE","description":"Disk fills up when time_to_live is set to a value less than zero in ceilometer","category":"Availability","severity":"WARN","hitCount":0,"summary":"1. For OSP 8 and 9, the disk storing /var/lib/mongodb fills up when either of metering_time_to_live or event_time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired.\n\n2. For OSP 7, the disk storing /var/lib/mongodb fills up when time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired\n","summary_html":"<ol>\n<li><p>For OSP 8 and 9, the disk storing /var/lib/mongodb fills up when either of metering_time_to_live or event_time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired.</p>\n</li>\n<li><p>For OSP 7, the disk storing /var/lib/mongodb fills up when time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired</p>\n</li>\n</ol>\n","plugin":"ceilometer_time_to_live","error_key":"CEILOMETER_TIME_TO_LIVE","plugin_name":"Disk fills up when improper parameters are set in ceilometer","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"ceilometer_time_to_live|CEILOMETER_DATABASE_SIZE_LARGE","description":"Disk fills up when metering_time_to_live and event_time_to_live are set to a value less than zero in ceilometer","category":"Availability","severity":"WARN","hitCount":0,"summary":"1. For OSP 8 and 9, the disk storing /var/lib/mongodb fills up when either of metering_time_to_live or event_time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired.\n\n2. For OSP 7, the disk storing /var/lib/mongodb fills up when time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired\n","summary_html":"<ol>\n<li><p>For OSP 8 and 9, the disk storing /var/lib/mongodb fills up when either of metering_time_to_live or event_time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired.</p>\n</li>\n<li><p>For OSP 7, the disk storing /var/lib/mongodb fills up when time_to_live is set to a value less than zero in ceilometer.conf as the meter data will never be expired</p>\n</li>\n</ol>\n","plugin":"ceilometer_time_to_live","error_key":"CEILOMETER_DATABASE_SIZE_LARGE","plugin_name":"Disk fills up when improper parameters are set in ceilometer","ansible":0,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"bonding_vlan_failover|BONDING_VLAN_FAILOVER_ISSUE","description":"Bonding failover fails when VLAN tagged interfaces are created on top of the bonding interface with active-backup mode","category":"Availability","severity":"WARN","hitCount":0,"summary":"A known kernel bug can cause bonding failover fails when VLAN tagged interfaces are created on top of the bonding interface with active-backup mode.\n","summary_html":"<p>A known kernel bug can cause bonding failover fails when VLAN tagged interfaces are created on top of the bonding interface with active-backup mode.</p>\n","plugin":"bonding_vlan_failover","error_key":"BONDING_VLAN_FAILOVER_ISSUE","plugin_name":"Bonding failover fails when VLAN tagged interfaces are created on top of the bonding interface with active-backup mode","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"bonding_partner_mac|CHECK_ETHERCHANNEL_CONFIG","description":"Network Connectivity Loss when using bonding mode 4 with a null-value registered in Partner Mac Address","category":"Availability","severity":"WARN","hitCount":0,"summary":"Improper switch configuration can cause a null-value registered in Partner Mac Address (00:00:00:00:00:00) and lead to network connectivity loss when using bonding mode 4 802.3ad (LACP).\n","summary_html":"<p>Improper switch configuration can cause a null-value registered in Partner Mac Address (00:00:00:00:00:00) and lead to network connectivity loss when using bonding mode 4 802.3ad (LACP).</p>\n","plugin":"bonding_partner_mac","error_key":"CHECK_ETHERCHANNEL_CONFIG","plugin_name":"Network Connectivity Loss when using bonding mode 4 with a null-value registered in Partner Mac Address","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"bonding_negotiate|BOND_NEGOTIATE_ISSUE","description":"Network connectivity loss when using bonding mode 4 and specific kernel versions","category":"Availability","severity":"WARN","hitCount":0,"summary":"Bonding with mode 4 (IEEE 802.3ad Dynamic link aggregation) can fail to negotiate with the same aggregator id on all slave interfaces when using kernel versions prior to `2.6.32-131.0.15.el6` and this will lead to network connectivity loss.\n","summary_html":"<p>Bonding with mode 4 (IEEE 802.3ad Dynamic link aggregation) can fail to negotiate with the same aggregator id on all slave interfaces when using kernel versions prior to <code>2.6.32-131.0.15.el6</code> and this will lead to network connectivity loss.</p>\n","plugin":"bonding_negotiate","error_key":"BOND_NEGOTIATE_ISSUE","plugin_name":"Network connectivity loss when using bonding mode 4 and specific kernel versions","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"bonding_lacp|BOND_SLAVE_INTERFACE_ABNORMAL","description":"Decreased network performance when using an incorrect LACP hash in networking bonding","category":"Performance","severity":"WARN","hitCount":0,"summary":"In bonding mode 2 or mode 4, network traffic is not distributed to all slave interfaces due to an incorrect hash policy. This results in decreased network performance. \n","summary_html":"<p>In bonding mode 2 or mode 4, network traffic is not distributed to all slave interfaces due to an incorrect hash policy. This results in decreased network performance. </p>\n","plugin":"bonding_lacp","error_key":"BOND_SLAVE_INTERFACE_ABNORMAL","plugin_name":"Decreased network performance when using an incorrect LACP hash in networking bonding","ansible":1,"rec_impact":2,"rec_likelihood":3,"resolution_risk":2,"acked":false},{"rule_id":"bonding_heartbeat|CLUSTER_HEARTBEAT_MODE_NOT_SUPPORTED","description":"Cluster unavailable when setting unsupported bonding mode on `Cluster-Interconnect` interfaces(cluster heartbeat interfaces)","category":"Stability","severity":"WARN","hitCount":0,"summary":"Cluster unavailable when setting unsupported bonding mode on `Cluster-Interconnect` interfaces (cluster heartbeat interfaces).\n","summary_html":"<p>Cluster unavailable when setting unsupported bonding mode on <code>Cluster-Interconnect</code> interfaces (cluster heartbeat interfaces).</p>\n","plugin":"bonding_heartbeat","error_key":"CLUSTER_HEARTBEAT_MODE_NOT_SUPPORTED","plugin_name":"Cluster unavailable when setting unsupported bonding mode on `Cluster-Interconnect` interfaces(cluster heartbeat interfaces)","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"bonding_down_state|BOND_DOWN_STATE","description":"Bonding can activate incorrect slave interfaces when using mode 4 and specific kernel versions","category":"Availability","severity":"WARN","hitCount":0,"summary":"Bonding with mode 4 (802.3ad) can activate incorrect slave interfaces which are actually in the down state when using kernel versions prior to `2.6.32-220.23.1.el6` and this will lead to network connectivity loss. \n","summary_html":"<p>Bonding with mode 4 (802.3ad) can activate incorrect slave interfaces which are actually in the down state when using kernel versions prior to <code>2.6.32-220.23.1.el6</code> and this will lead to network connectivity loss. </p>\n","plugin":"bonding_down_state","error_key":"BOND_DOWN_STATE","plugin_name":"Bonding can activate incorrect slave interfaces when using mode 4 and specific kernel versions","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"bnx2x_tso_issue|BNX2X_TSO_ISSUE","description":"Kernel panic when using TSO in bnx2x driver","category":"Stability","severity":"WARN","hitCount":0,"summary":"Kernel panic when using TSO in bnx2x driver.\n","summary_html":"<p>Kernel panic when using TSO in bnx2x driver.</p>\n","plugin":"bnx2x_tso_issue","error_key":"BNX2X_TSO_ISSUE","plugin_name":"Kernel panic when using TSO in bnx2x driver","ansible":1,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"blocked_FC_remote_port_time_out|FC_REMOTE_PORT_TIME_OUT_V2","description":"Devices are inaccessible when Fibre Channel port cannot be reached due to a hardware issue","category":"Availability","severity":"WARN","hitCount":0,"summary":"The remote Fibre Channel storage port cannot be reached which results in the issue that server cannot access the devices behind these ports.\n","summary_html":"<p>The remote Fibre Channel storage port cannot be reached which results in the issue that server cannot access the devices behind these ports.</p>\n","plugin":"blocked_FC_remote_port_time_out","error_key":"FC_REMOTE_PORT_TIME_OUT_V2","plugin_name":"Devices are inaccessible when Fibre Channel port cannot be reached due to a hardware issue","ansible":0,"rec_impact":3,"rec_likelihood":1,"resolution_risk":1,"acked":false},{"rule_id":"be2net_generate_pause_frames|BE2NET_GENERATE_PAUSE_FRAMES","description":"Loss of connectivity when continuous pause frames are generated in be2net driver","category":"Stability","severity":"WARN","hitCount":0,"summary":"The continuous pause frames are generated in be2net driver. This is caused by a firmware bug on the Emulex OoneConnect NIC, which makes the attached switch disable the switchport and cause loss of connectivity.\n","summary_html":"<p>The continuous pause frames are generated in be2net driver. This is caused by a firmware bug on the Emulex OoneConnect NIC, which makes the attached switch disable the switchport and cause loss of connectivity.</p>\n","plugin":"be2net_generate_pause_frames","error_key":"BE2NET_GENERATE_PAUSE_FRAMES","plugin_name":"Loss of connectivity when continuous pause frames are generated in be2net driver","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":3,"acked":false},{"rule_id":"bash_injection|VULNERABLE_BASH_DETECTED","description":"Bash locally vulnerable via environment variables (CVE-2014-6271, CVE-2014-7169/Shellshock)","category":"Security","severity":"WARN","hitCount":2,"summary":"In September 2014, an exploitable bug known as Shellshock was discovered in commonly shipped versions of the bash shell.","summary_html":"<p>In September 2014, an exploitable bug known as Shellshock was discovered in commonly shipped versions of the bash shell.</p>\n","plugin":"bash_injection","error_key":"VULNERABLE_BASH_DETECTED","plugin_name":"Bash locally vulnerable via environment variables (CVE-2014-6271, CVE-2014-7169/Shellshock)","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"ansible_deprecated_repo|ANSIBLE_DEPRECATED_REPO","description":"New Ansible Engine packages are inaccessible when dedicated Ansible repo is not enabled","category":"Stability","severity":"WARN","hitCount":2,"summary":"New Ansible Engine packages are inaccessible when dedicated Ansible repo is not enabled.\n","summary_html":"<p>New Ansible Engine packages are inaccessible when dedicated Ansible repo is not enabled.</p>\n","plugin":"ansible_deprecated_repo","error_key":"ANSIBLE_DEPRECATED_REPO","plugin_name":"New Ansible Engine packages are inaccessible when dedicated Ansible repo is not enabled","ansible":1,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"amd_opteron_mod2|AMD_Opteron_BIOS_out_of_date","description":"Kernel crashes randomly due to AMD Opteron bug with old BIOS version","category":"Stability","severity":"WARN","hitCount":0,"summary":"AMD Opteron Model 2 CPUs with specific versions of BIOS can cause a vmcore-producing interruption.","summary_html":"<p>AMD Opteron Model 2 CPUs with specific versions of BIOS can cause a vmcore-producing interruption.</p>\n","plugin":"amd_opteron_mod2","error_key":"AMD_Opteron_BIOS_out_of_date","plugin_name":"Kernel crashes randomly due to AMD Opteron bug with old BIOS version","ansible":0,"rec_impact":3,"rec_likelihood":2,"resolution_risk":1,"acked":false},{"rule_id":"alias_interface_invalid|ALIAS_INTERFACE_INVALID","description":"Interface enabled at boot-time when ONBOOT parameter is disabled in configuration file","category":"Availability","severity":"WARN","hitCount":0,"summary":"When \"ONBOOT=no\" is set on alias interfaces (ex. eth0:1), it has no effect. When the parent interface (ex. eth0) is brought up, the alias will always come up.","summary_html":"<p>When &quot;ONBOOT=no&quot; is set on alias interfaces (ex. eth0:1), it has no effect. When the parent interface (ex. eth0) is brought up, the alias will always come up.</p>\n","plugin":"alias_interface_invalid","error_key":"ALIAS_INTERFACE_INVALID","plugin_name":"Interface enabled at boot-time when ONBOOT parameter is disabled in configuration file","ansible":1,"rec_impact":1,"rec_likelihood":3,"resolution_risk":1,"acked":false},{"rule_id":"abort_command_issued|ABORT_COMMAND_ISSUED","description":"Performance degradation of I/O when commands timeout due to faulty storage hardware","category":"Stability","severity":"WARN","hitCount":0,"summary":"Occurrences of the message \"Abort Command Issued\"  indicate an error condition being returned from the storage area network (SAN).","summary_html":"<p>Occurrences of the message &quot;Abort Command Issued&quot;  indicate an error condition being returned from the storage area network (SAN).</p>\n","plugin":"abort_command_issued","error_key":"ABORT_COMMAND_ISSUED","plugin_name":"Performance degradation of I/O when commands timeout due to faulty storage hardware","ansible":0,"rec_impact":2,"rec_likelihood":2,"resolution_risk":1,"acked":false}],"ruleBinding":"implicit","hitCount":78,"affectedSystemCount":14,"alwaysShow":false,"id":465,"title":"Medium Risk Actions","summary":"Actions identified with a medium level of risk","content":"Actions identified with a medium level of risk","priority":30,"listed":"never","tag":null,"category":null,"severity":"WARN","hidden":false,"slug":"medium-risk"};

/***/ }),

/***/ "./src/App.js":
/*!********************!*\
  !*** ./src/App.js ***!
  \********************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});

var _extends2 = __webpack_require__(/*! babel-runtime/helpers/extends */ "./node_modules/babel-runtime/helpers/extends.js");

var _extends3 = _interopRequireDefault(_extends2);

var _classCallCheck2 = __webpack_require__(/*! babel-runtime/helpers/classCallCheck */ "./node_modules/babel-runtime/helpers/classCallCheck.js");

var _classCallCheck3 = _interopRequireDefault(_classCallCheck2);

var _createClass2 = __webpack_require__(/*! babel-runtime/helpers/createClass */ "./node_modules/babel-runtime/helpers/createClass.js");

var _createClass3 = _interopRequireDefault(_createClass2);

var _possibleConstructorReturn2 = __webpack_require__(/*! babel-runtime/helpers/possibleConstructorReturn */ "./node_modules/babel-runtime/helpers/possibleConstructorReturn.js");

var _possibleConstructorReturn3 = _interopRequireDefault(_possibleConstructorReturn2);

var _inherits2 = __webpack_require__(/*! babel-runtime/helpers/inherits */ "./node_modules/babel-runtime/helpers/inherits.js");

var _inherits3 = _interopRequireDefault(_inherits2);

var _propTypes = __webpack_require__(/*! prop-types */ "./node_modules/prop-types/index.js");

var _propTypes2 = _interopRequireDefault(_propTypes);

var _react = __webpack_require__(/*! react */ "./node_modules/react/index.js");

var _react2 = _interopRequireDefault(_react);

var _reactRouterDom = __webpack_require__(/*! react-router-dom */ "./node_modules/react-router-dom/es/index.js");

var _reactRedux = __webpack_require__(/*! react-redux */ "./node_modules/react-redux/es/index.js");

var _Routes = __webpack_require__(/*! ./Routes */ "./src/Routes.js");

__webpack_require__(/*! ./App.scss */ "./src/App.scss");

var _AppActions = __webpack_require__(/*! ./AppActions */ "./src/AppActions.js");

var AppActions = _interopRequireWildcard(_AppActions);

function _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

var App = function (_Component) {
    (0, _inherits3.default)(App, _Component);

    function App() {
        (0, _classCallCheck3.default)(this, App);
        return (0, _possibleConstructorReturn3.default)(this, (App.__proto__ || Object.getPrototypeOf(App)).apply(this, arguments));
    }

    (0, _createClass3.default)(App, [{
        key: 'componentDidMount',
        value: function componentDidMount() {
            var _this2 = this;

            insights.chrome.init();
            insights.chrome.identifyApp('advisor');
            insights.chrome.navigation(buildNavigation());

            this.appNav = insights.chrome.on('APP_NAVIGATION', function (event) {
                return _this2.props.history.push('/' + event.navId);
            });
            this.buildNav = this.props.history.listen(function () {
                return insights.chrome.navigation(buildNavigation());
            });

            this.props.fetchImpactedSystems();
            this.props.fetchMediumRiskRules();
        }
    }, {
        key: 'componentWillUnmount',
        value: function componentWillUnmount() {
            this.appNav();
            this.buildNav();
        }
    }, {
        key: 'render',
        value: function render() {
            return _react2.default.createElement(_Routes.Routes, { childProps: this.props });
        }
    }]);
    return App;
}(_react.Component);

App.propTypes = {
    history: _propTypes2.default.object,
    fetchImpactedSystems: _propTypes2.default.func,
    fetchMediumRiskRules: _propTypes2.default.func
};

var mapDispatchToProps = function mapDispatchToProps(dispatch) {
    return {
        fetchImpactedSystems: function fetchImpactedSystems() {
            return dispatch(AppActions.fetchImpactedSystems());
        },
        fetchMediumRiskRules: function fetchMediumRiskRules() {
            return dispatch(AppActions.fetchMediumRiskRules());
        }
    };
};

exports.default = (0, _reactRouterDom.withRouter)((0, _reactRedux.connect)(null, mapDispatchToProps)(App));


function buildNavigation() {
    var currentPath = window.location.pathname.split('/').slice(-1)[0];
    return [{
        title: 'Actions',
        id: 'actions'
    }, {
        title: 'Rules',
        id: 'rules'
    }].map(function (item) {
        return (0, _extends3.default)({}, item, {
            active: item.id === currentPath
        });
    });
}

/***/ }),

/***/ "./src/App.scss":
/*!**********************!*\
  !*** ./src/App.scss ***!
  \**********************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// extracted by mini-css-extract-plugin

/***/ }),

/***/ "./src/AppActions.js":
/*!***************************!*\
  !*** ./src/AppActions.js ***!
  \***************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.fetchRules = exports.fetchStats = exports.fetchMediumRiskRules = exports.fetchImpactedSystems = undefined;

var _AppConstants = __webpack_require__(/*! ./AppConstants */ "./src/AppConstants.js");

var ActionTypes = _interopRequireWildcard(_AppConstants);

var _Api = __webpack_require__(/*! ./Utilities/Api */ "./src/Utilities/Api.js");

var _Api2 = _interopRequireDefault(_Api);

var _actionsTypesIds_impactedSystems = __webpack_require__(/*! ../mockData/actions-types-ids_impacted-systems */ "./mockData/actions-types-ids_impacted-systems.json");

var _actionsTypesIds_impactedSystems2 = _interopRequireDefault(_actionsTypesIds_impactedSystems);

var _mediumRisk = __webpack_require__(/*! ../mockData/medium-risk */ "./mockData/medium-risk.json");

var _mediumRisk2 = _interopRequireDefault(_mediumRisk);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }

var impactedSystems = function impactedSystems() {
    return JSON.parse(JSON.stringify(_actionsTypesIds_impactedSystems2.default));
};
var mediumRiskRules = function mediumRiskRules() {
    return JSON.parse(JSON.stringify(_mediumRisk2.default));
};

var fetchImpactedSystems = exports.fetchImpactedSystems = function fetchImpactedSystems() {
    return {
        type: ActionTypes.IMPACTED_SYSTEMS_FETCH,
        payload: new Promise(function (resolve) {
            resolve(impactedSystems);
        })
    };
};
var fetchMediumRiskRules = exports.fetchMediumRiskRules = function fetchMediumRiskRules() {
    return {
        type: ActionTypes.MEDIUM_RISK_RULES_FETCH,
        payload: new Promise(function (resolve) {
            resolve(mediumRiskRules);
        })
    };
};
var fetchStats = exports.fetchStats = function fetchStats() {
    return {
        type: ActionTypes.STATS_FETCH,
        payload: new Promise(function (resolve, reject) {
            _Api2.default.get(ActionTypes.STATS_FETCH_URL).then(function (response) {
                resolve(response.data);
            }).catch(function (e) {
                return reject(e);
            });
        })
    };
};
var fetchRules = exports.fetchRules = function fetchRules(options) {
    return {
        type: ActionTypes.RULES_FETCH,
        payload: new Promise(function (resolve, reject) {
            _Api2.default.get(ActionTypes.RULES_FETCH_URL, {}, options).then(function (response) {
                resolve(response.data);
            }).catch(function (e) {
                return reject(e);
            });
        })
    };
};

/***/ }),

/***/ "./src/AppConstants.js":
/*!*****************************!*\
  !*** ./src/AppConstants.js ***!
  \*****************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
  value: true
});
var RULES_FETCH = exports.RULES_FETCH = 'RULES_FETCH';
var SYSTEM_FETCH = exports.SYSTEM_FETCH = 'SYSTEM_FETCH';
var SYSTEMTYPE_FETCH = exports.SYSTEMTYPE_FETCH = 'SYSTEMTYPE_FETCH';
var MEDIUM_RISK_RULES_FETCH = exports.MEDIUM_RISK_RULES_FETCH = 'MEDIUM_RISK_RULES_FETCH';
var IMPACTED_SYSTEMS_FETCH = exports.IMPACTED_SYSTEMS_FETCH = 'IMPACTED_SYSTEMS_FETCH';
var STATS_FETCH = exports.STATS_FETCH = 'STATS_FETCH';

var RULES_FETCH_URL = exports.RULES_FETCH_URL = '/rule/';
var STATS_FETCH_URL = exports.STATS_FETCH_URL = '/stats/';
var SYSTEM_FETCH_URL = exports.SYSTEM_FETCH_URL = '/system/';
var SYSTEMTYPE_FETCH_URL = exports.SYSTEMTYPE_FETCH_URL = '/systemtype/';

/***/ }),

/***/ "./src/AppReducer.js":
/*!***************************!*\
  !*** ./src/AppReducer.js ***!
  \***************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AdvisorStore = undefined;

var _seamlessImmutable = __webpack_require__(/*! seamless-immutable */ "./node_modules/seamless-immutable/seamless-immutable.development.js");

var _seamlessImmutable2 = _interopRequireDefault(_seamlessImmutable);

var _AppConstants = __webpack_require__(/*! ./AppConstants */ "./src/AppConstants.js");

var ActionTypes = _interopRequireWildcard(_AppConstants);

function _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

// eslint-disable-next-line new-cap
var initialState = (0, _seamlessImmutable2.default)({
    mediumRiskRules: {},
    mediumRiskRulesFetchStatus: '',
    impactedSystems: [],
    impactedSystemsFetchStatus: '',
    rules: {},
    rulesFetchStatus: '',
    stats: {},
    statsFetchStatus: '',
    system: {},
    systemFetchStatus: '',
    systemtype: {},
    systemtypeFetchStatus: ''
});

var AdvisorStore = exports.AdvisorStore = function AdvisorStore() {
    var state = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : initialState;
    var action = arguments[1];

    switch (action.type) {

        case ActionTypes.MEDIUM_RISK_RULES_FETCH + '_PENDING':
            return state.set('mediumRiskRulesFetchStatus', 'pending');
        case ActionTypes.MEDIUM_RISK_RULES_FETCH + '_FULFILLED':
            return _seamlessImmutable2.default.merge(state, {
                mediumRiskRules: action.payload,
                mediumRiskRulesFetchStatus: 'fulfilled' });
        case ActionTypes.MEDIUM_RISK_RULES_FETCH + '_REJECTED':
            return state.set('mediumRiskRulesFetchStatus', 'rejected');

        case ActionTypes.IMPACTED_SYSTEMS_FETCH + '_PENDING':
            return state.set('impactedSystemsFetchStatus', 'pending');
        case ActionTypes.IMPACTED_SYSTEMS_FETCH + '_FULFILLED':
            return _seamlessImmutable2.default.merge(state, {
                impactedSystems: action.payload.resources,
                impactedSystemsFetchStatus: 'fulfilled' });
        case ActionTypes.IMPACTED_SYSTEMS_FETCH + '_REJECTED':
            return state.set('impactedSystemsFetchStatus', 'rejected');

        case ActionTypes.RULES_FETCH + '_PENDING':
            return state.set('rulesFetchStatus', 'pending');
        case ActionTypes.RULES_FETCH + '_FULFILLED':
            return _seamlessImmutable2.default.merge(state, {
                rules: action.payload,
                rulesFetchStatus: 'fulfilled' });
        case ActionTypes.RULES_FETCH + '_REJECTED':
            return state.set('rulesFetchStatus', 'rejected');

        case ActionTypes.STATS_FETCH + '_PENDING':
            return state.set('statsFetchStatus', 'pending');
        case ActionTypes.STATS_FETCH + '_FULFILLED':
            return _seamlessImmutable2.default.merge(state, {
                stats: action.payload,
                statsFetchStatus: 'fulfilled' });
        case ActionTypes.STATS_FETCH + '_REJECTED':
            return state.set('statsFetchStatus', 'rejected');

        case ActionTypes.SYSTEM_FETCH + '_PENDING':
            return state.set('systemFetchStatus', 'pending');
        case ActionTypes.SYSTEM_FETCH + '_FULFILLED':
            return _seamlessImmutable2.default.merge(state, {
                system: action.payload,
                systemFetchStatus: 'fulfilled' });
        case ActionTypes.SYSTEM_FETCH + '_REJECTED':
            return state.set('systemFetchStatus', 'rejected');

        case ActionTypes.SYSTEMTYPE_FETCH + '_PENDING':
            return state.set('systemtypeFetchStatus', 'pending');
        case ActionTypes.SYSTEMTYPE_FETCH + '_FULFILLED':
            return _seamlessImmutable2.default.merge(state, {
                systemtype: action.payload,
                systemtypeFetchStatus: 'fulfilled' });
        case ActionTypes.SYSTEMTYPE_FETCH + '_REJECTED':
            return state.set('systemFetchStatus', 'rejected');

        default:
            return state;
    }
};

/***/ }),

/***/ "./src/Routes.js":
/*!***********************!*\
  !*** ./src/Routes.js ***!
  \***********************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Routes = undefined;

var _objectWithoutProperties2 = __webpack_require__(/*! babel-runtime/helpers/objectWithoutProperties */ "./node_modules/babel-runtime/helpers/objectWithoutProperties.js");

var _objectWithoutProperties3 = _interopRequireDefault(_objectWithoutProperties2);

var _reactRouterDom = __webpack_require__(/*! react-router-dom */ "./node_modules/react-router-dom/es/index.js");

var _propTypes = __webpack_require__(/*! prop-types */ "./node_modules/prop-types/index.js");

var _propTypes2 = _interopRequireDefault(_propTypes);

var _react = __webpack_require__(/*! react */ "./node_modules/react/index.js");

var _react2 = _interopRequireDefault(_react);

var _asyncComponent = __webpack_require__(/*! ./Utilities/asyncComponent */ "./src/Utilities/asyncComponent.js");

var _asyncComponent2 = _interopRequireDefault(_asyncComponent);

var _some = __webpack_require__(/*! lodash/some */ "./node_modules/lodash/some.js");

var _some2 = _interopRequireDefault(_some);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

/**
 * Aysnc imports of components
 *
 * https://webpack.js.org/guides/code-splitting/
 * https://reactjs.org/docs/code-splitting.html
 *
 * pros:
 *      1) code splitting
 *      2) can be used in server-side rendering
 * cons:
 *      1) nameing chunk names adds unnecessary docs to code,
 *         see the difference with DashboardMap and InventoryDeployments.
 *
 */
var Actions = (0, _asyncComponent2.default)(function () {
    return __webpack_require__.e(/*! import() | Actions */ "Actions").then(__webpack_require__.t.bind(null, /*! ./SmartComponents/Actions/Actions */ "./src/SmartComponents/Actions/Actions.js", 7));
});
var Rules = (0, _asyncComponent2.default)(function () {
    return __webpack_require__.e(/*! import() | Rules */ "Rules").then(__webpack_require__.t.bind(null, /*! ./SmartComponents/Rules/Rules */ "./src/SmartComponents/Rules/Rules.js", 7));
});
var paths = {
    actions: '/actions',
    rules: '/rules'
};

var InsightsRoute = function InsightsRoute(_ref) {
    var Component = _ref.component,
        rootClass = _ref.rootClass,
        rest = (0, _objectWithoutProperties3.default)(_ref, ['component', 'rootClass']);

    var root = document.getElementById('root');
    root.removeAttribute('class');
    root.classList.add('page__' + rootClass, 'pf-l-page__main');
    root.setAttribute('role', 'main');

    return _react2.default.createElement(Component, rest);
};

InsightsRoute.propTypes = {
    component: _propTypes2.default.func,
    rootClass: _propTypes2.default.string
};

/**
 * the Switch component changes routes depending on the path.
 *
 * Route properties:
 *      exact - path must match exactly,
 *      path - https://prod.foo.redhat.com:1337/insights/advisor/rules
 *      component - component to be rendered when a route has been chosen.
 */
var Routes = exports.Routes = function Routes(props) {
    var path = props.childProps.location.pathname;

    return _react2.default.createElement(
        _reactRouterDom.Switch,
        null,
        _react2.default.createElement(InsightsRoute, { path: paths.actions, component: Actions, rootClass: 'actions' }),
        _react2.default.createElement(InsightsRoute, { path: paths.rules, component: Rules, rootClass: 'rules' }),
        _react2.default.createElement(_reactRouterDom.Route, { render: function render() {
                return (0, _some2.default)(paths, function (p) {
                    return p === path;
                }) ? null : _react2.default.createElement(_reactRouterDom.Redirect, { to: paths.actions });
            } })
    );
};

/***/ }),

/***/ "./src/Store/index.js":
/*!****************************!*\
  !*** ./src/Store/index.js ***!
  \****************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.init = init;
exports.getStore = getStore;
exports.register = register;

var _reduxPromiseMiddleware = __webpack_require__(/*! redux-promise-middleware */ "./node_modules/redux-promise-middleware/dist/es/index.js");

var _reduxPromiseMiddleware2 = _interopRequireDefault(_reduxPromiseMiddleware);

var _AppReducer = __webpack_require__(/*! ../AppReducer */ "./src/AppReducer.js");

var _redux = __webpack_require__(/*! redux */ "./node_modules/redux/es/redux.js");

var _insightsFrontendComponents = __webpack_require__(/*! @red-hat-insights/insights-frontend-components */ "./node_modules/@red-hat-insights/insights-frontend-components/index.js");

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

var registry = void 0;

function init() {
    var composeEnhancers = window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ || _redux.compose;

    for (var _len = arguments.length, middleware = Array(_len), _key = 0; _key < _len; _key++) {
        middleware[_key] = arguments[_key];
    }

    registry = (0, _insightsFrontendComponents.getRegistry)({}, [].concat(middleware, [(0, _reduxPromiseMiddleware2.default)()]), composeEnhancers);

    registry.register({ AdvisorStore: _AppReducer.AdvisorStore });

    return registry;
}

function getStore() {
    return registry.getStore();
}

function register() {
    var _registry;

    return (_registry = registry).register.apply(_registry, arguments);
}

/***/ }),

/***/ "./src/Utilities/Api.js":
/*!******************************!*\
  !*** ./src/Utilities/Api.js ***!
  \******************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});

var _axios = __webpack_require__(/*! axios */ "./node_modules/axios/index.js");

var _axios2 = _interopRequireDefault(_axios);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

exports.default = {
    get: function get(url) {
        var headers = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
        var params = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};

        return _axios2.default.get(url, {
            headers: headers,
            params: params
        });
    },
    put: function put(url) {
        var data = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
        var headers = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};

        return _axios2.default.put(url, data, {
            headers: headers
        });
    },
    post: function post(url) {
        var data = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
        var headers = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};

        return _axios2.default.post(url, data, {
            headers: headers
        });
    }
};

/***/ }),

/***/ "./src/Utilities/asyncComponent.js":
/*!*****************************************!*\
  !*** ./src/Utilities/asyncComponent.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


Object.defineProperty(exports, "__esModule", {
    value: true
});

var _regenerator = __webpack_require__(/*! babel-runtime/regenerator */ "./node_modules/babel-runtime/regenerator/index.js");

var _regenerator2 = _interopRequireDefault(_regenerator);

var _asyncToGenerator2 = __webpack_require__(/*! babel-runtime/helpers/asyncToGenerator */ "./node_modules/babel-runtime/helpers/asyncToGenerator.js");

var _asyncToGenerator3 = _interopRequireDefault(_asyncToGenerator2);

var _classCallCheck2 = __webpack_require__(/*! babel-runtime/helpers/classCallCheck */ "./node_modules/babel-runtime/helpers/classCallCheck.js");

var _classCallCheck3 = _interopRequireDefault(_classCallCheck2);

var _createClass2 = __webpack_require__(/*! babel-runtime/helpers/createClass */ "./node_modules/babel-runtime/helpers/createClass.js");

var _createClass3 = _interopRequireDefault(_createClass2);

var _possibleConstructorReturn2 = __webpack_require__(/*! babel-runtime/helpers/possibleConstructorReturn */ "./node_modules/babel-runtime/helpers/possibleConstructorReturn.js");

var _possibleConstructorReturn3 = _interopRequireDefault(_possibleConstructorReturn2);

var _inherits2 = __webpack_require__(/*! babel-runtime/helpers/inherits */ "./node_modules/babel-runtime/helpers/inherits.js");

var _inherits3 = _interopRequireDefault(_inherits2);

exports.default = asyncComponent;

var _react = __webpack_require__(/*! react */ "./node_modules/react/index.js");

var _react2 = _interopRequireDefault(_react);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

/**
 * Webpack allows loading components asynchronously by using import().
 *
 *      Ex) const Component = asyncComponent(() => import('component');
 *
 *          class aClass extends React.Component {
 *              render() {
 *                  return (<Component prop1="prop1" prop2="prop2" ... />);
 *              }
 *          }
 *
 *  https://reactjs.org/docs/higher-order-components.html
 *
 * @param importComponent a function that contains and async import statement
 *      Ex) () => import('react-component')
 *
 * @returns {AsyncComponent} The imported component or can return a loading
 */
function asyncComponent(importComponent) {
    var AsyncComponent = function (_Component) {
        (0, _inherits3.default)(AsyncComponent, _Component);

        function AsyncComponent(props) {
            (0, _classCallCheck3.default)(this, AsyncComponent);

            var _this = (0, _possibleConstructorReturn3.default)(this, (AsyncComponent.__proto__ || Object.getPrototypeOf(AsyncComponent)).call(this, props));

            _this.state = {
                component: null
            };
            return _this;
        }

        (0, _createClass3.default)(AsyncComponent, [{
            key: 'componentDidMount',
            value: function () {
                var _ref = (0, _asyncToGenerator3.default)( /*#__PURE__*/_regenerator2.default.mark(function _callee() {
                    var _ref2, component;

                    return _regenerator2.default.wrap(function _callee$(_context) {
                        while (1) {
                            switch (_context.prev = _context.next) {
                                case 0:
                                    _context.next = 2;
                                    return importComponent();

                                case 2:
                                    _ref2 = _context.sent;
                                    component = _ref2.default;


                                    this.setState({
                                        component: component
                                    });

                                case 5:
                                case 'end':
                                    return _context.stop();
                            }
                        }
                    }, _callee, this);
                }));

                function componentDidMount() {
                    return _ref.apply(this, arguments);
                }

                return componentDidMount;
            }()
        }, {
            key: 'render',
            value: function render() {
                var C = this.state.component;

                return C ? _react2.default.createElement(C, this.props) : _react2.default.createElement(
                    'div',
                    null,
                    'Loading...'
                );
            }
        }]);
        return AsyncComponent;
    }(_react.Component);

    return AsyncComponent;
}

/***/ }),

/***/ "./src/entry-dev.js":
/*!**************************!*\
  !*** ./src/entry-dev.js ***!
  \**************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var _react = __webpack_require__(/*! react */ "./node_modules/react/index.js");

var _react2 = _interopRequireDefault(_react);

var _reactDom = __webpack_require__(/*! react-dom */ "./node_modules/react-dom/index.js");

var _reactDom2 = _interopRequireDefault(_reactDom);

var _reactRouterDom = __webpack_require__(/*! react-router-dom */ "./node_modules/react-router-dom/es/index.js");

var _reactRedux = __webpack_require__(/*! react-redux */ "./node_modules/react-redux/es/index.js");

var _Store = __webpack_require__(/*! ./Store */ "./src/Store/index.js");

var _App = __webpack_require__(/*! ./App */ "./src/App.js");

var _App2 = _interopRequireDefault(_App);

var _reduxLogger = __webpack_require__(/*! redux-logger */ "./node_modules/redux-logger/dist/redux-logger.js");

var _reduxLogger2 = _interopRequireDefault(_reduxLogger);

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

_reactDom2.default.render(_react2.default.createElement(
    _reactRedux.Provider,
    { store: (0, _Store.init)(_reduxLogger2.default).getStore() },
    _react2.default.createElement(
        _reactRouterDom.BrowserRouter,
        { basename: '/insights/platform/advisor' },
        _react2.default.createElement(_App2.default, null)
    )
), document.getElementById('root'));

/***/ })

/******/ });
//# sourceMappingURL=../sourcemaps/App.js.map